{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MungingData","text":"<p>This site contains blog posts on Apache Spark, pandas, Delta Lake and related technologies.</p> <p>The code is fully open source and blog posts from the community are encouraged.</p>"},{"location":"apache-spark/advanced-string-matching-with-rlike/","title":"Advanced String Matching with Spark's rlike Method","text":"<p>The Spark <code>rlike</code> method allows you to write powerful string matching algorithms with regular expressions (regexp).</p> <p>This blog post will outline tactics to detect strings that match multiple different patterns and how to abstract these regular expression patterns to CSV files.</p> <p>Writing Beautiful Spark Code is the best way to learn how to use regular expressions when working with Spark StringType columns.</p>"},{"location":"apache-spark/advanced-string-matching-with-rlike/#substring-matching","title":"Substring matching","text":"<p>Let\u2019s create a DataFrame and use <code>rlike</code> to identify all strings that contain the substring <code>\"cat\"</code>.</p> <pre><code>val df = List(\n  (\"the cat and the hat\"),\n  (\"i love your cat\"),\n  (\"dogs are cute\"),\n  (\"pizza please\")\n).toDF(\"phrase\")\n\ndf\n  .withColumn(\n    \"contains_cat\",\n    col(\"phrase\").rlike(\"cat\")\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+-------------------+------------+\n|phrase             |contains_cat|\n+-------------------+------------+\n|the cat and the hat|true        |\n|i love your cat    |true        |\n|dogs are cute      |false       |\n|pizza please       |false       |\n+-------------------+------------+\n</code></pre> <p>There is nothing special about this example and if you\u2019re only looking to match a single substring, it\u2019s better to use <code>contains</code> than <code>rlike</code>.</p> <p>Let\u2019s rework this code to detect all strings that contain the substrings <code>\"cat\"</code> or <code>\"dog\"</code>.</p> <pre><code>df\n  .withColumn(\n    \"contains_cat_or_dog\",\n    col(\"phrase\").rlike(\"cat|dog\")\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+-------------------+------------+\n|phrase             |contains_cat|\n+-------------------+------------+\n|the cat and the hat|true        |\n|i love your cat    |true        |\n|dogs are cute      |true        |\n|pizza please       |false       |\n+-------------------+------------+\n</code></pre> <p>We can refactor this code by storing the animals in a list and concatenating them as a pipe delimited string for the <code>rlike</code> method.</p> <pre><code>val animals = List(\"cat\", \"dog\")\n\ndf\n  .withColumn(\n    \"contains_cat_or_dog\",\n    col(\"phrase\").rlike(animals.mkString(\"|\"))\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+-------------------+-------------------+\n|phrase             |contains_cat_or_dog|\n+-------------------+-------------------+\n|the cat and the hat|true               |\n|i love your cat    |true               |\n|dogs are cute      |true               |\n|pizza please       |false              |\n+-------------------+-------------------+\n</code></pre>"},{"location":"apache-spark/advanced-string-matching-with-rlike/#matching-strings-that-start-with-or-end-with-substrings","title":"Matching strings that start with or end with substrings","text":"<p>Let\u2019s create a new DataFrame and match all strings that begin with the substring <code>\"i like\"</code> or <code>\"i want\"</code>.</p> <pre><code>val df = List(\n  (\"i like tacos\"),\n  (\"i want love\"),\n  (\"pie is what i like\"),\n  (\"pizza pizza\"),\n  (\"you like pie\")\n).toDF(\"phrase\")\n\ndf\n  .withColumn(\n    \"starts_with_desire\",\n    col(\"phrase\").rlike(\"^i like|^i want\")\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+------------------+------------------+\n|phrase            |starts_with_desire|\n+------------------+------------------+\n|i like tacos      |true              |\n|i want love       |true              |\n|pie is what i like|false             |\n|pizza pizza       |false             |\n|you like pie      |false             |\n+------------------+------------------+\n</code></pre> <p>We can also append an <code>ends_with_food</code> column using regular expresssions.</p> <pre><code>val foods = List(\"tacos\", \"pizza\", \"pie\")\ndf\n  .withColumn(\n    \"ends_with_food\",\n    col(\"phrase\").rlike(foods.map(_ + \"$\").mkString(\"|\"))\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+------------------+--------------+\n|phrase            |ends_with_food|\n+------------------+--------------+\n|i like tacos      |true          |\n|i want love       |false         |\n|pie is what i like|false         |\n|pizza pizza       |true          |\n|you like pie      |true          |\n+------------------+--------------+\n</code></pre>"},{"location":"apache-spark/advanced-string-matching-with-rlike/#matching-strings-that-contain-regex-characters","title":"Matching strings that contain regex characters","text":"<p>Suppose we want to find all the strings that contain the substring <code>\"fun|stuff\"</code>. We don\u2019t want all strings that contain fun or stuff\u200a\u2014\u200awe want all strings that match the substring <code>fun|stuff</code> exactly.</p> <p>The approch we\u2019ve been using won\u2019t work as desired, because it will match all strings that contain <code>fun</code> or <code>stuff</code>.</p> <pre><code>df\n  .withColumn(\n    \"contains_fun_pipe_stuff\",\n    col(\"phrase\").rlike(\"fun|stuff\")\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+------------------+-----------------------+\n|phrase            |contains_fun_pipe_stuff|\n+------------------+-----------------------+\n|fun|stuff         |true                   |\n|dancing is fun    |true                   |\n|you have stuff    |true                   |\n|where is fun|stuff|true                   |\n|something else    |false                  |\n+------------------+-----------------------+\n</code></pre> <p>We can use the <code>java.util.regex.Pattern</code> to quote the regular expression and properly match the <code>fun|stuff</code> string exactly.</p> <pre><code>import java.util.regex.Pattern\n\ndf\n  .withColumn(\n    \"contains_fun_pipe_stuff\",\n    col(\"phrase\").rlike(Pattern.quote(\"fun|stuff\"))\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+------------------+-----------------------+\n|phrase            |contains_fun_pipe_stuff|\n+------------------+-----------------------+\n|fun|stuff         |true                   |\n|dancing is fun    |false                  |\n|you have stuff    |false                  |\n|where is fun|stuff|true                   |\n|something else    |false                  |\n+------------------+-----------------------+\n</code></pre> <p><code>Pattern.quote(\"fun|stuff\")</code> returns <code>\"\\Qfun|stuff\\E\"</code>.</p> <p>The <code>Pattern.quote()</code> method wraps the string in <code>\\Q</code> and <code>\\E</code> to turn the text into a regexp literal, as described in this Stackoverflow thread.</p> <p>Alternatively, we can escape the pipe character in the regexp with <code>\\\\</code>.</p> <pre><code>df\n  .withColumn(\n    \"contains_fun_pipe_stuff\",\n    col(\"phrase\").rlike(\"fun\\\\|stuff\")\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+------------------+-----------------------+\n|phrase            |contains_fun_pipe_stuff|\n+------------------+-----------------------+\n|fun|stuff         |true                   |\n|dancing is fun    |false                  |\n|you have stuff    |false                  |\n|where is fun|stuff|true                   |\n|something else    |false                  |\n+------------------+-----------------------+\n</code></pre>"},{"location":"apache-spark/advanced-string-matching-with-rlike/#abstracting-multiple-pattern-match-criteria-to-csv-files","title":"Abstracting multiple pattern match criteria to CSV\u00a0files","text":"<p>You may want to store multiple string matching criteria in a separate CSV file rather than directly in the code. Let\u2019s create a CSV that matches all strings that start with <code>coffee</code>, end with <code>bread</code> or contain <code>nice|person</code>. Here\u2019s the content of the <code>random_matches.csv</code> file.</p> <pre><code>^coffee\nbread$\nnice\\\\|person\n</code></pre> <p>The pipe character in the CSV file needs to be escaped with <code>\\\\</code>.</p> <p>Here\u2019s the how to use the CSV file to match strings that match at least one of the regexp criteria.</p> <pre><code>val df = List(\n  (\"coffee is good\"),\n  (\"i need coffee\"),\n  (\"bread is good\"),\n  (\"i need bread\"),\n  (\"you're a nice|person\"),\n  (\"that is nice\")\n).toDF(\"phrase\")\n\nval weirdMatchesPath = new java.io.File(s\"./src/test/resources/random_matches.csv\").getCanonicalPath\n\nval weirdMatchesDF = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(weirdMatchesPath)\n\nval matchString = DataFrameHelpers.columnToArray[String](\n  weirdMatchesDF,\n  \"match_criteria\"\n).mkString(\"|\")\n\ndf\n  .withColumn(\n    \"weird_matches\",\n    col(\"phrase\").rlike(matchString)\n  )\n  .show(truncate = false)\n</code></pre> <pre><code>+--------------------+-------------+\n|phrase              |weird_matches|\n+--------------------+-------------+\n|coffee is good      |true         |\n|i need coffee       |false        |\n|bread is good       |false        |\n|i need bread        |true         |\n|you're a nice|person|true         |\n|that is nice        |false        |\n+--------------------+-------------+\n</code></pre>"},{"location":"apache-spark/advanced-string-matching-with-rlike/#next-steps","title":"Next steps","text":"<p>Some people, when confronted with a problem, think \u201cI know, I\u2019ll use regular expressions.\u201d Now they have two problems. \u2014Jamie Zawinski</p> <p>Using regular expressions is controversial to say the least. Regular expressions are powerful tools for advanced string matching, but can create code bases that are difficult to maintain. Thoroughly testing regular expression behavior and documenting the expected results in comments is vital, especially when multiple regexp criteria are chained together.</p> <p>Spark\u2019s <code>rlike</code> method allows for powerful string matching. You\u2019ll be rewarded with great results if you can learn to use these tools effectively.</p>"},{"location":"apache-spark/aggregations/","title":"Aggregations with Spark (groupBy, cube, rollup)","text":"<p>Spark has a variety of aggregate functions to group, cube, and rollup DataFrames.</p> <p>This post will explain how to use aggregate functions with Spark.</p> <p>Check out Beautiful Spark Code for a detailed overview of how to structure and test aggregations in production applications.</p>"},{"location":"apache-spark/aggregations/#groupby","title":"groupBy()","text":"<p>Let's create a DataFrame with two famous soccer players and the number of goals they scored in some games.</p> <pre><code>val goalsDF = Seq(\n  (\"messi\", 2),\n  (\"messi\", 1),\n  (\"pele\", 3),\n  (\"pele\", 1)\n).toDF(\"name\", \"goals\")\n</code></pre> <p>Let's inspect the contents of the DataFrame:</p> <pre><code>goalsDF.show()\n\n+-----+-----+\n| name|goals|\n+-----+-----+\n|messi|    2|\n|messi|    1|\n| pele|    3|\n| pele|    1|\n+-----+-----+\n</code></pre> <p>Let's use <code>groupBy()</code> to calculate the total number of goals scored by each player.</p> <pre><code>import org.apache.spark.sql.functions._\n\ngoalsDF\n  .groupBy(\"name\")\n  .agg(sum(\"goals\"))\n  .show()\n</code></pre> <pre><code>+-----+----------+\n| name|sum(goals)|\n+-----+----------+\n| pele|         4|\n|messi|         3|\n+-----+----------+\n</code></pre> <p>We need to import <code>org.apache.spark.sql.functions._</code> to access the <code>sum()</code> method in <code>agg(sum(\"goals\")</code>. There are a ton of aggregate functions defined in the functions object.</p> <p>The <code>groupBy</code> method is defined in the Dataset class. <code>groupBy</code> returns a RelationalGroupedDataset object where the <code>agg()</code> method is defined.</p> <p>Spark makes great use of object oriented programming!</p> <p>The <code>RelationalGroupedDataset</code> class also defines a <code>sum()</code> method that can be used to get the same result with less code.</p> <pre><code>goalsDF\n  .groupBy(\"name\")\n  .sum()\n  .show()\n</code></pre> <pre><code>+-----+----------+\n| name|sum(goals)|\n+-----+----------+\n| pele|         4|\n|messi|         3|\n+-----+----------+\n</code></pre>"},{"location":"apache-spark/aggregations/#groupby-with-two-arguments","title":"groupBy() with two arguments","text":"<p>Let's create another DataFrame with information on students, their country, and their continent.</p> <pre><code>val studentsDF = Seq(\n  (\"mario\", \"italy\", \"europe\"),\n  (\"stefano\", \"italy\", \"europe\"),\n  (\"victor\", \"spain\", \"europe\"),\n  (\"li\", \"china\", \"asia\"),\n  (\"yuki\", \"japan\", \"asia\"),\n  (\"vito\", \"italy\", \"europe\")\n).toDF(\"name\", \"country\", \"continent\")\n</code></pre> <p>Let's get a count of the number of students in each continent / country.</p> <pre><code>studentsDF\n  .groupBy(\"continent\", \"country\")\n  .agg(count(\"*\"))\n  .show()\n</code></pre> <pre><code>+---------+-------+--------+\n|continent|country|count(1)|\n+---------+-------+--------+\n|   europe|  italy|       3|\n|     asia|  japan|       1|\n|   europe|  spain|       1|\n|     asia|  china|       1|\n+---------+-------+--------+\n</code></pre> <p>We can also leverage the <code>RelationalGroupedDataset#count()</code> method to get the same result:</p> <pre><code>studentsDF\n  .groupBy(\"continent\", \"country\")\n  .count()\n  .show()\n</code></pre> <pre><code>+---------+-------+-----+\n|continent|country|count|\n+---------+-------+-----+\n|   europe|  italy|    3|\n|     asia|  japan|    1|\n|   europe|  spain|    1|\n|     asia|  china|    1|\n+---------+-------+-----+\n</code></pre>"},{"location":"apache-spark/aggregations/#groupby-with-filters","title":"groupBy() with filters","text":"<p>Let's create another DataFrame with the number of goals and assists for two hockey players during a few seasons:</p> <pre><code>val hockeyPlayersDF = Seq(\n  (\"gretzky\", 40, 102, 1990),\n  (\"gretzky\", 41, 122, 1991),\n  (\"gretzky\", 31, 90, 1992),\n  (\"messier\", 33, 61, 1989),\n  (\"messier\", 45, 84, 1991),\n  (\"messier\", 35, 72, 1992),\n  (\"messier\", 25, 66, 1993)\n).toDF(\"name\", \"goals\", \"assists\", \"season\")\n</code></pre> <p>Let's calculate the average number of goals and assists for each player in the 1991 and 1992 seasons.</p> <pre><code>hockeyPlayersDF\n  .where($\"season\".isin(\"1991\", \"1992\"))\n  .groupBy(\"name\")\n  .agg(avg(\"goals\"), avg(\"assists\"))\n  .show()\n</code></pre> <pre><code>+-------+----------+------------+\n|   name|avg(goals)|avg(assists)|\n+-------+----------+------------+\n|messier|      40.0|        78.0|\n|gretzky|      36.0|       106.0|\n+-------+----------+------------+\n</code></pre> <p>Now let's calculate the average number of goals and assists for each player with more than 100 assists on average.</p> <pre><code>hockeyPlayersDF\n  .groupBy(\"name\")\n  .agg(avg(\"goals\"), avg(\"assists\").as(\"average_assists\"))\n  .where($\"average_assists\" &gt;= 100)\n  .show()\n</code></pre> <pre><code>+-------+------------------+------------------+\n|   name|        avg(goals)|   average_assists|\n+-------+------------------+------------------+\n|gretzky|37.333333333333336|104.66666666666667|\n+-------+------------------+------------------+\n</code></pre> <p>Many SQL implementations use the <code>HAVING</code> keyword for filtering after aggregations. The same Spark <code>where()</code> clause works when filtering both before and after aggregations.</p>"},{"location":"apache-spark/aggregations/#cube","title":"cube()","text":"<p><code>cube</code> isn't used too frequently, so feel free to skip this section.</p> <p>Let's create another sample dataset and replicate the <code>cube()</code> examples in this Stackoverflow answer.</p> <pre><code>val df = Seq(\n  (\"bar\", 2L),\n  (\"bar\", 2L),\n  (\"foo\", 1L),\n  (\"foo\", 2L)\n).toDF(\"word\", \"num\")\n</code></pre> <p>The <code>cube</code> function \"takes a list of columns and applies aggregate expressions to all possible combinations of the grouping columns\".</p> <pre><code>df\n  .cube($\"word\", $\"num\")\n  .count()\n  .sort(asc(\"word\"), asc(\"num\"))\n  .show()\n</code></pre> <pre><code>+----+----+-----+\n|word| num|count|\n+----+----+-----+\n|null|null|    4| Total rows in df\n|null|   1|    1| Count where num equals 1\n|null|   2|    3| Count where num equals 2\n| bar|null|    2| Where word equals bar\n| bar|   2|    2| Where word equals bar and num equals 2\n| foo|null|    2| Where word equals foo\n| foo|   1|    1| Where word equals foo and num equals 1\n| foo|   2|    1| Where word equals foo and num equals 2\n+----+----+-----+\n</code></pre> <p>The order of the arguments passed to the <code>cube()</code> function don't matter, so <code>cube($\"word\", $\"num\")</code> will return the same results as <code>cube($\"num\", $\"word\")</code>.</p>"},{"location":"apache-spark/aggregations/#rollup","title":"rollup()","text":"<p><code>rollup</code> is a subset of <code>cube</code> that \"computes hierarchical subtotals from left to right\".</p> <pre><code>df\n  .rollup($\"word\", $\"num\")\n  .count()\n  .sort(asc(\"word\"), asc(\"num\"))\n  .show()\n</code></pre> <pre><code>+----+----+-----+\n|word| num|count|\n+----+----+-----+\n|null|null|    4| Count of all rows\n| bar|null|    2| Count when word is bar\n| bar|   2|    2| Count when num is 2\n| foo|null|    2| Count when word is foo\n| foo|   1|    1| When word is foo and num is 1\n| foo|   2|    1| When word is foo and num is 2\n+----+----+-----+\n</code></pre> <p><code>rollup()</code> returns a subset of the rows returned by <code>cube()</code>. <code>rollup</code> returns 6 rows whereas <code>cube</code> returns 8 rows. Here are the missing rows.</p> <pre><code>+----+----+-----+\n|word| num|count|\n+----+----+-----+\n|null|   1|    1| Word is null and num is 1\n|null|   2|    3| Word is null and num is 2\n+----+----+-----+\n</code></pre> <p><code>rollup($\"word\", $\"num\")</code> doesn't return the counts when only <code>word</code> is <code>null</code>.</p> <p>Let's switch around the order of the arguments passed to <code>rollup</code> and view the difference in the results.</p> <pre><code>df\n  .rollup($\"num\", $\"word\")\n  .count()\n  .sort(asc(\"word\"), asc(\"num\"))\n  .select(\"word\", \"num\", \"count\")\n  .show()\n</code></pre> <pre><code>+----+----+-----+\n|word| num|count|\n+----+----+-----+\n|null|null|    4|\n|null|   1|    1|\n|null|   2|    3|\n| bar|   2|    2|\n| foo|   1|    1|\n| foo|   2|    1|\n+----+----+-----+\n</code></pre> <p>Here are the rows missing from <code>rollup($\"num\", $\"word\")</code> compared to <code>cube($\"word\", $\"num\")</code>.</p> <pre><code>+----+----+-----+\n|word| num|count|\n+----+----+-----+\n| bar|null|    2| Word equals bar and num is null\n| foo|null|    2| Word equals foo and num is null\n+----+----+-----+\n</code></pre> <p><code>rollup($\"num\", $\"word\")</code> doesn't return the counts when only <code>num</code> is <code>null</code>.</p>"},{"location":"apache-spark/aggregations/#next-steps","title":"Next steps","text":"<p>Spark makes it easy to run aggregations at scale.</p> <p>In production applications, you'll often want to do much more than run a simple aggregation. You'll want to verify the correctness of your code with tests and incrementally update aggregations. Make sure you learn how to test your aggregation functions!</p> <p>If you're still struggling with the Spark basics, make sure to read a good book to grasp the fundamentals.</p> <p>Study the <code>groupBy</code> function, the aggregate functions, and the RelationalGroupedDataset class to quickly master aggregations in Spark.</p>"},{"location":"apache-spark/arraytype-columns/","title":"Working with Spark ArrayType columns","text":"<p>Spark DataFrame columns support arrays, which are great for data sets that have an arbitrary length.</p> <p>This blog post will demonstrate Spark methods that return ArrayType columns, describe how to create your own ArrayType columns, and explain when to use arrays in your analyses.</p> <p>See this post if you're using Python / PySpark. The rest of this blog uses Scala.</p> <p>The Beautiful Spark book is the best way for you to learn about the most important parts of Spark, like ArrayType columns. The book is easy to read and will help you level-up your Spark skills.</p>"},{"location":"apache-spark/arraytype-columns/#scala-collections","title":"Scala collections","text":"<p>Scala has different types of collections: lists, sequences, and arrays. Let's quickly review the different types of Scala collections before jumping into collections for Spark analyses.</p> <p>Let's create and sort a collection of numbers.</p> <pre><code>List(10, 2, 3).sorted // List[Int] = List(2, 3, 10)\nSeq(10, 2, 3).sorted // Seq[Int] = List(2, 3, 10)\nArray(10, 2, 3).sorted // Array[Int] = Array(2, 3, 10)\n</code></pre> <p><code>List</code>, <code>Seq</code>, and <code>Array</code> differ slightly, but generally work the same. Most Spark programmers don't need to know about how these collections differ.</p> <p>Spark uses arrays for <code>ArrayType</code> columns, so we'll mainly use arrays in our code snippets.</p>"},{"location":"apache-spark/arraytype-columns/#splitting-a-string-into-an-arraytype-column","title":"Splitting a string into an ArrayType column","text":"<p>Let\u2019s create a DataFrame with a <code>name</code> column and a <code>hit_songs</code> pipe delimited string. Then let\u2019s use the <code>split()</code> method to convert <code>hit_songs</code> into an array of strings.</p> <pre><code>val singersDF = Seq(\n  (\"beatles\", \"help|hey jude\"),\n  (\"romeo\", \"eres mia\")\n).toDF(\"name\", \"hit_songs\")\n\nval actualDF = singersDF.withColumn(\n  \"hit_songs\",\n  split(col(\"hit_songs\"), \"\\\\|\")\n)\n</code></pre> <pre><code>actualDF.show()\n\n+-------+----------------+\n|   name|       hit_songs|\n+-------+----------------+\n|beatles|[help, hey jude]|\n|  romeo|      [eres mia]|\n+-------+----------------+\n</code></pre> <pre><code>actualDF.printSchema()\n\nroot\n |-- name: string (nullable = true)\n |-- hit_songs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre> <p>An <code>ArrayType</code> column is suitable in this example because a singer can have an arbitrary amount of hit songs. We don\u2019t want to create a DataFrame with <code>hit_song1</code>, <code>hit_song2</code>, \u2026, <code>hit_songN</code> columns.</p>"},{"location":"apache-spark/arraytype-columns/#directly-creating-an-arraytype-column","title":"Directly creating an <code>ArrayType</code> column","text":"<p>Let\u2019s use the spark-daria <code>createDF</code> method to create a DataFrame with an ArrayType column directly. See this blog post for more information about the <code>createDF</code> method.</p> <p>Let's create another <code>singersDF</code> with some different artists.</p> <pre><code>val singersDF = spark.createDF(\n  List(\n    (\"bieber\", Array(\"baby\", \"sorry\")),\n    (\"ozuna\", Array(\"criminal\"))\n  ), List(\n    (\"name\", StringType, true),\n    (\"hit_songs\", ArrayType(StringType, true), true)\n  )\n)\n</code></pre> <pre><code>singersDF.show()\n\n+------+-------------+\n|  name|    hit_songs|\n+------+-------------+\n|bieber|[baby, sorry]|\n| ozuna|   [criminal]|\n+------+-------------+\n</code></pre> <pre><code>singersDF.printSchema()\n\nroot\n |-- name: string (nullable = true)\n |-- hit_songs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n</code></pre> <p>The <code>ArrayType</code> case class is instantiated with an <code>elementType</code> and a <code>containsNull</code> flag. In <code>ArrayType(StringType, true)</code>, <code>StringType</code> is the <code>elementType</code> and <code>true</code> is the <code>containsNull</code> flag.</p> <p>See the documentation for the class here.</p>"},{"location":"apache-spark/arraytype-columns/#array_contains","title":"<code>array_contains</code>","text":"<p>The Spark functions object provides helper methods for working with <code>ArrayType</code> columns. The <code>array_contains</code> method returns <code>true</code> if the column contains a specified element.</p> <p>Let\u2019s create an array with people and their favorite colors. Then let\u2019s use <code>array_contains</code> to append a <code>likes_red</code> column that returns true if the person likes red.</p> <pre><code>val peopleDF = spark.createDF(\n  List(\n    (\"bob\", Array(\"red\", \"blue\")),\n    (\"maria\", Array(\"green\", \"red\")),\n    (\"sue\", Array(\"black\"))\n  ), List(\n    (\"name\", StringType, true),\n    (\"favorite_colors\", ArrayType(StringType, true), true)\n  )\n)\n\nval actualDF = peopleDF.withColumn(\n  \"likes_red\",\n  array_contains(col(\"favorite_colors\"), \"red\")\n)\n</code></pre> <pre><code>actualDF.show()\n\n+-----+---------------+---------+\n| name|favorite_colors|likes_red|\n+-----+---------------+---------+\n|  bob|    [red, blue]|     true|\n|maria|   [green, red]|     true|\n|  sue|        [black]|    false|\n+-----+---------------+---------+\n</code></pre>"},{"location":"apache-spark/arraytype-columns/#explode","title":"<code>explode</code>","text":"<p>Let's use the same DataFrame before and the <code>explode()</code> to create a new row for every element in each array.</p> <pre><code>val df = peopleDF.select(\n  col(\"name\"),\n  explode(col(\"favorite_colors\")).as(\"color\")\n)\n</code></pre> <pre><code>df.show()\n\n+-----+-----+\n| name|color|\n+-----+-----+\n|  bob|  red|\n|  bob| blue|\n|maria|green|\n|maria|  red|\n|  sue|black|\n+-----+-----+\n</code></pre> <p><code>peopleDF</code> has 3 rows and <code>df</code> has 5 rows. The <code>explode()</code> method adds rows to a DataFrame.</p>"},{"location":"apache-spark/arraytype-columns/#collect_list","title":"<code>collect_list</code>","text":"<p>The <code>collect_list</code> method collapses a DataFrame into fewer rows and stores the collapsed data in an <code>ArrayType</code> column.</p> <p>Let's create a DataFrame with <code>letter1</code>, <code>letter2</code>, and <code>number1</code> columns.</p> <pre><code>val df = Seq(\n  (\"a\", \"b\", 1),\n  (\"a\", \"b\", 2),\n  (\"a\", \"b\", 3),\n  (\"z\", \"b\", 4),\n  (\"a\", \"x\", 5)\n).toDF(\"letter1\", \"letter2\", \"number1\")\n\ndf.show()\n</code></pre> <pre><code>+-------+-------+-------+\n|letter1|letter2|number1|\n+-------+-------+-------+\n|      a|      b|      1|\n|      a|      b|      2|\n|      a|      b|      3|\n|      z|      b|      4|\n|      a|      x|      5|\n+-------+-------+-------+\n</code></pre> <p>Let's use the <code>collect_list()</code> method to eliminate all the rows with duplicate <code>letter1</code> and <code>letter2</code> rows in the DataFrame and collect all the <code>number1</code> entries as a list.</p> <pre><code>df\n  .groupBy(\"letter1\", \"letter2\")\n  .agg(collect_list(\"number1\") as \"number1s\")\n  .show()\n</code></pre> <pre><code>+-------+-------+---------+\n|letter1|letter2| number1s|\n+-------+-------+---------+\n|      a|      x|      [5]|\n|      z|      b|      [4]|\n|      a|      b|[1, 2, 3]|\n+-------+-------+---------+\n</code></pre> <p>We can see that <code>number1s</code> is an <code>ArrayType</code> column.</p> <pre><code>df.printSchema\n\nroot\n |-- letter1: string (nullable = true)\n |-- letter2: string (nullable = true)\n |-- number1s: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n</code></pre>"},{"location":"apache-spark/arraytype-columns/#single-column-array-functions","title":"Single column array functions","text":"<p>Spark added a ton of useful array functions in the 2.4 release.</p> <p>We will start with the functions for a single <code>ArrayType</code> column and then move on to the functions for multiple <code>ArrayType</code> columns.</p> <p>Let's start by creating a DataFrame with an <code>ArrayType</code> column.</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(1, 2)),\n    (Array(1, 2, 3, 1)),\n    (null)\n  ), List(\n    (\"nums\", ArrayType(IntegerType, true), true)\n  )\n)\n</code></pre> <pre><code>df.show()\n\n+------------+\n|        nums|\n+------------+\n|      [1, 2]|\n|[1, 2, 3, 1]|\n|        null|\n+------------+\n</code></pre> <p>Let's use the <code>array_distinct()</code> method to remove all of the duplicate array elements in the <code>nums</code> column.</p> <pre><code>df\n  .withColumn(\"nums_distinct\", array_distinct($\"nums\"))\n  .show()\n\n+------------+-------------+\n|        nums|nums_distinct|\n+------------+-------------+\n|      [1, 2]|       [1, 2]|\n|[1, 2, 3, 1]|    [1, 2, 3]|\n|        null|         null|\n+------------+-------------+\n</code></pre> <p>Let's use <code>array_join()</code> to create a pipe delimited string of all elements in the arrays.</p> <pre><code>df\n  .withColumn(\"nums_joined\", array_join($\"nums\", \"|\"))\n  .show()\n\n+------------+-----------+\n|        nums|nums_joined|\n+------------+-----------+\n|      [1, 2]|        1|2|\n|[1, 2, 3, 1]|    1|2|3|1|\n|        null|       null|\n+------------+-----------+\n</code></pre> <p>Let's use the <code>printSchema</code> method to verify that the <code>nums_joined</code> column is a <code>StringType</code>.</p> <pre><code>df\n  .withColumn(\"nums_joined\", array_join($\"nums\", \"|\"))\n  .printSchema()\n\nroot\n |-- nums: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n |-- nums_joined: string (nullable = true)\n</code></pre> <p>Let's use <code>array_max</code> to grab the maximum value from the arrays.</p> <pre><code>df\n  .withColumn(\"nums_max\", array_max($\"nums\"))\n  .show()\n\n+------------+--------+\n|        nums|nums_max|\n+------------+--------+\n|      [1, 2]|       2|\n|[1, 2, 3, 1]|       3|\n|        null|    null|\n+------------+--------+\n</code></pre> <p>Let's use <code>array_min</code> to grab the minimum value from the arrays.</p> <pre><code>df\n  .withColumn(\"nums_min\", array_min($\"nums\"))\n  .show()\n\n+------------+--------+\n|        nums|nums_min|\n+------------+--------+\n|      [1, 2]|       1|\n|[1, 2, 3, 1]|       1|\n|        null|    null|\n+------------+--------+\n</code></pre> <p>Let's use the <code>array_remove</code> method to remove all the 1s from each of the arrays.</p> <pre><code>df\n  .withColumn(\"nums_sans_1\", array_remove($\"nums\", 1))\n  .show()\n\n+------------+-----------+\n|        nums|nums_sans_1|\n+------------+-----------+\n|      [1, 2]|        [2]|\n|[1, 2, 3, 1]|     [2, 3]|\n|        null|       null|\n+------------+-----------+\n</code></pre> <p>Let's use <code>array_sort</code> to sort all of the arrays in ascending order.</p> <pre><code>df\n  .withColumn(\"nums_sorted\", array_sort($\"nums\"))\n  .show()\n\n+------------+------------+\n|        nums| nums_sorted|\n+------------+------------+\n|      [1, 2]|      [1, 2]|\n|[1, 2, 3, 1]|[1, 1, 2, 3]|\n|        null|        null|\n+------------+------------+\n</code></pre>"},{"location":"apache-spark/arraytype-columns/#spark-3-array-functions","title":"Spark 3 Array Functions","text":"<p>Spark 3 added some incredibly useful array functions as described in this post.</p> <p>exists, forall, transform, aggregate, and zip_with makes it much easier to use ArrayType columns with native Spark code instead of using UDFs. Make sure to read the blog post that discusses these functions in detail if you're using Spark 3.</p>"},{"location":"apache-spark/arraytype-columns/#generic-single-column-array-functions","title":"Generic single column array functions","text":"<p>Skip this section if you're using Spark 3. The approach outlined in this section is only needed for Spark 2.</p> <p>Suppose you have an array of strings and would like to see if all elements in the array begin with the letter <code>c</code>. Here's how you can run this check on a Scala array:</p> <pre><code>Array(\"cream\", \"cookies\").forall(_.startsWith(\"c\")) // true\nArray(\"taco\", \"clam\").forall(_.startsWith(\"c\")) // false\n</code></pre> <p>You can use the spark-daria <code>forall()</code> method to run this computation on a Spark DataFrame with an <code>ArrayType</code> column.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.functions._\n\nval df = spark.createDF(\n  List(\n    (Array(\"cream\", \"cookies\")),\n    (Array(\"taco\", \"clam\"))\n  ), List(\n    (\"words\", ArrayType(StringType, true), true)\n  )\n)\n\ndf.withColumn(\n  \"all_words_begin_with_c\",\n  forall[String]((x: String) =&gt; x.startsWith(\"c\")).apply(col(\"words\"))\n).show()\n</code></pre> <pre><code>+----------------+----------------------+\n|           words|all_words_begin_with_c|\n+----------------+----------------------+\n|[cream, cookies]|                  true|\n|    [taco, clam]|                 false|\n+----------------+----------------------+\n</code></pre> <p>The native Spark API doesn't provide access to all the helpful collection methods provided by Scala. spark-daria uses User Defined Functions to define <code>forall</code> and <code>exists</code> methods. Email me or create an issue if you would like any additional UDFs to be added to spark-daria.</p>"},{"location":"apache-spark/arraytype-columns/#multiple-column-array-functions","title":"Multiple column array functions","text":"<p>Let's create a DataFrame with two <code>ArrayType</code> columns so we can try out the built-in Spark array functions that take multiple columns as input.</p> <pre><code>val numbersDF = spark.createDF(\n  List(\n    (Array(1, 2), Array(4, 5, 6)),\n    (Array(1, 2, 3, 1), Array(2, 3, 4)),\n    (null, Array(6, 7))\n  ), List(\n    (\"nums1\", ArrayType(IntegerType, true), true),\n    (\"nums2\", ArrayType(IntegerType, true), true)\n  )\n)\n</code></pre> <p>Let's use <code>array_intersect</code> to get the elements present in both the arrays without any duplication.</p> <pre><code>numbersDF\n  .withColumn(\"nums_intersection\", array_intersect($\"nums1\", $\"nums2\"))\n  .show()\n\n+------------+---------+-----------------+\n|       nums1|    nums2|nums_intersection|\n+------------+---------+-----------------+\n|      [1, 2]|[4, 5, 6]|               []|\n|[1, 2, 3, 1]|[2, 3, 4]|           [2, 3]|\n|        null|   [6, 7]|             null|\n+------------+---------+-----------------+\n</code></pre> <p>Let's use <code>array_union</code> to get the elements in either array, without duplication.</p> <pre><code>numbersDF\n  .withColumn(\"nums_union\", array_union($\"nums1\", $\"nums2\"))\n  .show()\n</code></pre> <pre><code>+------------+---------+---------------+\n|       nums1|    nums2|     nums_union|\n+------------+---------+---------------+\n|      [1, 2]|[4, 5, 6]|[1, 2, 4, 5, 6]|\n|[1, 2, 3, 1]|[2, 3, 4]|   [1, 2, 3, 4]|\n|        null|   [6, 7]|           null|\n+------------+---------+---------------+\n</code></pre> <p>Let's use <code>array_except</code> to get the elements that are in <code>num1</code> and not in <code>num2</code> without any duplication.</p> <pre><code>numbersDF\n  .withColumn(\"nums1_nums2_except\", array_except($\"nums1\", $\"nums2\"))\n  .show()\n\n+------------+---------+------------------+\n|       nums1|    nums2|nums1_nums2_except|\n+------------+---------+------------------+\n|      [1, 2]|[4, 5, 6]|            [1, 2]|\n|[1, 2, 3, 1]|[2, 3, 4]|               [1]|\n|        null|   [6, 7]|              null|\n+------------+---------+------------------+\n</code></pre>"},{"location":"apache-spark/arraytype-columns/#split-array-column-into-multiple-columns","title":"Split array column into multiple columns","text":"<p>We can split an array column into multiple columns with <code>getItem</code>. Lets create a DataFrame with a <code>letters</code> column and demonstrate how this single <code>ArrayType</code> column can be split into a DataFrame with three <code>StringType</code> columns.</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(\"a\", \"b\", \"c\")),\n    (Array(\"d\", \"e\", \"f\")),\n    (null)\n  ), List(\n    (\"letters\", ArrayType(StringType, true), true)\n  )\n)\n</code></pre> <pre><code>df.show()\n\n+---------+\n|  letters|\n+---------+\n|[a, b, c]|\n|[d, e, f]|\n|     null|\n+---------+\n</code></pre> <p>This example uses the same data as this Stackoverflow question.</p> <p>Let's use <code>getItem</code> to break out the array into <code>col1</code>, <code>col2</code>, and <code>col3</code>.</p> <pre><code>df\n  .select(\n    $\"letters\".getItem(0).as(\"col1\"),\n    $\"letters\".getItem(1).as(\"col2\"),\n    $\"letters\".getItem(2).as(\"col3\")\n  )\n  .show()\n\n+----+----+----+\n|col1|col2|col3|\n+----+----+----+\n|   a|   b|   c|\n|   d|   e|   f|\n|null|null|null|\n+----+----+----+\n</code></pre> <p>Here's how we can use <code>getItem</code> with a loop.</p> <pre><code>df\n  .select(\n    (0 until 3).map(i =&gt; $\"letters\".getItem(i).as(s\"col$i\")): _*\n  )\n  .show()\n\n+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   a|   b|   c|\n|   d|   e|   f|\n|null|null|null|\n+----+----+----+\n</code></pre> <p>Our code snippet above is a little ugly because the 3 is hardcoded. We can calculate the size of every array in the column, take the max size, and use that rather than hardcoding.</p> <pre><code>val numCols = df\n  .withColumn(\"letters_size\", size($\"letters\"))\n  .agg(max($\"letters_size\"))\n  .head()\n  .getInt(0)\n\ndf\n  .select(\n    (0 until numCols).map(i =&gt; $\"letters\".getItem(i).as(s\"col$i\")): _*\n  )\n  .show()\n\n+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   a|   b|   c|\n|   d|   e|   f|\n|null|null|null|\n+----+----+----+\n</code></pre>"},{"location":"apache-spark/arraytype-columns/#complex-spark-column-types","title":"Complex Spark Column types","text":"<p>Spark supports MapType and StructType columns in addition to the ArrayType columns covered in this post.</p>"},{"location":"apache-spark/arraytype-columns/#closing-thoughts","title":"Closing thoughts","text":"<p>Spark <code>ArrayType</code> columns makes it easy to work with collections at scale.</p> <p>Master the content covered in this blog to add a powerful skill to your toolset.</p>"},{"location":"apache-spark/best-books/","title":"Best Apache Spark Books","text":"<p>Apache Spark is a big data engine that has quickly become one of the biggest distributed processing frameworks in the world.</p> <p>It's used by all the big financial institutions and technology companies.</p> <p>Small teams also find Spark invaluable. Even small research teams can spin up a cluster of cloud computers with the computational powers of a super computer and pay by the second for resources used.</p> <p>Learning how to use Spark effectively isn't easy\u2026</p> <p>Cluster computing is complex!</p> <p>The best Spark book for you depends on your current level and what you'd like to do with Spark. If you're a data engineer and want to focus on using Spark to clean a massive dataset, then you should stay away from the Spark machine learning algorithms.</p>"},{"location":"apache-spark/best-books/#high-level-review","title":"High level review","text":"<p>Writing Beautiful Spark Code, Testing Spark Applications, and Advanced Analytics with Spark are the best Spark books (they all use the Scala API).</p> <p>Full disclosure: I wrote Beautiful Spark Code and Testing Spark Applications, mainly to give the community some good options.</p> <p>I don't know of any good PySpark books. Message me if you know of a good book, so I can update this blog post!</p> <ul> <li>Writing Beautiful Spark Code teaches you how to become a proficient Spark programmer quickly. It focuses on the most used API functions and workflows you need to get your job done. It does not focus on low level implementation details or theory. This book will teach you all the practical design patterns you need to be productive with Spark.</li> <li>Testing Spark Applications teaches you how to test the public interface of your applications. You will see how testing encourages you to write higher quality code and identify bottlenecks. Code without tests is hard to debug and refactor. Messy code is also prone to production bugs. Learning to test will take some upfront time, but will save you from a lot of application errors.</li> <li>Apache Spark in 24 hours is a great book on the current state of big data technologies</li> <li>Advanced Analytics with Spark is great for learning how to run machine learning algorithms at scale</li> <li>Learning Spark is useful if you're using the RDD API (it's outdated for DataFrame users)</li> </ul>"},{"location":"apache-spark/best-books/#beginner-books","title":"Beginner books","text":""},{"location":"apache-spark/best-books/#apache-spark-in-24-hours-sams-teach-yourself","title":"Apache Spark in 24 Hours, Sams Teach Yourself","text":"<p>Jeffrey Aven has 30 years of industry experience and is an experienced teacher / consultant in Australia. His experience and desire to teach topics in a logical manner makes his book a great place to learn about Spark and how it can fit into a production grade big data ecosystem.</p> <p>The book focuses on PySpark, but also shows examples in Scala. It does a good job covering the different APIs, but not getting bogged down by showing every single example with all the programming languages supported by Spark.</p> <p>Chapter 16 focuses on the R API for the few readers that are interested in R. The author doesn't bog down the entire text with R examples.</p> <p>The author's extensive experience in the big data industry allows him to give a wonderful overview on how Spark fits in a big data tech stack. He talks about HDFS, YARN, Hadoop. AWS ec2, AWS EMR, Databricks, and MapReduce.</p> <p>A tech stack that includes Spark won't include all of the aforementioned technologies, but it's easy to skip sections that aren't relevant. I loved learning about important technologies that I don't work with directly, like HDFS, at a high level.</p> <p>The main downside of the book is the focus on RDDs instead of DataFrames. The DataFrame API is easier to work with and RDDs should generally be avoided by practitioners.</p> <p>So if you're looking for a book that'll help you get your job done this week, this isn't the best book for you. But if you're looking for a book that'll make you a better data engineer or data scientist in the next few months, this book is perfect.</p> <p>Most books that cover such a wide variety of technologies are terrible. They cover too much material and don't connect the dots so the reader feels lost and overwhelmed.</p> <p>Jeffrey Aven's Apache Spark in 24 Hours does a masterful job introducing the reader to technologies important to the modern big data stack, with a focus on Spark. He's uniquely qualified to write such a high quality book because he's an experience teacher and has 30 years of experience in the industry.</p> <p>This book will help you appreciate the history of big data and will make you a better data scientist / data engineer.</p>"},{"location":"apache-spark/best-books/#spark-the-definitive-guide","title":"Spark: The Definitive Guide","text":"<p>Spark: The Definitive Guide is 600 page book that introduces the main features of the Spark engine. This is the best beginner Spark book as of 2019.</p> <p>If you're willing to slog through a big text, you'll be able to learn from this book, but it'll require some patience.</p> <ul> <li>Four programatic APIs are covered to a varying degree, so you're bound to see code snippets that aren't relevant for you. PySpark programmers only want to see the Python API, Scala programmers only want to see the Scala API, and Java programmers only want to see the Java API. Spark books should be released in different languages - there should be one Spark: The Definitive Guide book and another PySpark: The Definitive Guide book so readers don't need to wade through irrelevant code snippets.</li> <li>Spark is a vast data engine with packages for SQL, machine learning, streaming, and graphs. Spark also provides access to lower level RDD APIs. Beginner Spark programmers should master the SQL API before moving on to the other APIs. This book introduces advanced Spark APIs before covering the important foundational SQL concepts which will confuse some readers.</li> <li>Book feels like a reference manual at times. The discussion on Spark types starts with a quick discussion on how to access the ByteType in Scala, Java, and Python and then presents a full page reference table for all the Python types, another full page table for the Scala types and and third full page table for the Java types. They note that the API can change and even link to the API documentation. Sections like these feel like a book version of the API docs.</li> <li>Topics aren't presented in a logical order. A teacher that's using this book for a college class suggests reading Chapter 1 and then skipping to Chapter 4 because Chapters 2 and 3 will confuse you. Chapter 3 dives into the K-Means clustering way before the reader is perpared to handle a complex machine learning algorithm.</li> <li>Tangents upon tangents. On page 58, the Spark types discussion is interrupted with a section on the difference between the Dataset and DataFrame APIs. This distinction is only relevant for Scala / Java users. The tangent embeds a NOTE about the Catalyst Optimizer, a low level detail that's only relevant for advanced Spark users. Introductory texts shouldn't cover highly technical low level details, especially not at the beginning of the book.</li> <li>Code snippets are difficult to read in the printed version. In the electronic version, the code snippets are in color and are easy to read. The text wasn't converted to black and white before it was printed, so the code is printed as light grey text on white paper. For readers with bad eyesight, the paperback code is unreadable.</li> <li>Python 2 was used for the Python code which wasn't a great choice because Python 2 is end of life as of January 1, 2020.</li> </ul>"},{"location":"apache-spark/best-books/#learning-spark","title":"Learning Spark","text":"<p>Learning Spark is a great book if you're working with Spark 1 or would like to better understand RDDs in Spark 2.</p> <p>Most data engineers and data scientists that are working with Spark 2 only use the DataFrame API and don't ever need to consider the lower level RDD API.</p> <p>This book is outdated for most Spark users, but it's well written / organized and a great learning tool for RDD users.</p>"},{"location":"apache-spark/best-books/#machine-learning","title":"Machine Learning","text":""},{"location":"apache-spark/best-books/#advanced-analytics-with-spark","title":"Advanced Analytics with Spark","text":"<p>Advanced Analytics with Spark will teach you how to implement common machine learning models at scale. There are a lot of Python libraries that make it easy to train models on smaller data sets. The implementations in this book can be run on Spark clusters and scaled to handle massive datasets.</p> <p>The book starts with an introduction to data analysis with Scala and Spark. It lays out an unfortunate truth for some readers - most of the code in this book uses the Scala API. The authors make a compelling argument for using the Scala for data science, but it's unlikely Python developers will agree ;)</p> <p>The book chapters are written by Sandy Ryza, Uri Laserson, Sean Owen, and Josh Wills. The authors didn't collaborate on the individual chapters - each chapter was written by a single author.</p> <p>The introduction to Scala chapter uses Spark 2.1 and covers the basics on data munging, RDDs, and DataFrames. They introduce YARN and HDFS, two technologies that the book should have avoided in my opinion. Books should aim to use the minimum number of technologies required to teach readers and HDFS isn't needed.</p> <p>If you don't have any experience with Spark or Scala, this introductory chapter probably won't suffice. Spark and Scala are complex and can't be taught in a single chapter. Leave a comment if you'd like me to write a data munging book that would be a good prerequisite to this book.</p> <p>The recommending music chapter is written by Sean Owen and uses the audioscribbler data set and the alternating least squares recommender algorithm. The chapter is approachable for readers that aren't familiar with the data set or the algorthm.</p> <p>The chapter even gives a nice high level overview of the matrix factorization model that underlies the algorithm. The code snippets are easy to follow and the datasets are easy to find in the GitHub repo for the book. After the model is built, the author walks you through evaluating recommendation quality with an area under curve analysis.</p> <p>The next chapter, also by Sean Owen, uses decision trees to predict forest cover using a dataset of forest covering parcels of land in Colorado. The author asserts that the tree dataset to illustarte the decision tree algorith is a coincidence - he's obviously a punny guy!</p> <p>Like the previous chapter, the author covers the algorithm at a high level before digging into the details. Regression algorithms with decision trees are examples of supervised learning for building predictions.</p> <p>The chapter explains how to build feature vectors and how decision trees generalize into a more powerful algorithm called random decision forests. Once the model is built, the author shows you how it can be used to make predictions.</p> <p>The next chapter uses the K-means clustering algorithm to build an anomoly detection in network traffic model. I especially like how this chapter shows how to plot three dimensional data with SparkR.</p> <p>It's hard to imagine, but the book continues with several more amazing chapters that I won't cover in detail. The last two chapters in the book are on genomic and neuroimaging data and are difficult to follow because the data is complex. Those are the only chapters you might end up skipping.</p>"},{"location":"apache-spark/broadcast-joins/","title":"Introduction to Spark Broadcast Joins","text":"<p>Spark broadcast joins are perfect for joining a large DataFrame with a small DataFrame.</p> <p>Broadcast joins cannot be used when joining two large DataFrames.</p> <p>This post explains how to do a simple broadcast join and how the <code>broadcast()</code> function helps Spark optimize the execution plan.</p> <p>Check out Writing Beautiful Spark Code for full coverage of broadcast joins.</p>"},{"location":"apache-spark/broadcast-joins/#conceptual-overview","title":"Conceptual overview","text":"<p>Spark splits up data on different nodes in a cluster so multiple computers can process data in parallel. Traditional joins are hard with Spark because the data is split.</p> <p>Broadcast joins are easier to run on a cluster. Spark can \"broadcast\" a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster. After the small DataFrame is broadcasted, Spark can perform a join without shuffling any of the data in the large DataFrame.</p>"},{"location":"apache-spark/broadcast-joins/#simple-example","title":"Simple example","text":"<p>Let's create a DataFrame with information about people and another DataFrame with information about cities. In this example, both DataFrames will be small, but let's pretend that the <code>peopleDF</code> is huge and the <code>citiesDF</code> is tiny.</p> <pre><code>val peopleDF = Seq(\n  (\"andrea\", \"medellin\"),\n  (\"rodolfo\", \"medellin\"),\n  (\"abdul\", \"bangalore\")\n).toDF(\"first_name\", \"city\")\n\npeopleDF.show()\n</code></pre> <pre><code>+----------+---------+\n|first_name|     city|\n+----------+---------+\n|    andrea| medellin|\n|   rodolfo| medellin|\n|     abdul|bangalore|\n+----------+---------+\n</code></pre> <pre><code>val citiesDF = Seq(\n  (\"medellin\", \"colombia\", 2.5),\n  (\"bangalore\", \"india\", 12.3)\n).toDF(\"city\", \"country\", \"population\")\n\ncitiesDF.show()\n</code></pre> <pre><code>+---------+--------+----------+\n|     city| country|population|\n+---------+--------+----------+\n| medellin|colombia|       2.5|\n|bangalore|   india|      12.3|\n+---------+--------+----------+\n</code></pre> <p>Let's broadcast the <code>citiesDF</code> and join it with the <code>peopleDF</code>.</p> <pre><code>peopleDF.join(\n  broadcast(citiesDF),\n  peopleDF(\"city\") &lt;=&gt; citiesDF(\"city\")\n).show()\n</code></pre> <pre><code>+----------+---------+---------+--------+----------+\n|first_name|     city|     city| country|population|\n+----------+---------+---------+--------+----------+\n|    andrea| medellin| medellin|colombia|       2.5|\n|   rodolfo| medellin| medellin|colombia|       2.5|\n|     abdul|bangalore|bangalore|   india|      12.3|\n+----------+---------+---------+--------+----------+\n</code></pre> <p>The Spark null safe equality operator (<code>&lt;=&gt;</code>) is used to perform this join.</p>"},{"location":"apache-spark/broadcast-joins/#analyzing-physical-plans-of-joins","title":"Analyzing physical plans of joins","text":"<p>Let's use the <code>explain()</code> method to analyze the physical plan of the broadcast join.</p> <pre><code>peopleDF.join(\n  broadcast(citiesDF),\n  peopleDF(\"city\") &lt;=&gt; citiesDF(\"city\")\n).explain()\n</code></pre> <pre><code>== Physical Plan ==\nBroadcastHashJoin [coalesce(city#6, )], [coalesce(city#21, )], Inner, BuildRight, (city#6 &lt;=&gt; city#21)\n:- LocalTableScan [first_name#5, city#6]\n+- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], )))\n   +- LocalTableScan [city#21, country#22, population#23]\n</code></pre> <p>In this example, Spark is smart enough to return the same physical plan, even when the <code>broadcast()</code> method isn't used.</p> <pre><code>peopleDF.join(\n  citiesDF,\n  peopleDF(\"city\") &lt;=&gt; citiesDF(\"city\")\n).explain()\n</code></pre> <pre><code>== Physical Plan ==\nBroadcastHashJoin [coalesce(city#6, )], [coalesce(city#21, )], Inner, BuildRight, (city#6 &lt;=&gt; city#21)\n:- LocalTableScan [first_name#5, city#6]\n+- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], )))\n   +- LocalTableScan [city#21, country#22, population#23]\n</code></pre> <p>Spark isn't always smart about optimally broadcasting DataFrames when the code is complex, so it's best to use the <code>broadcast()</code> method explicitly and inspect the physical plan.</p>"},{"location":"apache-spark/broadcast-joins/#eliminating-the-duplicate-city-column","title":"Eliminating the duplicate <code>city</code> column","text":"<p>We can pass a sequence of columns with the shortcut join syntax to automatically delete the duplicate column.</p> <pre><code>peopleDF.join(\n  broadcast(citiesDF),\n  Seq(\"city\")\n).show()\n</code></pre> <pre><code>+---------+----------+--------+----------+\n|     city|first_name| country|population|\n+---------+----------+--------+----------+\n| medellin|    andrea|colombia|       2.5|\n| medellin|   rodolfo|colombia|       2.5|\n|bangalore|     abdul|   india|      12.3|\n+---------+----------+--------+----------+\n</code></pre> <p>Let's look at the physical plan that's generated by this code.</p> <pre><code>peopleDF.join(\n  broadcast(citiesDF),\n  Seq(\"city\")\n).explain()\n</code></pre> <pre><code>== Physical Plan ==\nProject [city#6, first_name#5, country#22, population#23]\n+- BroadcastHashJoin [city#6], [city#21], Inner, BuildRight\n   :- Project [_1#2 AS first_name#5, _2#3 AS city#6]\n   :  +- Filter isnotnull(_2#3)\n   :     +- LocalTableScan [_1#2, _2#3]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n      +- Project [_1#17 AS city#21, _2#18 AS country#22, _3#19 AS population#23]\n         +- Filter isnotnull(_1#17)\n            +- LocalTableScan [_1#17, _2#18, _3#19]\n</code></pre> <p>Code that returns the same result without relying on the sequence join generates an entirely different physical plan.</p> <pre><code>peopleDF.join(\n  broadcast(citiesDF),\n  peopleDF(\"city\") &lt;=&gt; citiesDF(\"city\")\n)\n  .drop(citiesDF(\"city\"))\n  .explain()\n</code></pre> <pre><code>== Physical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- BroadcastHashJoin [coalesce(city#6, )], [coalesce(city#21, )], Inner, BuildRight, (city#6 &lt;=&gt; city#21)\n   :- LocalTableScan [first_name#5, city#6]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], )))\n      +- LocalTableScan [city#21, country#22, population#23]\n</code></pre> <p>It's best to avoid the shortcut join syntax so your physical plans stay as simple as possible.</p>"},{"location":"apache-spark/broadcast-joins/#diving-deeper-into-explain","title":"Diving deeper into <code>explain()</code>","text":"<p>You can pass the <code>explain()</code> method a <code>true</code> argument to see the parsed logical plan, analyzed logical plan, and optimized logical plan in addition to the physical plan.</p> <pre><code>peopleDF.join(\n  broadcast(citiesDF),\n  peopleDF(\"city\") &lt;=&gt; citiesDF(\"city\")\n)\n  .drop(citiesDF(\"city\"))\n  .explain(true)\n</code></pre> <pre><code>== Parsed Logical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- Join Inner, (city#6 &lt;=&gt; city#21)\n   :- Project [_1#2 AS first_name#5, _2#3 AS city#6]\n   :  +- LocalRelation [_1#2, _2#3]\n   +- ResolvedHint isBroadcastable=true\n      +- Project [_1#17 AS city#21, _2#18 AS country#22, _3#19 AS population#23]\n         +- LocalRelation [_1#17, _2#18, _3#19]\n\n== Analyzed Logical Plan ==\nfirst_name: string, city: string, country: string, population: double\nProject [first_name#5, city#6, country#22, population#23]\n+- Join Inner, (city#6 &lt;=&gt; city#21)\n   :- Project [_1#2 AS first_name#5, _2#3 AS city#6]\n   :  +- LocalRelation [_1#2, _2#3]\n   +- ResolvedHint isBroadcastable=true\n      +- Project [_1#17 AS city#21, _2#18 AS country#22, _3#19 AS population#23]\n         +- LocalRelation [_1#17, _2#18, _3#19]\n\n== Optimized Logical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- Join Inner, (city#6 &lt;=&gt; city#21)\n   :- LocalRelation [first_name#5, city#6]\n   +- ResolvedHint isBroadcastable=true\n      +- LocalRelation [city#21, country#22, population#23]\n\n== Physical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- BroadcastHashJoin [coalesce(city#6, )], [coalesce(city#21, )], Inner, BuildRight, (city#6 &lt;=&gt; city#21)\n   :- LocalTableScan [first_name#5, city#6]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], )))\n      +- LocalTableScan [city#21, country#22, population#23]\n</code></pre> <p>Notice how the parsed, analyzed, and optimized logical plans all contain <code>ResolvedHint isBroadcastable=true</code> because the <code>broadcast()</code> function was used. This hint isn't included when the <code>broadcast()</code> function isn't used.</p> <pre><code>peopleDF.join(\n  citiesDF,\n  peopleDF(\"city\") &lt;=&gt; citiesDF(\"city\")\n)\n  .drop(citiesDF(\"city\"))\n  .explain(true)\n</code></pre> <pre><code>== Parsed Logical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- Join Inner, (city#6 &lt;=&gt; city#21)\n   :- Project [_1#2 AS first_name#5, _2#3 AS city#6]\n   :  +- LocalRelation [_1#2, _2#3]\n   +- Project [_1#17 AS city#21, _2#18 AS country#22, _3#19 AS population#23]\n      +- LocalRelation [_1#17, _2#18, _3#19]\n\n== Analyzed Logical Plan ==\nfirst_name: string, city: string, country: string, population: double\nProject [first_name#5, city#6, country#22, population#23]\n+- Join Inner, (city#6 &lt;=&gt; city#21)\n   :- Project [_1#2 AS first_name#5, _2#3 AS city#6]\n   :  +- LocalRelation [_1#2, _2#3]\n   +- Project [_1#17 AS city#21, _2#18 AS country#22, _3#19 AS population#23]\n      +- LocalRelation [_1#17, _2#18, _3#19]\n\n== Optimized Logical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- Join Inner, (city#6 &lt;=&gt; city#21)\n   :- LocalRelation [first_name#5, city#6]\n   +- LocalRelation [city#21, country#22, population#23]\n\n== Physical Plan ==\nProject [first_name#5, city#6, country#22, population#23]\n+- BroadcastHashJoin [coalesce(city#6, )], [coalesce(city#21, )], Inner, BuildRight, (city#6 &lt;=&gt; city#21)\n   :- LocalTableScan [first_name#5, city#6]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, string, true], )))\n      +- LocalTableScan [city#21, country#22, population#23]\n</code></pre>"},{"location":"apache-spark/broadcast-joins/#next-steps","title":"Next steps","text":"<p>Broadcast joins are a great way to append data stored in relatively small single source of truth data files to large DataFrames. DataFrames up to 2GB can be broadcasted so a data file with tens or even hundreds of thousands of rows is a broadcast candidate.</p> <p>Broadcast joins are a powerful technique to have in your Apache Spark toolkit.</p> <p>Make sure to read up on broadcasting maps, another design pattern that's great for solving problems in distributed systems.</p>"},{"location":"apache-spark/broadcasting-maps-in-spark/","title":"Broadcasting Maps in Spark","text":"<p>Spark makes it easy to broadcast maps and perform hash lookups in a cluster computing environment.</p> <p>This post explains how to broadcast maps and how to use these broadcasted variables in analyses.</p>"},{"location":"apache-spark/broadcasting-maps-in-spark/#simple-example","title":"Simple example","text":"<p>Suppose you have an ArrayType column with a bunch of first names. You'd like to use a nickname map to standardize all of the first names.</p> <p>Here's how we'd write this code for a single Scala array.</p> <pre><code>import scala.util.Try\n\nval firstNames = Array(\"Matt\", \"Fred\", \"Nick\")\nval nicknames = Map(\"Matt\" -&gt; \"Matthew\", \"Nick\" -&gt; \"Nicholas\")\nval res = firstNames.map { (n: String) =&gt;\n  Try { nicknames(n) }.getOrElse(n)\n}\nres // equals Array(\"Matthew\", \"Fred\", \"Nicholas\")\n</code></pre> <p>Let's create a DataFrame with an ArrayType column that contains a list of first names and then append a <code>standardized_names</code> column that runs all the names through a Map.</p> <pre><code>import scala.util.Try\n\nval nicknames = Map(\"Matt\" -&gt; \"Matthew\", \"Nick\" -&gt; \"Nicholas\")\nval n = spark.sparkContext.broadcast(nicknames)\n\nval df = spark.createDF(\n  List(\n    (Array(\"Matt\", \"John\")),\n    (Array(\"Fred\", \"Nick\")),\n    (null)\n  ), List(\n    (\"names\", ArrayType(StringType, true), true)\n  )\n).withColumn(\n  \"standardized_names\",\n  array_map((name: String) =&gt; Try { n.value(name) }.getOrElse(name))\n    .apply(col(\"names\"))\n)\n\ndf.show(false)\n\n+------------+------------------+\n|names       |standardized_names|\n+------------+------------------+\n|[Matt, John]|[Matthew, John]   |\n|[Fred, Nick]|[Fred, Nicholas]  |\n|null        |null              |\n+------------+------------------+\n</code></pre> <p>We use the <code>spark.sparkContext.broadcast()</code> method to broadcast the <code>nicknames</code> map to all nodes in the cluster.</p> <p>Spark 2.4 added a <code>transform</code> method that's similar to the Scala <code>Array.map()</code> method, but this isn't easily accessible via the Scala API yet, so we map through all the array elements with the spark-daria <code>array_map</code> method.</p> <p>Note that we need to call <code>n.value()</code> to access the broadcasted value. This is slightly different than what's needed when writing vanilla Scala code.</p> <p>We have some code that works which is a great start. Let's clean this code up with some good Spark coding practices.</p>"},{"location":"apache-spark/broadcasting-maps-in-spark/#refactored-code","title":"Refactored code","text":"<p>Let's wrap the <code>withColumn</code> code in a Spark custom transformation, so it's more modular and easier to test.</p> <pre><code>val nicknames = Map(\"Matt\" -&gt; \"Matthew\", \"Nick\" -&gt; \"Nicholas\")\nval n = spark.sparkContext.broadcast(nicknames)\n\ndef withStandardizedNames(n: org.apache.spark.broadcast.Broadcast[Map[String, String]])(df: DataFrame) = {\n  df.withColumn(\n    \"standardized_names\",\n    array_map((name: String) =&gt; Try { n.value(name) }.getOrElse(name))\n      .apply(col(\"names\"))\n  )\n}\n\nval df = spark.createDF(\n  List(\n    (Array(\"Matt\", \"John\")),\n    (Array(\"Fred\", \"Nick\")),\n    (null)\n  ), List(\n    (\"names\", ArrayType(StringType, true), true)\n  )\n).transform(withStandardizedNames(n))\n\ndf.show(false)\n\n+------------+------------------+\n|names       |standardized_names|\n+------------+------------------+\n|[Matt, John]|[Matthew, John]   |\n|[Fred, Nick]|[Fred, Nicholas]  |\n|null        |null              |\n+------------+------------------+\n</code></pre> <p>The <code>withStandardizedNames()</code> transformation takes a <code>org.apache.spark.broadcast.Broadcast[Map[String, String]])</code> as an argument. We can pass our broadcasted Map around as a argument to functions. Scala is awesome.</p>"},{"location":"apache-spark/broadcasting-maps-in-spark/#building-maps-from-data-files","title":"Building Maps from data files","text":"<p>You can store the nickname data in a CSV file, convert it to a Map, and then broadcast the Map. It's typically best to store data in a CSV file instead of in a Map that lives in your codebase.</p> <p>Let's create a little CSV file with our nickname to firstname mappings.</p> <pre><code>nickname,firstname\nMatt,Matthew\nNick,Nicholas\n</code></pre> <p>Let's read the nickname CSV into a DataFrame, convert it to a Map, and then broadcast it.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DataFrameHelpers\n\nval nicknamesPath = new java.io.File(s\"./src/test/resources/nicknames.csv\").getCanonicalPath\n\nval nicknamesDF = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(nicknamesPath)\n\nval nicknames = DataFrameHelpers.twoColumnsToMap[String, String](\n  nicknamesDF,\n  \"nickname\",\n  \"firstname\"\n)\n\nval n = spark.sparkContext.broadcast(nicknames)\n\ndef withStandardizedNames(n: org.apache.spark.broadcast.Broadcast[Map[String, String]])(df: DataFrame) = {\n  df.withColumn(\n    \"standardized_names\",\n    array_map((name: String) =&gt; Try { n.value(name) }.getOrElse(name))\n      .apply(col(\"names\"))\n  )\n}\n\nval df = spark.createDF(\n  List(\n    (Array(\"Matt\", \"John\")),\n    (Array(\"Fred\", \"Nick\")),\n    (null)\n  ), List(\n    (\"names\", ArrayType(StringType, true), true)\n  )\n).transform(withStandardizedNames(n))\n\ndf.show(false)\n\n+------------+------------------+\n|names       |standardized_names|\n+------------+------------------+\n|[Matt, John]|[Matthew, John]   |\n|[Fred, Nick]|[Fred, Nicholas]  |\n|null        |null              |\n+------------+------------------+\n</code></pre> <p>This code uses the spark-daria <code>DataFrameHelpers.twoColumnsToMap()</code> method to convert the DataFrame to a Map. Use <code>spark-daria</code> whenever possible for these utility-type operations, so you don't need to reinvent the wheel.</p>"},{"location":"apache-spark/broadcasting-maps-in-spark/#conclusion","title":"Conclusion","text":"<p>You'll often want to broadcast small Spark DataFrames when making broadcast joins.</p> <p>This post illustrates how broadcasting Spark Maps is a powerful design pattern when writing code that executes on a cluster.</p> <p>Feel free to broadcast any variable to all the nodes in the cluster. You'll get huge performance gains whenever code is run in parallel on various nodes.</p>"},{"location":"apache-spark/building-jar-sbt/","title":"Building Spark JAR Files with SBT","text":"<p>Spark JAR files let you package a project into a single file so it can be run on a Spark cluster.</p> <p>A lot of developers develop Spark code in brower based notebooks because they're unfamiliar with JAR files. Scala is a difficult language and it's especially challenging when you can't leverage the development tools provided by an IDE like IntelliJ.</p> <p>This episode will demonstrate how to build JAR files with the SBT <code>package</code> and <code>assembly</code> commands and how to customize the code that's included in JAR files. Hopefully it will help you make the leap and start writing Spark code in SBT projects with a powerful IDE by your side!</p>"},{"location":"apache-spark/building-jar-sbt/#jar-file-basics","title":"JAR File Basics","text":"<p>A JAR (Java ARchive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file for distribution. - Wikipedia</p> <p>JAR files can be attached to Databricks clusters or launched via <code>spark-submit</code>.</p> <p>You can build a \"thin\" JAR file with the <code>sbt package</code> command. Thin JAR files only include the project's classes / objects / traits and don't include any of the project dependencies.</p> <p>You can build \"fat\" JAR files by adding sbt-assembly to your project. Fat JAR files inlude all the code from your project and all the code from the dependencies.</p> <p>Let's say you add the uJson library to your <code>build.sbt</code> file as a library dependency.</p> <pre><code>libraryDependencies += \"com.lihaoyi\" %% \"ujson\" % \"0.6.5\"\n</code></pre> <p>If you run <code>sbt package</code>, SBT will build a thin JAR file that only includes your project files. The thin JAR file will not include the uJson files.</p> <p>If you run <code>sbt assembly</code>, SBT will build a fat JAR file that includes both your project files and the uJson files.</p> <p>Let's dig into the gruesome details!</p>"},{"location":"apache-spark/building-jar-sbt/#building-a-thin-jar-file","title":"Building a Thin JAR File","text":"<p>As discussed, the <code>sbt package</code> builds a thin JAR file of your project.</p> <p>spark-daria is a good example of an open source project that is distributed as a thin JAR file. This is an excerpt of the spark-daria <code>build.sbt</code> file:</p> <pre><code>libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.0\" % \"provided\"\n\nlibraryDependencies += \"com.github.mrpowers\" % \"spark-fast-tests\" % \"2.3.0_0.8.0\" % \"test\"\nlibraryDependencies += \"com.lihaoyi\" %% \"utest\" % \"0.6.3\" % \"test\"\ntestFrameworks += new TestFramework(\"utest.runner.Framework\")\n\nartifactName := { (sv: ScalaVersion, module: ModuleID, artifact: Artifact) =&gt;\n  artifact.name + \"_\" + sv.binary + \"-\" + sparkVersion + \"_\" + module.revision + \".\" + artifact.extension\n}\n</code></pre> <p>Important take-aways:</p> <ul> <li>The \"provided\" string at the end of the <code>libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.0\" % \"provided\"</code> line indicates that the spark-sql dependency should be provided by the runtime environment that uses this JAR file.</li> <li>The \"test\" string at the end of the <code>libraryDependencies += \"com.github.mrpowers\" % \"spark-fast-tests\" % \"2.3.0_0.8.0\" % \"test\"</code> indicates that spark-fast-tests dependency is only for the test suite. The application code does not rely on spark-fast-tests, but spark-fast-tests is needed when running <code>sbt test</code>.</li> <li>The <code>artifactName := ...</code> line customizes the name of the JAR file created with the <code>sbt package</code> command. As discussed in the spark-style-guide, it's best to include the Scala version, Spark version, and project version in the JAR file name, so it's easier for your users to select the right JAR file for their projects.</li> </ul> <p>The <code>sbt package</code> command creates the <code>target/scala-2.11/spark-daria_2.11-2.3.0_0.19.0.jar</code> JAR file. We can use the <code>jar tvf</code> command to inspect the contents of the JAR file.</p> <pre><code>$ jar tvf target/scala-2.11/spark-daria_2.11-2.3.0_0.19.0.jar\n\n   255 Wed May 02 20:50:14 COT 2018 META-INF/MANIFEST.MF\n     0 Wed May 02 20:50:14 COT 2018 com/\n     0 Wed May 02 20:50:14 COT 2018 com/github/\n     0 Wed May 02 20:50:14 COT 2018 com/github/mrpowers/\n     0 Wed May 02 20:50:14 COT 2018 com/github/mrpowers/spark/\n     0 Wed May 02 20:50:14 COT 2018 com/github/mrpowers/spark/daria/\n     0 Wed May 02 20:50:14 COT 2018 com/github/mrpowers/spark/daria/sql/\n     0 Wed May 02 20:50:14 COT 2018 com/github/mrpowers/spark/daria/utils/\n  3166 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/DataFrameHelpers.class\n  1643 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/DataFrameHelpers$$anonfun$twoColumnsToMap$1.class\n   876 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/ColumnExt$.class\n  1687 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/transformations$$anonfun$1.class\n  3278 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/DataFrameColumnsChecker.class\n  3607 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/functions$$typecreator3$1.class\n  1920 Wed May 02 20:50:12 COT 2018 com/github/mrpowers/spark/daria/sql/DataFrameColumnsException$.class\n  ...\n  ...\n</code></pre> <p>The sbt-assembly plugin needs to be added to build fat JAR files that include the project's dependencies.</p>"},{"location":"apache-spark/building-jar-sbt/#building-a-fat-jar-file","title":"Building a Fat JAR File","text":"<p>spark-slack is a good example of a project that's distributed as a fat JAR file. The spark-slack JAR file includes all of the spark-slack code and all of the code in two external libraries (<code>net.gpedro.integrations.slack.slack-webhook</code> and <code>org.json4s.json4s-native</code>).</p> <p>Let's take a snippet from the spark-slack <code>build.sbt</code> file:</p> <pre><code>libraryDependencies ++= Seq(\n  \"net.gpedro.integrations.slack\" % \"slack-webhook\" % \"1.2.1\",\n  \"org.json4s\" %% \"json4s-native\" % \"3.3.0\"\n)\n\nlibraryDependencies += \"com.github.mrpowers\" % \"spark-daria\" % \"v2.3.0_0.18.0\" % \"test\"\nlibraryDependencies += \"com.github.mrpowers\" % \"spark-fast-tests\" % \"v2.3.0_0.7.0\" % \"test\"\nlibraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.0.1\" % \"test\"\n\nassemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)\nassemblyJarName in assembly := s\"${name.value}_${scalaBinaryVersion.value}-${sparkVersion.value}_${version.value}.jar\"\n</code></pre> <p>Important observations:</p> <ul> <li><code>\"net.gpedro.integrations.slack\" % \"slack-webhook\" % \"1.2.1\"</code> and <code>\"org.json4s\" %% \"json4s-native\" % \"3.3.0\"</code> aren't flagged at \"provided\" or \"test\" dependencies, so they will be included in the JAR file when <code>sbt assembly</code> is run.</li> <li><code>spark-daria</code>, <code>spark-fast-tests</code>, and <code>scalatest</code> are all flagged as \"test\" dependencies, so they won't be included in the JAR file.</li> <li><code>assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)</code> means that the Scala code itself should not be included in the JAR file. Your Spark runtime environment should already have Scala setup.</li> <li><code>assemblyJarName in assembly := s\"${name.value}_${scalaBinaryVersion.value}-${sparkVersion.value}_${version.value}.jar\"</code> customizes the JAR file name that's created by <code>sbt assembly</code>. Notice that <code>sbt package</code> and <code>sbt assembly</code> require different code to customize the JAR file name.</li> </ul> <p>Let's build the JAR file with <code>sbt assembly</code> and then inspect the content.</p> <pre><code>$ jar tvf target/scala-2.11/spark-slack_2.11-2.3.0_0.0.1.jar\n\n     0 Wed May 02 21:09:18 COT 2018 com/\n     0 Wed May 02 21:09:18 COT 2018 com/github/\n     0 Wed May 02 21:09:18 COT 2018 com/github/mrpowers/\n     0 Wed May 02 21:09:18 COT 2018 com/github/mrpowers/spark/\n     0 Wed May 02 21:09:18 COT 2018 com/github/mrpowers/spark/slack/\n     0 Wed May 02 21:09:18 COT 2018 com/github/mrpowers/spark/slack/slash_commands/\n     0 Wed May 02 21:09:18 COT 2018 com/google/\n     0 Wed May 02 21:09:18 COT 2018 com/google/gson/\n     0 Wed May 02 21:09:18 COT 2018 com/google/gson/annotations/\n     0 Wed May 02 21:09:18 COT 2018 com/google/gson/internal/\n     0 Wed May 02 21:09:18 COT 2018 com/google/gson/internal/bind/\n     0 Wed May 02 21:09:18 COT 2018 com/google/gson/reflect/\n     0 Wed May 02 21:09:18 COT 2018 com/google/gson/stream/\n     0 Wed May 02 21:09:18 COT 2018 com/thoughtworks/\n     0 Wed May 02 21:09:18 COT 2018 com/thoughtworks/paranamer/\n     0 Wed May 02 21:09:18 COT 2018 net/\n     0 Wed May 02 21:09:18 COT 2018 net/gpedro/\n     0 Wed May 02 21:09:18 COT 2018 net/gpedro/integrations/\n     0 Wed May 02 21:09:18 COT 2018 net/gpedro/integrations/slack/\n     0 Wed May 02 21:09:18 COT 2018 org/\n     0 Wed May 02 21:09:18 COT 2018 org/json4s/\n     0 Wed May 02 21:09:18 COT 2018 org/json4s/native/\n     0 Wed May 02 21:09:18 COT 2018 org/json4s/prefs/\n     0 Wed May 02 21:09:18 COT 2018 org/json4s/reflect/\n     0 Wed May 02 21:09:18 COT 2018 org/json4s/scalap/\n     0 Wed May 02 21:09:18 COT 2018 org/json4s/scalap/scalasig/\n  1879 Wed May 02 21:09:14 COT 2018 com/github/mrpowers/spark/slack/Notifier.class\n  1115 Wed May 02 21:09:14 COT 2018 com/github/mrpowers/spark/slack/SparkSessionWrapper$class.class\n   683 Wed May 02 21:09:14 COT 2018 com/github/mrpowers/spark/slack/SparkSessionWrapper.class\n  2861 Wed May 02 21:09:14 COT 2018 com/github/mrpowers/spark/slack/slash_commands/SlashParser.class\n  ...\n  ...\n</code></pre> <p><code>sbt assembly</code> provides us with the <code>com/github/mrpowers/spark/slack</code>, <code>net/gpedro/</code>, and <code>org/json4s/</code> as expected. But why does our fat JAR file include <code>com/google/gson/</code> code as well?</p> <p>If we look at the net.gpedro <code>pom.xml</code> file, we can see that the net.gpedro relies on com.google.code.gson:</p> <pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;\n    &lt;artifactId&gt;gson&lt;/artifactId&gt;\n    &lt;version&gt;${gson.version}&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>You'll want to be very careful to minimize your project dependencies. You'll also want to rely on external libraries that have minimal dependencies themselves as the dependies of a library quickly become your dependencies as soon as you add the library to your project.</p>"},{"location":"apache-spark/building-jar-sbt/#next-steps","title":"Next Steps","text":"<p>Make sure to always mark your <code>libraryDependencies</code> with \"provided\" or \"test\" whenever possible to keep your JAR files as thin as possible.</p> <p>Only add dependencies when it's absolutely required and try to avoid libraries that depend on a lot of other libraries.</p> <p>It's very easy to find yourself in dependency hell with Scala and you should proactively avoid this uncomfortable situation.</p> <p>Your Spark runtime environment should generally provide the Scala and Spark dependencies and you shouldn't include these in your JAR files.</p> <p>I fought long and hard to develop the <code>build.sbt</code> strategies outlined in this episode. Hopefully this will save you from some headache!</p>"},{"location":"apache-spark/chaining-custom-dataframe-transformations/","title":"Chaining Custom DataFrame Transformations in Spark","text":"<p><code>implicit classes</code> or the <code>Dataset#transform</code> method can be used to chain DataFrame transformations in Spark. This blog post will demonstrate how to chain DataFrame transformations and explain why the <code>Dataset#transform</code> method is preferred compared to <code>implicit classes</code>.</p> <p>Structuring Spark code as DataFrame transformations separates strong Spark programmers from \"spaghetti hackers\" as detailed in\u00a0Writing Beautiful Spark Code.\u00a0Following the blog post will make your Spark code much easier to test and reuse.</p> <p>If you\u2019re using PySpark, see this article on chaining custom PySpark DataFrame transformations.</p>"},{"location":"apache-spark/chaining-custom-dataframe-transformations/#dataset-transform-method","title":"Dataset Transform Method","text":"<p>The Dataset transform method provides a \u201cconcise syntax for chaining custom transformations.\u201d</p> <p>Suppose we have a <code>withGreeting()</code> method that appends a greeting column to a DataFrame and a <code>withFarewell()</code> method that appends a farewell column to a DataFrame.</p> <pre><code>def withGreeting(df: DataFrame): DataFrame = {\n  df.withColumn(\"greeting\", lit(\"hello world\"))\n}\n\ndef withFarewell(df: DataFrame): DataFrame = {\n  df.withColumn(\"farewell\", lit(\"goodbye\"))\n}\n</code></pre> <p>We can use the transform method to run the <code>withGreeting()</code> and <code>withFarewell()</code> methods.</p> <pre><code>val df = Seq(\n  \"funny\",\n  \"person\"\n).toDF(\"something\")\n\nval weirdDf = df\n  .transform(withGreeting)\n  .transform(withFarewell)\n</code></pre> <pre><code>weirdDf.show()\n\n+---------+-----------+--------+\n|something|   greeting|farewell|\n+---------+-----------+--------+\n|    funny|hello world| goodbye|\n|   person|hello world| goodbye|\n+---------+-----------+--------+\n</code></pre> <p>The transform method can easily be chained with built-in Spark DataFrame methods, like select.</p> <pre><code>df\n  .select(\"something\")\n  .transform(withGreeting)\n  .transform(withFarewell)\n</code></pre> <p>If the transform method is not used then we need to nest method calls and the code becomes less readable.</p> <pre><code>withFarewell(withGreeting(df))\n\n// even worse\nwithFarewell(withGreeting(df)).select(\"something\")\n</code></pre>"},{"location":"apache-spark/chaining-custom-dataframe-transformations/#transform-method-with-arguments","title":"Transform Method with Arguments","text":"<p>Custom DataFrame transformations that take arguments can also use the transform method by leveraging currying / multiple parameter lists in Scala.</p> <p>Let\u2019s use the same <code>withGreeting()</code> method from earlier and add a <code>withCat()</code> method that takes a string as an argument.</p> <pre><code>def withGreeting(df: DataFrame): DataFrame = {\n  df.withColumn(\"greeting\", lit(\"hello world\"))\n}\n\ndef withCat(name: String)(df: DataFrame): DataFrame = {\n  df.withColumn(\"cats\", lit(s\"$name meow\"))\n}\n</code></pre> <p>We can use the transform method to run the <code>withGreeting()</code> and <code>withCat()</code> methods.</p> <pre><code>val df = Seq(\n  \"funny\",\n  \"person\"\n).toDF(\"something\")\n\nval niceDf = df\n  .transform(withGreeting)\n  .transform(withCat(\"puffy\"))\n</code></pre> <pre><code>niceDf.show()\n\n+---------+-----------+----------+\n|something|   greeting|      cats|\n+---------+-----------+----------+\n|    funny|hello world|puffy meow|\n|   person|hello world|puffy meow|\n+---------+-----------+----------+\n</code></pre> <p>The transform method can be used for custom DataFrame transformations that take arguments as well!</p>"},{"location":"apache-spark/chaining-custom-dataframe-transformations/#monkey-patching-with-implicit-classes","title":"Monkey Patching with Implicit Classes","text":"<p>Implicit classes can be used to add methods to existing classes. The following code adds the same <code>withGreeting()</code> and <code>withFarewell()</code> methods to the DataFrame class itself.</p> <pre><code>object BadImplicit {\n\n  implicit class DataFrameTransforms(df: DataFrame) {\n\n    def withGreeting(): DataFrame = {\n      df.withColumn(\"greeting\", lit(\"hello world\"))\n    }\n\n    def withFarewell(): DataFrame = {\n      df.withColumn(\"farewell\", lit(\"goodbye\"))\n    }\n\n  }\n\n}\n</code></pre> <p>The <code>withGreeting()</code> and <code>withFarewell()</code> methods can be chained and executed as follows.</p> <pre><code>import BadImplicit._\n\nval df = Seq(\n  \"funny\",\n  \"person\"\n).toDF(\"something\")\n\nval hiDf = df.withGreeting().withFarewell()\n</code></pre> <p>Extending core classes works, but it's is poor programming practice that should be avoided.</p>"},{"location":"apache-spark/chaining-custom-dataframe-transformations/#avoiding-implicit-classes","title":"Avoiding Implicit Classes","text":"<p>Changing base classes is known as monkey patching and is a delightful feature of Ruby but can be perilous in untutored hands. \u2014 Sandi Metz</p> <p>Sandi\u2019s comment was aimed at the Ruby programming language, but the same principle applies to Scala implicit classes.</p> <p>Monkey patching in generally frowned upon in the Ruby community and should be discouraged in Scala.</p> <p>Spark was nice enough to provide a transform method so you don\u2019t need to monkey patch the DataFrame class. With some clever Scala programming, we can even make the transform method work with custom transformations that take arguments. This makes the transform method the clear winner!</p>"},{"location":"apache-spark/column-equality/","title":"Spark Column Equality","text":"<p>The term \"column equality\" refers to two different things in Spark:</p> <ol> <li>When a column is equal to a particular value (typically when filtering)</li> <li>When all the values in two columns are equal for all rows in the dataset (especially common when testing)</li> </ol> <p>This blog post will explore both types of Spark column equality.</p>"},{"location":"apache-spark/column-equality/#column-equality-for-filtering","title":"Column equality for filtering","text":"<p>Suppose you have a DataFrame with <code>team_name</code>, <code>num_championships</code>, and <code>state</code> columns.</p> <p>Here's how you can filter to only show the teams from TX (short for Texas).</p> <pre><code>df.filter(df(\"state\") === \"TX\")\n</code></pre> <p>Here's a sample dataset that you can paste into a Spark console to verify this result yourself.</p> <pre><code>val df = Seq(\n  (\"Rockets\", 2, \"TX\"),\n  (\"Warriors\", 6, \"CA\"),\n  (\"Spurs\", 5, \"TX\"),\n  (\"Knicks\", 2, \"NY\")\n).toDF(\"team_name\", \"num_championships\", \"state\")\n</code></pre> <p>Let's filter the DataFrame and make sure it only includes the teams from TX.</p> <pre><code>df.filter(df(\"state\") === \"TX\").show()\n\n+---------+-----------------+-----+\n|team_name|num_championships|state|\n+---------+-----------------+-----+\n|  Rockets|                2|   TX|\n|    Spurs|                5|   TX|\n+---------+-----------------+-----+\n</code></pre> <p>Writing Beautiful Spark Code is the best way to learn about filtering, including the main pitfall to avoid when filtering. Read the book to filter effectively.</p>"},{"location":"apache-spark/column-equality/#more-on-sparks-column-class","title":"More on Spark's Column class","text":"<p>You'll use the Spark Column class all the time and it's good to understand how it works.</p> <p>Here's the method signature for the <code>===</code> method defined in the Column class.</p> <pre><code>def ===(other: Any): Column\n</code></pre> <p>The <code>===</code> takes Any object as an argument and returns a Column.</p> <p>In <code>df(\"state\") === \"TX\"</code>, the <code>===</code> method is supplied a string argument. It can also be supplied a <code>Column</code> argument.</p> <pre><code>import org.apache.spark.sql.functions.lit\n\ndf.filter(df(\"state\") === lit(\"TX\")).show()\n</code></pre> <p>Scala methods can be invoked with spaces instead of dot notation. This is an example of syntactic sugar.</p> <p>We can use <code>df(\"state\").===(lit(\"TX\"))</code> to avoid syntactic sugar and invoke the <code>===</code> method with standard dot notation.</p> <pre><code>df.filter(df(\"state\").===(\"TX\")).show()\n\n+---------+-----------------+-----+\n|team_name|num_championships|state|\n+---------+-----------------+-----+\n|  Rockets|                2|   TX|\n|    Spurs|                5|   TX|\n+---------+-----------------+-----+\n</code></pre> <p>We could also use the <code>equalTo()</code> Column method that behaves like <code>===</code>:</p> <pre><code>df.filter(df(\"state\").equalTo(\"TX\")).show()\n</code></pre> <p>Study the Column methods to become a better Spark programmer!</p>"},{"location":"apache-spark/column-equality/#naive-column-equality","title":"Naive Column Equality","text":"<p>Let's create a DataFrame with <code>num</code> and <code>is_even_hardcoded</code> columns.</p> <pre><code>val df = Seq(\n  (2, true),\n  (6, true),\n  (5, false)\n).toDF(\"num\", \"is_even_hardcoded\")\n</code></pre> <pre><code>df.show()\n\n+---+-----------------+\n|num|is_even_hardcoded|\n+---+-----------------+\n|  2|             true|\n|  6|             true|\n|  5|            false|\n+---+-----------------+\n</code></pre> <p>Let's create a custom DataFrame transformation called <code>isEven</code> that'll return <code>true</code> if the number is even and <code>false</code> otherwise.</p> <pre><code>import org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\n\ndef isEven(inputColName: String, outputColName: String)(df: DataFrame) = {\n  df.withColumn(outputColName, col(inputColName) % 2 === lit(0))\n}\n</code></pre> <p>Check out Writing Beautiful Spark Code if this blog post is moving too fast and you need a gradual introduction to topics like DataFrame transformation functions.</p> <p>Let's run the <code>isEven()</code> function and visually verify that the hardcoded and programatic results are the same.</p> <pre><code>df.transform(isEven(\"num\", \"is_even\")).show()\n\n+---+-----------------+-------+\n|num|is_even_hardcoded|is_even|\n+---+-----------------+-------+\n|  2|             true|   true|\n|  6|             true|   true|\n|  5|            false|  false|\n+---+-----------------+-------+\n</code></pre> <p>Visually verifying column equality isn't good for big datasets or automated processes.</p> <p>Let's get this full DataFrame stored in a new variable, so we don't need to keep on running the <code>isEven</code> function.</p> <pre><code>val fullDF = df.transform(isEven(\"num\", \"is_even\"))\n</code></pre> <p>We can use the <code>ColumnComparer</code> trait defined in spark-fast-tests to verify column equality.</p> <pre><code>import com.github.mrpowers.spark.fast.tests.ColumnComparer\n\nassertColEquality(df, \"is_even_hardcoded\", \"is_even\")\n</code></pre> <p>When you're writing unit tests, you'll definitely want to use the spark-fast-tests library.</p> <p>Let's hack together some code that'll return <code>true</code> if two columns are equal.</p> <pre><code>def areColumnsEqual(df: DataFrame, colName1: String, colName2: String) = {\n  val elements = df\n    .select(colName1, colName2)\n    .collect()\n  val c1 = elements.map(_(0))\n  val c2 = elements.map(_(1))\n  c1.sameElements(c2)\n}\n\nareColumnsEqual(fullDF, \"is_even_hardcoded\", \"is_even\") // true\n</code></pre> <p>This function doesn't explain all the edge cases, but it's a good start!</p>"},{"location":"apache-spark/column-equality/#column-equality-with","title":"Column equality with ===","text":"<p>We can also evaluate column equality by comparing both columns with the <code>===</code> operator and making sure all values evaluate to <code>true</code>.</p> <p>As a reminder, here are the contents of <code>fullDF</code>:</p> <pre><code>fullDF.show()\n\n+---+-----------------+-------+\n|num|is_even_hardcoded|is_even|\n+---+-----------------+-------+\n|  2|             true|   true|\n|  6|             true|   true|\n|  5|            false|  false|\n+---+-----------------+-------+\n</code></pre> <p>Let's append a column to <code>fullDF</code> that returns <code>true</code> if <code>is_even_hardcoded</code> and <code>is_even</code> are equal:</p> <pre><code>fullDF\n  .withColumn(\"are_cols_equal\", $\"is_even_hardcoded\" === $\"is_even\")\n  .show()\n\n+---+-----------------+-------+--------------+\n|num|is_even_hardcoded|is_even|are_cols_equal|\n+---+-----------------+-------+--------------+\n|  2|             true|   true|          true|\n|  6|             true|   true|          true|\n|  5|            false|  false|          true|\n+---+-----------------+-------+--------------+\n</code></pre> <p>Let's write a function to verify that all the values in a given column are true.</p> <pre><code>def areAllRowsTrue(df: DataFrame, colName: String) = {\n  val elements = df\n    .select(colName)\n    .collect()\n  val c1 = elements.map(_(0))\n  c1.forall(_ == true)\n}\n</code></pre> <p>Let's verify that <code>areAllRowsTrue</code> returns <code>true</code> for the <code>are_cols_equal</code> column.</p> <pre><code>areAllRowsTrue(\n  fullDF.withColumn(\"are_cols_equal\", $\"is_even_hardcoded\" === $\"is_even\"),\n  \"are_cols_equal\"\n) // true\n</code></pre> <p>This <code>===</code> approach unfortunately doesn't work for all column types.</p>"},{"location":"apache-spark/column-equality/#arraytype-column-equality","title":"ArrayType Column Equality","text":"<p>Let's create a <code>numbersDF</code> with two ArrayType columns that contain integers.</p> <pre><code>val numbersDF = Seq(\n  (Seq(1, 2), Seq(1, 2)),\n  (Seq(3, 3), Seq(3, 3)),\n  (Seq(3, 3), Seq(6, 6))\n).toDF(\"nums1\", \"nums2\")\n</code></pre> <p>We can use <code>===</code> to assess ArrayType column equality.</p> <pre><code>numbersDF\n  .withColumn(\"nah\", $\"nums1\" === $\"nums2\")\n  .show()\n\n+------+------+-----+\n| nums1| nums2|  nah|\n+------+------+-----+\n|[1, 2]|[1, 2]| true|\n|[3, 3]|[3, 3]| true|\n|[3, 3]|[6, 6]|false|\n+------+------+-----+\n</code></pre> <p>Let's see if <code>===</code> works for nested arrays.</p> <p>Start by making a <code>fakeDF</code> DataFrame with two nested array columns.</p> <pre><code>val fakeDF = Seq(\n  (Seq(Seq(1, 2)), Seq(Seq(1, 2))),\n  (Seq(Seq(1, 3)), Seq(Seq(1, 2)))\n).toDF(\"nums1\", \"nums2\")\n</code></pre> <p>The code below confirms that the <code>===</code> operator can handle deep column comparisons gracefully.</p> <pre><code>fakeDF\n  .withColumn(\"nah\", $\"nums1\" === $\"nums2\")\n  .show()\n\n+--------+--------+-----+\n|   nums1|   nums2|  nah|\n+--------+--------+-----+\n|[[1, 2]]|[[1, 2]]| true|\n|[[1, 3]]|[[1, 2]]|false|\n+--------+--------+-----+\n</code></pre> <p>But watch out, the <code>===</code> operator doesn't work for all complex column types.</p>"},{"location":"apache-spark/column-equality/#maptype-column-equality","title":"MapType Column Equality","text":"<p>The <code>===</code> operator does not work for MapType columns.</p> <pre><code>val mapsDF = Seq(\n  (Map(\"one\" -&gt; 1), Map(\"one\" -&gt; 1))\n).toDF(\"m1\", \"m2\")\n</code></pre> <pre><code>mapsDF.withColumn(\"maps_equal\", $\"m1\" === $\"m2\").show()\n\norg.apache.spark.sql.AnalysisException: cannot resolve '(m1 = m2)' due to data type mismatch: EqualTo does not support ordering on type map&lt;string,int&gt;;;\nProject [m1#373, m2#374, (m1#373 = m2#374) AS maps_equal#384]\n Project [_1#370 AS m1#373, _2#371 AS m2#374]\n    LocalRelation [_1#370, _2#371]\n\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:115)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.immutable.List.map(List.scala:296)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3407)\n  at org.apache.spark.sql.Dataset.select(Dataset.scala:1335)\n  at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2253)\n  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2220)\n  ... 51 elided\n</code></pre> <p>We need to use different tactics for MapType column equality.</p> <p>The Scala <code>==</code> operator can successfully compare maps:</p> <pre><code>Map(\"one\" -&gt; 1) == Map(\"one\" -&gt; 1) // true\n</code></pre> <p>The <code>sameElements</code> Scala method also works:</p> <pre><code>Map(\"one\" -&gt; 1) sameElements Map(\"one\" -&gt; 1)\n</code></pre> <p>Recall that <code>sameElements</code> was used in the <code>areColumnsEqual</code> method we defined earlier:</p> <pre><code>def areColumnsEqual(df: DataFrame, colName1: String, colName2: String) = {\n  val elements = df\n    .select(colName1, colName2)\n    .collect()\n  val c1 = elements.map(_(0))\n  val c2 = elements.map(_(1))\n  c1.sameElements(c2)\n}\n</code></pre> <p>So <code>areColumnsEqual</code> will also work for comparing MapType columns:</p> <pre><code>areColumnsEqual(mapsDF, \"m1\", \"m2\") // true\n</code></pre>"},{"location":"apache-spark/column-equality/#structtype-column-equality","title":"StructType Column Equality","text":"<p>That's a whole new can of worms!!</p>"},{"location":"apache-spark/column-equality/#testing-with-column-equality","title":"Testing with Column Equality","text":"<p>Column equality is useful when writing unit tests.</p> <p>Testing Spark Applications is the best way to learn how to test your Spark code. This section provides a great introduction, but you should really read [the book](Testing Spark Applications) if you'd like to accelerate your learning process.</p> <p>You'll want to leverage a library like spark-fast-tests with column comparison methods, so you don't need to write them yourself.</p> <p>Let's look at how spark-daria uses the spark-fast-tests <code>assertColumnEquality</code> method to test the <code>removeNonWordCharacters()</code> function that removes all the non-word characters from a string.</p> <pre><code>def removeNonWordCharacters(col: Column): Column = {\n  regexp_replace(col, \"[^\\\\w\\\\s]+\", \"\")\n}\n</code></pre> <p>Let's write a test to make sure the <code>removeNonWordCharacters</code> converts <code>\"Bruce &amp;&amp;**||ok88\"</code> to <code>\"Bruce ok88\"</code>, <code>\"55 oba&amp;&amp;&amp;ma\"</code> to <code>\"55 obama\"</code>, etc.</p> <p>Here's the test:</p> <pre><code>import utest._\n\nimport com.github.mrpowers.spark.fast.tests.ColumnComparer\n\nobject FunctionsTest extends TestSuite with ColumnComparer with SparkSessionTestWrapper {\n  val tests = Tests {\n    'removeNonWordCharacters - {\n      \"removes all non-word characters from a string, excluding whitespace\" - {\n        val df = spark\n          .createDF(\n            List(\n              (\"Bruce &amp;&amp;**||ok88\", \"Bruce ok88\"),\n              (\"55    oba&amp;&amp;&amp;ma\", \"55    obama\"),\n              (\"  ni!!ce  h^^air person  \", \"  nice  hair person  \"),\n              (null, null)\n            ),\n            List(\n              (\"some_string\", StringType, true),\n              (\"expected\", StringType, true)\n            )\n          )\n          .withColumn(\n            \"some_string_remove_non_word_chars\",\n            functions.removeNonWordCharacters(col(\"some_string\"))\n          )\n\n        assertColumnEquality(\n          df,\n          \"some_string_remove_non_word_chars\",\n          \"expected\"\n        )\n      }\n    }\n  }\n}\n</code></pre> <p>We hardcode the expected result in the DataFrame and add the <code>some_string_remove_non_word_chars</code> column by running the <code>removeNonWordCharacters()</code> function.</p> <p>The <code>assertColumnEquality</code> method that's defined in spark-fast-tests verifies the <code>some_string_remove_non_word_chars</code> and <code>expected</code> are equal.</p> <p>This is how you should design most of your Spark unit tests.</p>"},{"location":"apache-spark/column-equality/#conclusion","title":"Conclusion","text":"<p>Spark column equality is a surprisingly deep topic\u2026 we haven't even covered all the edge cases!</p> <p>Make sure you understand how column comparisons work at a high level.</p> <p>Use spark-fast-tests to write elegant tests and abstract column comparison details from your codebase.</p> <p>Studying the spark-fast-tests codebase is a great way to learn more about Spark!</p>"},{"location":"apache-spark/column-methods/","title":"Exploring Spark's Column Methods","text":"<p>The Spark Column class defines a variety of column methods that are vital for manipulating DataFrames.</p> <p>This blog post demonstrates how to instantiate Column objects and covers the commonly used Column methods.</p>"},{"location":"apache-spark/column-methods/#a-simple-example","title":"A simple example","text":"<p>Let's create a little DataFrame with superheros and their city of origin.</p> <pre><code>val df = Seq(\n  (\"thor\", \"new york\"),\n  (\"aquaman\", \"atlantis\"),\n  (\"wolverine\", \"new york\")\n).toDF(\"superhero\", \"city\")\n</code></pre> <p>Let's use the <code>startsWith()</code> column method to identify all cities that start with the word <code>new</code>:</p> <pre><code>df\n  .withColumn(\"city_starts_with_new\", $\"city\".startsWith(\"new\"))\n  .show()\n</code></pre> <pre><code>+---------+--------+--------------------+\n|superhero|    city|city_starts_with_new|\n+---------+--------+--------------------+\n|     thor|new york|                true|\n|  aquaman|atlantis|               false|\n|wolverine|new york|                true|\n+---------+--------+--------------------+\n</code></pre> <p>A Column object is instantiated with the <code>$\"city\"</code> statement. Let's look at all the different ways to create Column objects.</p>"},{"location":"apache-spark/column-methods/#instantiating-column-objects","title":"Instantiating Column objects","text":"<p>Column objects must be created to run Column methods.</p> <p>A Column object corresponding with the <code>city</code> column can be created using the following three syntaxes:</p> <ol> <li><code>$\"city\"</code></li> <li><code>df(\"city\")</code></li> <li><code>col(\"city\")</code> (must run <code>import org.apache.spark.sql.functions.col</code> first)</li> </ol> <p>Column objects are commonly passed as arguments to SQL functions (e.g. <code>upper($\"city\")</code>).</p> <p>We will create column objects in all the examples that follow.</p>"},{"location":"apache-spark/column-methods/#gt","title":"<code>gt()</code>","text":"<p>Let's create a DataFrame with an integer column so we can run some numerical column methods.</p> <pre><code>val df = Seq(\n  (10, \"cat\"),\n  (4, \"dog\"),\n  (7, null)\n).toDF(\"num\", \"word\")\n</code></pre> <p>Let's use the <code>gt()</code> method to identify all rows with a <code>num</code> greater than five.</p> <pre><code>df\n  .withColumn(\"num_gt_5\", col(\"num\").gt(5))\n  .show()\n</code></pre> <pre><code>+---+----+--------+\n|num|word|num_gt_5|\n+---+----+--------+\n| 10| cat|    true|\n|  4| dog|   false|\n|  7|null|    true|\n+---+----+--------+\n</code></pre>"},{"location":"apache-spark/column-methods/#substr","title":"<code>substr()</code>","text":"<p>Let's use the <code>substr()</code> method to create a new column with the first two letters of the <code>word</code> column.</p> <pre><code>df\n  .withColumn(\"word_first_two\", col(\"word\").substr(0, 2))\n  .show()\n</code></pre> <pre><code>+---+----+--------------+\n|num|word|word_first_two|\n+---+----+--------------+\n| 10| cat|            ca|\n|  4| dog|            do|\n|  7|null|          null|\n+---+----+--------------+\n</code></pre> <p>Notice that the <code>substr()</code> method returns <code>null</code> when it's supplied <code>null</code> as input. All other Column methods and SQL functions behave similarly (i.e. they return <code>null</code> when the input is <code>null</code>).</p>"},{"location":"apache-spark/column-methods/#operator","title":"<code>+</code> operator","text":"<p>Let's use the <code>+</code> operator to add five to the <code>num</code> column.</p> <pre><code>df\n  .withColumn(\"num_plus_five\", col(\"num\").+(5))\n  .show()\n</code></pre> <pre><code>+---+----+-------------+\n|num|word|num_plus_five|\n+---+----+-------------+\n| 10| cat|           15|\n|  4| dog|            9|\n|  7|null|           12|\n+---+----+-------------+\n</code></pre> <p>We can also skip the dot notation when invoking the function.</p> <pre><code>df\n  .withColumn(\"num_plus_five\", col(\"num\") + 5)\n  .show()\n</code></pre> <p>The syntactic sugar makes it harder to see that <code>+</code> is a method defined in the Column class. Take a look at the docs to convince yourself!</p>"},{"location":"apache-spark/column-methods/#lit","title":"<code>lit()</code>","text":"<p>Let's use the <code>/</code> method to take two divided by the <code>num</code> column.</p> <pre><code>df\n  .withColumn(\"two_divided_by_num\", lit(2) / col(\"num\"))\n  .show()\n</code></pre> <pre><code>+---+----+------------------+\n|num|word|two_divided_by_num|\n+---+----+------------------+\n| 10| cat|               0.2|\n|  4| dog|               0.5|\n|  7|null|0.2857142857142857|\n+---+----+------------------+\n</code></pre> <p>Notice that the <code>lit()</code> function must be used to convert two into a Column object before the division can take place.</p> <pre><code>df\n  .withColumn(\"two_divided_by_num\", 2 / col(\"num\"))\n  .show()\n</code></pre> <p>Here is the error message:</p> <pre><code>notebook:2: error: overloaded method value / with alternatives:\n  (x: Double)Double &lt;and&gt;\n  (x: Float)Float &lt;and&gt;\n  (x: Long)Long &lt;and&gt;\n  (x: Int)Int &lt;and&gt;\n  (x: Char)Int &lt;and&gt;\n  (x: Short)Int &lt;and&gt;\n  (x: Byte)Int\n cannot be applied to (org.apache.spark.sql.Column)\n  .withColumn(\"two_divided_by_num\", 2 / col(\"num\"))\n</code></pre> <p>The <code>/</code> method is defined in both the Scala Int and Spark Column classes. We need to convert the number to a Column object, so the compiler knows to use the <code>/</code> method defined in the Spark Column class. Upon analyzing the error message, we can see that the compiler is mistakenly trying to use the <code>/</code> operator defined in the Scala Int class.</p>"},{"location":"apache-spark/column-methods/#isnull","title":"<code>isNull</code>","text":"<p>Let's use the <code>isNull</code> method to identify the rows with a <code>word</code> of <code>null</code>.</p> <pre><code>df\n  .withColumn(\"word_is_null\", col(\"word\").isNull)\n  .show()\n</code></pre> <pre><code>+---+----+------------+\n|num|word|word_is_null|\n+---+----+------------+\n| 10| cat|       false|\n|  4| dog|       false|\n|  7|null|        true|\n+---+----+------------+\n</code></pre>"},{"location":"apache-spark/column-methods/#isnotnull","title":"<code>isNotNull</code>","text":"<p>Let's use the <code>isNotNull</code> method to filter out all rows with a <code>word</code> of <code>null</code>.</p> <pre><code>df\n  .where(col(\"word\").isNotNull)\n  .show()\n</code></pre> <pre><code>+---+----+\n|num|word|\n+---+----+\n| 10| cat|\n|  4| dog|\n+---+----+\n</code></pre>"},{"location":"apache-spark/column-methods/#when-otherwise","title":"<code>when</code> / <code>otherwise</code>","text":"<p>Let's create a final DataFrame with <code>word1</code> and <code>word2</code> columns, so we can play around with the <code>===</code>, <code>when()</code>, and <code>otherwise()</code> methods</p> <pre><code>val df = Seq(\n  (\"bat\", \"bat\"),\n  (\"snake\", \"rat\"),\n  (\"cup\", \"phone\"),\n  (\"key\", null)\n).toDF(\"word1\", \"word2\")\n</code></pre> <p>Let's write a little word comparison algorithm that analyzes the differences between the two words.</p> <pre><code>import org.apache.spark.sql.functions._\n\ndf\n  .withColumn(\n    \"word_comparison\",\n    when($\"word1\" === $\"word2\", \"same words\")\n      .when(length($\"word1\") &gt; length($\"word2\"), \"word1 is longer\")\n      .otherwise(\"i am confused\")\n  ).show()\n</code></pre> <pre><code>+-----+-----+---------------+\n|word1|word2|word_comparison|\n+-----+-----+---------------+\n|  bat|  bat|     same words|\n|snake|  rat|word1 is longer|\n|  cup|phone|  i am confused|\n|  key| null|  i am confused|\n+-----+-----+---------------+\n</code></pre> <p><code>when()</code> and <code>otherwise()</code> are how to write <code>if</code> / <code>else if</code> / <code>else</code> logic in Spark.</p>"},{"location":"apache-spark/column-methods/#next-steps","title":"Next steps","text":"<p>Spark's Colum methods are frequently used and mastery of this class is vital.</p> <p>Scala syntactic sugar can make it difficult for programmers without a lot of object oriented experience to have difficulty identifying when Column methods are invoked (and differenting them from SQL functions).</p> <p>Keep reading though this post till you've mastered all these vital concepts!</p>"},{"location":"apache-spark/compacting-files/","title":"Compacting Files with Spark to Address the Small File Problem","text":"<p>Spark runs slowly when it reads data from a lot of small files in S3.  You can make your Spark code run faster by creating a job that compacts small files into larger files.</p> <p>Open table formats like Delta Lake, Hudi, and Iceberg have built-in file compaction utilities.  It's best to simply use the built-in functionality in an open table format.</p> <p>Here's what to do if you're stuck with a Parquet data lake that cannot be converted to an open table:</p> <ol> <li>you can temporarily convert it to a Delta table</li> <li>compact the small files</li> <li>run a vacuum operation to clean up the old files</li> <li>convert back to a Parquet data lake</li> </ol> <p>The rest of this post explains how to manually comact small files, but you should not do this anymore.  Using an open table format with reliable transactions is much better.</p> <p>The \u201csmall file problem\u201d is especially problematic for data stores that are updated incrementally. The small problem get progressively worse if the incremental updates are more frequent and the longer incremental updates run between full refreshes.</p> <p>Garren Staubli wrote a great blog does a great job explaining why small files are a big problem for Spark analyses. This blog will describe how to get rid of small files using Spark.</p>"},{"location":"apache-spark/compacting-files/#simple-example","title":"Simple example","text":"<p>Let's look at a folder with some small files (we'd like all the files in our data lake to be 1GB):</p> <ul> <li>File A: 0.5 GB</li> <li>File B: 0.5 GB</li> <li>File C: 0.1 GB</li> <li>File D: 0.2 GB</li> <li>File E: 0.3 GB</li> <li>File F: 1 GB</li> <li>File G: 1 GB</li> <li>File H: 1 GB</li> </ul> <p>Our folder has 4.6 GB of data.</p> <p>Let's use the <code>repartition()</code> method to shuffle the data and write it to another directory with five 0.92 GB files.</p> <pre><code>val df = spark.read.parquet(\"s3_path_with_the_data\")\nval repartitionedDF = df.repartition(5)\nrepartitionedDF.write.parquet(\"another_s3_path\")\n</code></pre> <p>The <code>repartition()</code> method makes it easy to build a folder with equally sized files.</p>"},{"location":"apache-spark/compacting-files/#only-repartitioning-the-small-files","title":"Only repartitioning the small files","text":"<p>Files F, G, and H are already perfectly sized, so it'll be more performant to simply repartition Files A, B, C, D, and E (the small files).</p> <p>The small files contain 1.6 GB of data. We can read in the small files, write out 2 files with 0.8 GB of data each, and then delete all the small files. Let's take a look at some pseudocode.</p> <pre><code>val df = spark.read.parquet(\"fileA, fileB, fileC, fileD, fileE\")\nval newDF = df.repartition(2)\nnewDF.write.parquet(\"s3_path_with_the_data\")\n// run a S3 command to delete fileA, fileB, fileC, fileD, fileE\n</code></pre> <p>Here's what <code>s3_path_with_the_data</code> will look like after the small files have been compacted.</p> <ul> <li>File F: 1 GB</li> <li>File G: 1 GB</li> <li>File H: 1 GB</li> <li>File I: 0.8 GB</li> <li>File J: 0.8 GB</li> </ul> <p>This approach is nice because the data isn't written to a new directory. All of our code that references with <code>s3_path_with_the_data</code> will still work.</p>"},{"location":"apache-spark/compacting-files/#real-code-to-repartition-the-small-files","title":"Real code to repartition the small files","text":"<p>Kaggle has an open source CSV hockey dataset called <code>game_shifts.csv</code> that has 5.56 million rows of data and 5 columns.</p> <p>Let's split up this CSV into 6 separate files and store them in the <code>nhl_game_shifts</code> S3 directory:</p> <ul> <li>game_shiftsA.csv: 68.7 MB</li> <li>game_shiftsB.csv: 68.7 MB</li> <li>game_shiftsC.csv: 51.5 MB</li> <li>game_shiftsD.csv: 1.0 MB</li> <li>game_shiftsE.csv: 0.5 MB</li> <li>game_shiftsF.csv: 0.7 MB</li> </ul> <p>Let's read game_shiftsC, game_shiftsD, game_shiftsE, and game_shiftsF into a DataFrame, shuffle the data to a single partition, and write out the data as a single file.</p> <pre><code>import org.apache.spark.sql.SaveMode\n\nval smallFilesDF = spark.read\n.option(\"header\", \"true\")\n.csv(s\"/mnt/some-bucket/nhl_game_shifts/{game_shiftsC.csv,game_shiftsD.csv,game_shiftsE.csv,game_shiftsF.csv}\")\n\nsmallFilesDF\n.repartition(1)\n.write\n.mode(SaveMode.Append)\n.csv(\"/mnt/some-bucket/nhl_game_shifts\")\n</code></pre> <p>Let's run some AWS CLI commands to delete files C, D, E, and F.</p> <pre><code>aws s3 rm s3://some-bucket/nhl_game_shifts/game_shiftsC.csv\naws s3 rm s3://some-bucket/nhl_game_shifts/game_shiftsD.csv\naws s3 rm s3://some-bucket/nhl_game_shifts/game_shiftsE.csv\naws s3 rm s3://some-bucket/nhl_game_shifts/game_shiftsF.csv\n</code></pre> <p>Here's what <code>s3://some-bucket/nhl_game_shifts</code> contains after this code is run:</p> <ul> <li>game_shiftsA.csv: 68.7 MB</li> <li>game_shiftsB.csv: 68.7 MB</li> <li>part-00000-tid-\u2026-c000.csv: 53.7 MB</li> </ul>"},{"location":"apache-spark/compacting-files/#programatically-compacting-the-small-files","title":"Programatically compacting the small files","text":"<p>Let's use the AWS CLI to identify the small files in a S3 folder.</p> <p>Need to finish the rest of this section\u2026</p>"},{"location":"apache-spark/compacting-files/#small-file-problem-in-hadoop","title":"Small file problem in Hadoop","text":"<p>Hadoop's small file problem has been well documented for quite some time. Cloudera does a great job examining this problem as well.</p>"},{"location":"apache-spark/compacting-files/#next-steps","title":"Next steps","text":"<p>It's important to quantify how many small data files are contained in folders that are queried frequently.</p> <p>If there are folders with a lot of small files, you should compact the files and see if that improves query performance.</p> <p>Eliminating small files can significanly improve performance.</p>"},{"location":"apache-spark/compacting-parquet-files/","title":"Compacting Parquet Files","text":"<p>This post describes how to programatically compact Parquet files in a folder.</p> <p>Incremental updates frequently result in lots of small files that can be slow to read. It's best to periodically compact the small files into larger files, so they can be read faster.</p>"},{"location":"apache-spark/compacting-parquet-files/#tldr","title":"TL;DR","text":"<p>You can easily compact Parquet files in a folder with the spark-daria <code>ParquetCompactor</code> class. Suppose you have a folder with a thousand 11 MB files that you'd like to compact into 20 files. Here's the code that'll perform the compaction.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.ParquetCompactor\n\nnew ParquetCompactor(\"/path/to/the/data\", 20).run()\n</code></pre>"},{"location":"apache-spark/compacting-parquet-files/#compaction-steps","title":"Compaction steps","text":"<p>Here are the high level compaction steps:</p> <ol> <li>List all of the files in the directory</li> <li>Coalesce the files</li> <li>Write out the compacted files</li> <li>Delete the uncompacted files</li> </ol> <p>Let's walk through the spark-daria compaction code to see how the files are compacted.</p> <p>Start by writing all the uncompacted filenames in the folder to a separate directory. We'll use this filename listing to delete all the uncompacted files later.</p> <pre><code>val df = spark.read.parquet(dirname)\n\ndf.withColumn(\"input_file_name_part\", regexp_extract(input_file_name(), \"\"\"part.+c\\d{3}\"\"\", 0))\n  .select(\"input_file_name_part\")\n  .distinct\n  .write\n  .parquet(s\"$dirname/input_file_name_parts\")\n</code></pre> <p>Let's read in all the uncompacted data into a DataFrame, coalesce the data into <code>numOutputFiles</code> partitions, and then write out the partitioned data.</p> <pre><code>val fileNames = spark.read\n  .parquet(s\"$dirname/input_file_name_parts\")\n  .collect\n  .map((r: Row) =&gt; r(0).asInstanceOf[String])\n\nval uncompactedDF = spark.read\n  .parquet(s\"$dirname/{${fileNames.mkString(\",\")}}*.parquet\")\n\nuncompactedDF\n  .coalesce(numOutputFiles)\n  .write\n  .mode(SaveMode.Append)\n  .parquet(dirname)\n</code></pre> <p>Our data lake now contains the unpartitioned files and the compacted files. We have the same data stored twice.</p> <p>Let's delete all of the unpartitioned files and then delete the directory that was tracking the unpartitioned file names.</p> <pre><code>import org.apache.hadoop.fs.{FileSystem, Path}\n\nval fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n\nfileNames.foreach { filename: String =&gt;\n  fs.delete(new Path(s\"$dirname/$filename.snappy.parquet\"), false)\n}\n\nfs.delete(new Path(s\"$dirname/input_file_name_parts\"), true)\n</code></pre> <p>The optimal number of partitions depends on how much data is stored in the folder. It's generally best to use 1GB files. If you're folder contains 260GB of data, you should use 260 partitions.</p>"},{"location":"apache-spark/compacting-parquet-files/#programatically-computing-folder-sizes","title":"Programatically computing folder sizes","text":"<p>We can infer the optimal number of files for a folder by how much data is in the folder.</p> <p>The AWS CLI makes it easy to calculate the number of data in a folder with this command.</p> <pre><code>aws s3 ls --summarize --human-readable --recursive s3://bucket-name/directory\n</code></pre> <p>Accessing the AWS CLI via your Spark runtime isn't always the easiest, so you can also use some <code>org.apache.hadoop</code> code. The code returns the number of bytes in a folder.</p> <pre><code>val filePath   = new org.apache.hadoop.fs.Path(dirname)\nval fileSystem = filePath.getFileSystem(spark.sparkContext.hadoopConfiguration)\nfileSystem.getContentSummary(filePath).getLength\n</code></pre>"},{"location":"apache-spark/compacting-parquet-files/#compacting-partitioned-lakes","title":"Compacting partitioned lakes","text":"<p>Let's iterate over every partition in a partitioned data lake and compact each partition:</p> <pre><code>import new com.github.mrpowers.spark.daria.sql.ParquetCompactor\nimport com.github.mrpowers.spark.daria.utils.DirHelpers\n\nval dirname = \"/some/path\"\nval partitionNames = Array(\"partition1\", \"partition2)\npartitionNames.foreach{ p: String =&gt;\n  val numBytes = DirHelpers.numBytes(s\"$dirname/$p\")\n  val numGigaBytes = DirHelpers.bytesToGb(numBytes)\n  val num1GBPartitions = DirHelpers.num1GBPartitions(numGigaBytes)\n\n  new ParquetCompactor(s\"$dirname/$p\", num1GBPartitions).run()\n}\n</code></pre> <p>We calculate the number of gigabytes in each partition and use that to set the optimal number of files per partition. This script only compacts one partition at a time, so it shouldn't overload a cluster.</p>"},{"location":"apache-spark/compacting-parquet-files/#conclusion","title":"Conclusion","text":"<p>Compacting Parquet data lakes is important so the data lake can be read quickly.</p> <p>Compaction is particularly important for partitioned Parquet data lakes that tend to have tons of files.</p> <p>Use the tactics in this blog to keep your Parquet files close to the 1GB ideal size and keep your data lake read times fast.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/","title":"Convert streaming CSV data to Delta Lake with different latency requirements","text":"<p>This blog post explains how to incrementally convert streaming CSV data into Delta Lake with different latency requirements. A streaming CSV data source is used because it's easy to demo, but the lessons covered in this post also apply to streaming event platforms like Kafka or Kinesis.</p> <p>This post will show you three ways to convert CSV to Delta Lake, with different latency and cost implications. In general, lower latency solutions cost more to run in production.</p> <p>Here are the different options we'll cover:</p> <ul> <li>Structured Streaming &amp; Trigger Once: low cost &amp; high latency</li> <li>Structured Streaming &amp; microbatch processing: higher cost &amp; lower latency</li> <li>Reading streaming data directly: lowest latency</li> </ul> <p>All of the code covered in this post is organized in this notebook if you'd like to run these computations on your local machine.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/#project-and-data-setup","title":"Project and data setup","text":"<p>We're going to setup a Structured Streaming process to watch a directory that'll have data that's added incrementally. The CSV data files will look like this:</p> <pre><code>student_name,graduation_year,major\nsomeXXperson,2023,math\nliXXyao,2025,physics\n</code></pre> <p>The data files have three columns: <code>student_name</code>, <code>graduation_year</code>, and <code>major</code>. As you can see the <code>student_name</code> column contains both the <code>first_name</code> and <code>last_name</code>, separated by <code>XX</code>. We'll want to properly split <code>first_name</code> and <code>last_name</code> into separate columns before writing to the Delta Lake.</p> <p>Here's the function that'll normalize the <code>student_name</code> column:</p> <pre><code>def with_normalized_names(df):\n    split_col = pyspark.sql.functions.split(df[\"student_name\"], \"XX\")\n    return (\n        df.withColumn(\"first_name\", split_col.getItem(0))\n        .withColumn(\"last_name\", split_col.getItem(1))\n        .drop(\"student_name\")\n    )\n</code></pre> <p>When you're reading data with Structured Streaming, you also need to specify the schema as follows:</p> <pre><code>schema = (\n    StructType()\n    .add(\"student_name\", \"string\")\n    .add(\"graduation_year\", \"string\")\n    .add(\"major\", \"string\")\n)\n</code></pre> <p>Now let's look at how to initialize a PySpark SparkSession with Delta Lake so you can run these examples in a localhost notebook.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/#creating-the-pyspark-sparksession","title":"Creating the PySpark SparkSession","text":"<p>Here's how to create the PySpark SparkSession when you're using Delta Lake:</p> <pre><code>import pyspark\nfrom delta import *\nfrom pyspark.sql.types import StructType\n\nbuilder = (\n    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n)\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n</code></pre> <p>See this blog post on installing PySpark and Delta Lake with conda if you haven't installed all the dependencies in your localhost environment yet.</p> <p>Now let's look at performing incremental updates with Structured Streaming and Trigger Once.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/#option-1-structured-streaming-and-trigger-once","title":"Option 1: Structured Streaming and Trigger Once","text":"<p>We have <code>students1.csv</code>, <code>students2.csv</code> and <code>students3.csv</code> files. We'll manually move these files into the <code>tmp_students_incremental</code> directory throughout this example to simulate a directory that's being incrementally updated with CSV data in a streaming manner.</p> <p>Let's start by moving the <code>students1.csv</code> file into the <code>tmp_students_incremental</code> directory:</p> <pre><code>mkdir data/tmp_students_incremental\ncp data/students/students1.csv data/tmp_students_incremental\n</code></pre> <p>Let's read this data into a streaming DataFrame:</p> <pre><code>df = (\n    spark.readStream.schema(schema)\n    .option(\"header\", True)\n    .csv(\"data/tmp_students_incremental\")\n)\n</code></pre> <p>We'd now like to apply the <code>with_normalized_names</code> transformation and write the data to a Delta Lake. Let's wrap this trigger once invocation in a function:</p> <pre><code>def perform_trigger_once_update():\n    checkpointPath = \"data/tmp_students_checkpoint/\"\n    deltaPath = \"data/tmp_students_delta\"\n    return df.transform(lambda df: with_normalized_names(df)).writeStream.trigger(\n        once=True\n    ).format(\"delta\").option(\"checkpointLocation\", checkpointPath).start(deltaPath)\n</code></pre> <p>You need a <code>checkpointLocation</code> to track the files that have already been processed. When the trigger once function is invoked, it'll will look at all the CSV files in the streaming directory, check the files that have already been processed in the <code>checkpointLocation</code> directory, and only process the new files.</p> <p>Run the <code>perform_trigger_once_update()</code> function and then observe the contents of the Delta Lake.</p> <pre><code>perform_trigger_once_update()\n\nspark.read.format(\"delta\").load(deltaPath).show()\n</code></pre> <pre><code>+---------------+-------+----------+---------+\n|graduation_year|  major|first_name|last_name|\n+---------------+-------+----------+---------+\n|           2023|   math|      some|   person|\n|           2025|physics|        li|      yao|\n+---------------+-------+----------+---------+\n</code></pre> <p>The CSV data was cleaned with the <code>with_normalized_names</code> function and is properly written to the Delta Lake.</p> <p>Now copy over the <code>students2.csv</code> file to the <code>tmp_students_incremental</code> folder with <code>cp data/students/students2.csv data/tmp_students_incremental</code>, perform another trigger once update, and observe the contents of the Delta Lake.</p> <pre><code>perform_trigger_once_update()\n\nspark.read.format(\"delta\").load(deltaPath).show()\n</code></pre> <pre><code>+---------------+-------+----------+---------+\n|graduation_year|  major|first_name|last_name|\n+---------------+-------+----------+---------+\n|           2022|    bio|    sophia|     raul|\n|           2025|physics|      fred|       li|\n|           2023|   math|      some|   person|\n|           2025|physics|        li|      yao|\n+---------------+-------+----------+---------+\n</code></pre> <p>Spark correctly updated the Delta Lake with the data in <code>students2.csv</code>.</p> <p>Perform the final incremental update by copying <code>students3.csv</code> to <code>tmp_students_incremental</code>:</p> <pre><code>cp data/students/students3.csv data/tmp_students_incremental\n</code></pre> <p>Perform another incremental update and view the contents of the Delta Lake.</p> <pre><code>perform_trigger_once_update()\n\nspark.read.format(\"delta\").load(deltaPath).show()\n</code></pre> <pre><code>+---------------+-------+----------+---------+\n|graduation_year|  major|first_name|last_name|\n+---------------+-------+----------+---------+\n|           2025|    bio|     chris|     borg|\n|           2026|physics|     david|    cross|\n|           2022|    bio|    sophia|     raul|\n|           2025|physics|      fred|       li|\n|           2023|   math|      some|   person|\n|           2025|physics|        li|      yao|\n+---------------+-------+----------+---------+\n</code></pre> <p>You're able to incrementally update the Delta Lake by simply invoking the <code>perform_trigger_once_update()</code> function. Spark is intelligent enough to only process the new data for each invocation.</p> <p>You can invoke the <code>perform_trigger_once_update()</code> function as frequently or as seldom as you'd like. You can invoke the function every 3 hours, every day, or every week. It depends on the latency requirements of the Delta Lake that's being updated.</p> <p>Suppose your Delta Lake is queried by a business user on a daily basis every morning. The Delta Lake is only queried once per day, so you only need to perform daily updates. In this case, you can setup a cron job to run <code>perform_trigger_once_update()</code> every morning at 8AM, so the Delta Lake is updated by 9AM for the business user.</p> <p>Trigger once updates are less costly than constant updates that require a cluster to be continuously running. If you only need periodic updates, it's more economical to kick off an incremental update with cron, perform the update, and then shut down the cluster when the update finishes.</p> <p>Now let's look at a different situation where you need to build a system that updates the Delta Lake every 2 seconds. This needs to be architected differently to account for the different latency requirement.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/#option-2-structured-streaming-microbatch-processing","title":"Option 2: Structured Streaming &amp; microbatch processing","text":"<p>This section shows how to use a Structured Streaming cluster to update a Delta Lake with streaming data every two seconds. This cluster needs to be kept running at all times.</p> <p>As before, let's read the CSV data with <code>readStream</code>:</p> <pre><code>df = (\n    spark.readStream.schema(schema)\n    .option(\"header\", True)\n    .csv(\"data/tmp_students_incremental\")\n)\n</code></pre> <p>Let's write out any new data to the Delta Lake every two seconds.</p> <pre><code>checkpointPath = \"data/tmp_students_checkpoint/\"\ndeltaPath = \"data/tmp_students_delta\"\n\ndf.transform(lambda df: with_normalized_names(df)).writeStream.trigger(\n    processingTime=\"2 seconds\"\n).format(\"delta\").option(\"checkpointLocation\", checkpointPath).start(deltaPath)\n</code></pre> <p>Copy over the <code>students1.csv</code> data file with <code>cp data/students/students1.csv data/tmp_students_incremental</code>, wait two seconds, and then the Delta Lake will be automatically updated.</p> <p>Check the contents of the Delta Lake:</p> <pre><code>spark.read.format(\"delta\").load(deltaPath).show()\n\n+---------------+-------+----------+---------+\n|graduation_year|  major|first_name|last_name|\n+---------------+-------+----------+---------+\n|           2023|   math|      some|   person|\n|           2025|physics|        li|      yao|\n+---------------+-------+----------+---------+\n</code></pre> <p>Now copy over the <code>students2.csv</code> file, wait two seconds, and check that the Delta Lake has been automatically updated.</p> <pre><code>cp data/students/students2.csv data/tmp_students_incremental\n\nspark.read.format(\"delta\").load(deltaPath).show()\n\n+---------------+-------+----------+---------+\n|graduation_year|  major|first_name|last_name|\n+---------------+-------+----------+---------+\n|           2023|   math|      some|   person|\n|           2025|physics|        li|      yao|\n|           2022|    bio|    sophia|     raul|\n|           2025|physics|      fred|       li|\n+---------------+-------+----------+---------+\n</code></pre> <p>With trigger once, we needed to invoke a function every time we wanted the update to run. When the <code>trigger</code> is set to <code>processingTime=\"2 seconds\"</code>, you don't need to invoke a function to perform the update - it happens automatically every two seconds.</p> <p>Finally copy over <code>students3.csv</code>, wait two seconds, and again check that the Delta Lake was updated.</p> <pre><code>cp data/students/students3.csv data/tmp_students_incremental\n\nspark.read.format(\"delta\").load(deltaPath).show()\n\n+---------------+-------+----------+---------+\n|graduation_year|  major|first_name|last_name|\n+---------------+-------+----------+---------+\n|           2025|    bio|     chris|     borg|\n|           2026|physics|     david|    cross|\n|           2023|   math|      some|   person|\n|           2025|physics|        li|      yao|\n|           2022|    bio|    sophia|     raul|\n|           2025|physics|      fred|       li|\n+---------------+-------+----------+---------+\n</code></pre> <p>You can just keep the cluster running and Spark will automatically detect any new streaming data and write it to the Delta Lake every two seconds. This of course means you need to keep the cluster running 24/7, which is more expensive than running a periodic job with trigger once. This approach offers lower latency with higher cost. It's a great option when you're building a system that needs to be updated with low latency.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/#option-3-reading-streaming-data-directly","title":"Option 3: Reading streaming data directly","text":"<p>You can also read the streaming CSV data directly, which will have the lowest latency.</p> <p>Here's how to continuously stream data to the console:</p> <pre><code>df.writeStream \\\n  .format(\"console\") \\\n  .trigger(continuous='1 second') \\\n  .start()\n</code></pre> <p>See the Structured Streaming Programming Guide for more details.</p> <p>Continuous streaming is still experimental, but is a promising option for applications that have extremely low latency requirements.</p>"},{"location":"apache-spark/convert-csv-to-delta-lake-latency-trigger/#conclusion","title":"Conclusion","text":"<p>This post shows a variety of ways to incrementally update a Delta Lake, depending on the latency requirements of your application.</p> <p>Make sure to investigate the latency requirements of the end users in detail before building an ETL pipeline. Sometimes users will casually mention that they'd like a realtime dashboard and upon further digging, you'll find that they actually will only be looking at the dashboard a couple of times a day. You don't need realtime updates if you're not going to change decision making based on the last couple of seconds of data.</p> <p>An ETL pipeline that's consumed by an automated process for a high-frequency trading system, on the other hand, might actually need to be realtime.</p> <p>\"Realtime\" is a loaded term in the streaming space. A pipeline with 5 seconds of latency is way easier to build than something that's truly \"realtime\".</p> <p>It's best to work backwards, determine the latency requirements of the system, and then architect a pipeline that meets the needs of the end users. There is no sense in building a pipeline with 2 second latency, and incurring the costs of a cluster that's constantly running, if the end users only need hourly updates.</p> <p>The lower the latency of the system, the higher the probability that the pipeline will generate lots of small files. Delta Lake is great at performing backwards compatible small file compaction. Make sure you've setup auto optimization if your pipeline will generate lots of small files.</p> <p>This post covered a simple example, but your streaming pipeline may be more complicated:</p> <ul> <li>you may have multiple streaming data sources that need to be joined before writing to the Delta Lake</li> <li>you may want to perform stateful aggregations before writing to the Delta Lake</li> <li>writing to multiple data sources</li> </ul> <p>Future blog posts will explain these different scenarios in detail. Luckily for you, Structured Streaming makes it easy to build pipelines that are incrementally updated for all of these situations. Enjoy the beautiful user interface!</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/","title":"Defining DataFrame Schemas with StructField and StructType","text":"<p>Spark DataFrames schemas are defined as a collection of typed columns. The entire schema is stored as a <code>StructType</code> and individual columns are stored as <code>StructFields</code>.</p> <p>This blog post explains how to create and modify Spark schemas via the <code>StructType</code> and <code>StructField</code> classes.</p> <p>We'll show how to work with <code>IntegerType</code>, <code>StringType</code>, <code>LongType</code>, <code>ArrayType</code>, <code>MapType</code> and <code>StructType</code> columns.</p> <p>Mastering Spark schemas is necessary for debugging code and writing tests.</p> <p>This blog post provides a great introduction to these topics, but Writing Beautiful Spark Code provides a much more comprehensive review of the topics covered in this post. The book is the fastest way for you to become a strong Spark programmer.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#defining-a-schema-to-create-a-dataframe","title":"Defining a schema to create a DataFrame","text":"<p>Let's invent some sample data, define a schema, and create a DataFrame.</p> <pre><code>import org.apache.spark.sql.types._\n\nval data = Seq(\n  Row(8, \"bat\"),\n  Row(64, \"mouse\"),\n  Row(-27, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"number\", IntegerType, true),\n    StructField(\"word\", StringType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <pre><code>df.show()\n\n+------+-----+\n|number| word|\n+------+-----+\n|     8|  bat|\n|    64|mouse|\n|   -27|horse|\n+------+-----+\n</code></pre> <p><code>StructType</code> objects are instantiated with a <code>List</code> of <code>StructField</code> objects.</p> <p>The <code>org.apache.spark.sql.types</code> package must be imported to access <code>StructType</code>, <code>StructField</code>, <code>IntegerType</code>, and <code>StringType</code>.</p> <p>The <code>createDataFrame()</code> method takes two arguments:</p> <ol> <li>RDD of the data</li> <li>The DataFrame schema (a <code>StructType</code> object)</li> </ol> <p>The <code>schema()</code> method returns a <code>StructType</code> object:</p> <pre><code>df.schema\n\nStructType(\n  StructField(number,IntegerType,true),\n  StructField(word,StringType,true)\n)\n</code></pre>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#structfield","title":"<code>StructField</code>","text":"<p><code>StructFields</code> model each column in a DataFrame.</p> <p><code>StructField</code> objects are created with the <code>name</code>, <code>dataType</code>, and <code>nullable</code> properties. Here's an example:</p> <pre><code>StructField(\"word\", StringType, true)\n</code></pre> <p>The <code>StructField</code> above sets the <code>name</code> field to <code>\"word\"</code>, the <code>dataType</code> field to <code>StringType</code>, and the <code>nullable</code> field to <code>true</code>.</p> <p><code>\"word\"</code> is the name of the column in the DataFrame.</p> <p><code>StringType</code> means that the column can only take string values like <code>\"hello\"</code> - it cannot take other values like <code>34</code> or <code>false</code>.</p> <p>When the <code>nullable</code> field is set to <code>true</code>, the column can accept <code>null</code> values.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#defining-schemas-with-the-operator","title":"Defining schemas with the <code>::</code> operator","text":"<p>We can also define a schema with the <code>::</code> operator, like the examples in the StructType documentation.</p> <pre><code>val schema = StructType(\n  StructField(\"number\", IntegerType, true) ::\n  StructField(\"word\", StringType, true) :: Nil\n)\n</code></pre> <p>The <code>::</code> operator makes it easy to construct lists in Scala. We can also use <code>::</code> to make a list of numbers.</p> <pre><code>5 :: 4 :: Nil\n</code></pre> <p>Notice that the last element always has to be <code>Nil</code> or the code will error out.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#defining-schemas-with-the-add-method","title":"Defining schemas with the <code>add()</code> method","text":"<p>We can use the <code>StructType#add()</code> method to define schemas.</p> <pre><code>val schema = StructType(Seq(StructField(\"number\", IntegerType, true)))\n  .add(StructField(\"word\", StringType, true))\n</code></pre> <p><code>add()</code> is an overloaded method and there are several different ways to invoke it - this will work too:</p> <pre><code>val schema = StructType(Seq(StructField(\"number\", IntegerType, true)))\n  .add(\"word\", StringType, true)\n</code></pre> <p>Check the StructType documentation for all the different ways <code>add()</code> can be used.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#common-errors","title":"Common errors","text":""},{"location":"apache-spark/dataframe-schema-structfield-structtype/#extra-column-defined-in-schema","title":"Extra column defined in Schema","text":"<p>The following code has an extra column defined in the schema and will error out with this message: <code>java.lang.RuntimeException: Error while encoding: java.lang.ArrayIndexOutOfBoundsException: 2</code>.</p> <pre><code>val data = Seq(\n  Row(8, \"bat\"),\n  Row(64, \"mouse\"),\n  Row(-27, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"number\", IntegerType, true),\n    StructField(\"word\", StringType, true),\n    StructField(\"num2\", IntegerType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <p>The data only contains two columns, but the schema contains three <code>StructField</code> columns.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#type-mismatch","title":"Type mismatch","text":"<p>The following code incorrectly characterizes a string column as an integer column and will error out with this message: <code>java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.String is not a valid external type for schema of int</code>.</p> <pre><code>val data = Seq(\n  Row(8, \"bat\"),\n  Row(64, \"mouse\"),\n  Row(-27, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"num1\", IntegerType, true),\n    StructField(\"num2\", IntegerType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n\ndf.show()\n</code></pre> <p>The first column of data (<code>8</code>, <code>64</code>, and <code>-27</code>) can be characterized as <code>IntegerType</code> data.</p> <p>The second column of data (<code>\"bat\"</code>, <code>\"mouse\"</code>, and <code>\"horse\"</code>) cannot be characterized as an <code>IntegerType</code> column - this could would work if this column was recharacterized as <code>StringType</code>.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#nullable-property-exception","title":"Nullable property exception","text":"<p>The following code incorrectly tries to add <code>null</code> to a column with a <code>nullable</code> property set to false and will error out with this message: <code>java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: The 0th field 'word1' of input row cannot be null</code>.</p> <pre><code>val data = Seq(\n  Row(\"hi\", \"bat\"),\n  Row(\"bye\", \"mouse\"),\n  Row(null, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"word1\", StringType, false),\n    StructField(\"word2\", StringType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n\ndf.show()\n</code></pre>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#longtype","title":"<code>LongType</code>","text":"<p>Integers use 32 bits whereas long values use 64 bits.</p> <p>Integers can hold values between -2 billion to 2 billion (<code>-scala.math.pow(2, 31)</code> to <code>scala.math.pow(2, 31) - 1</code> to be exact).</p> <p>Long values are suitable for bigger integers. You can create a long value in Scala by appending <code>L</code> to an integer - e.g. <code>4L</code> or <code>-60L</code>.</p> <p>Let's create a DataFrame with a <code>LongType</code> column.</p> <pre><code>val data = Seq(\n  Row(5L, \"bat\"),\n  Row(-10L, \"mouse\"),\n  Row(4L, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"long_num\", LongType, true),\n    StructField(\"word\", StringType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <pre><code>df.show()\n\n+--------+-----+\n|long_num| word|\n+--------+-----+\n|       5|  bat|\n|     -10|mouse|\n|       4|horse|\n+--------+-----+\n</code></pre> <p>You'll get the following error message if you try to add integers to a <code>LongType</code> column: <code>java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of bigint</code></p> <p>Here's an example of the erroneous code:</p> <pre><code>val data = Seq(\n  Row(45, \"bat\"),\n  Row(2, \"mouse\"),\n  Row(3, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"long_num\", LongType, true),\n    StructField(\"word\", StringType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n\ndf.show()\n</code></pre>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#arraytype","title":"<code>ArrayType</code>","text":"<p>Spark supports columns that contain arrays of values. Scala offers lists, sequences, and arrays. In regular Scala code, it's best to use <code>List</code> or <code>Seq</code>, but <code>Arrays</code> are frequently used with Spark.</p> <p>Here's how to create an array of numbers with Scala:</p> <pre><code>val numbers = Array(1, 2, 3)\n</code></pre> <p>Let's create a DataFrame with an <code>ArrayType</code> column.</p> <pre><code>val data = Seq(\n  Row(\"bieber\", Array(\"baby\", \"sorry\")),\n  Row(\"ozuna\", Array(\"criminal\"))\n)\n\nval schema = StructType(\n  List(\n    StructField(\"name\", StringType, true),\n    StructField(\"hit_songs\", ArrayType(StringType, true), true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <pre><code>df.show()\n\n+------+-------------+\n|  name|    hit_songs|\n+------+-------------+\n|bieber|[baby, sorry]|\n| ozuna|   [criminal]|\n+------+-------------+\n</code></pre>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#maptype","title":"<code>MapType</code>","text":"<p>Scala maps store key / value pairs (maps are called \"hashes\" in other programming languages). Let's create a Scala map with beers and their country of origin.</p> <pre><code>val beers = Map(\"aguila\" -&gt; \"Colombia\", \"modelo\" -&gt; \"Mexico\")\n</code></pre> <p>Let's grab the value that's associated with the key <code>\"modelo\"</code>:</p> <pre><code>beers(\"modelo\") // Mexico\n</code></pre> <p>Let's create a DataFrame with a <code>MapType</code> column.</p> <pre><code>val data = Seq(\n  Row(\"sublime\", Map(\n    \"good_song\" -&gt; \"santeria\",\n    \"bad_song\" -&gt; \"doesn't exist\")\n  ),\n  Row(\"prince_royce\", Map(\n    \"good_song\" -&gt; \"darte un beso\",\n    \"bad_song\" -&gt; \"back it up\")\n  )\n)\n\nval schema = StructType(\n  List(\n    StructField(\"name\", StringType, true),\n    StructField(\"songs\", MapType(StringType, StringType, true), true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <pre><code>df.show(false)\n\n+------------+----------------------------------------------------+\n|name        |songs                                               |\n+------------+----------------------------------------------------+\n|sublime     |[good_song -&gt; santeria, bad_song -&gt; doesn't exist]  |\n|prince_royce|[good_song -&gt; darte un beso, bad_song -&gt; back it up]|\n+------------+----------------------------------------------------+\n</code></pre> <p>Notice that <code>MapType</code> is instantiated with three arguments (e.g. <code>MapType(StringType, StringType, true)</code>). The first argument is the <code>keyType</code>, the second argument is the <code>valueType</code>, and the third argument is a boolean flag for <code>valueContainsNull</code>. Map values can contain <code>null</code> if <code>valueContainsNull</code> is set to <code>true</code>, but the key can never be <code>null</code>.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#structtype-nested-schemas","title":"<code>StructType</code> nested schemas","text":"<p>DataFrame schemas can be nested. A DataFrame column can be a <code>struct</code> - it's essentially a schema within a schema.</p> <p>Let's create a DataFrame with a <code>StructType</code> column.</p> <pre><code>val data = Seq(\n  Row(\"bob\", Row(\"blue\", 45)),\n  Row(\"mary\", Row(\"red\", 64))\n)\n\nval schema = StructType(\n  List(\n    StructField(\"name\", StringType, true),\n    StructField(\n      \"person_details\",\n      StructType(\n        List(\n          StructField(\"favorite_color\", StringType, true),\n          StructField(\"age\", IntegerType, true)\n        )\n      ),\n      true\n    )\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <pre><code>df.show()\n\n+----+--------------+\n|name|person_details|\n+----+--------------+\n| bob|    [blue, 45]|\n|mary|     [red, 64]|\n+----+--------------+\n</code></pre> <p>We can use the <code>printSchema()</code> method to illustrate that <code>person_details</code> is a <code>struct</code> column:</p> <pre><code>df.printSchema()\n\nroot\n |-- name: string (nullable = true)\n |-- person_details: struct (nullable = true)\n |    |-- favorite_color: string (nullable = true)\n |    |-- age: integer (nullable = true)\n</code></pre>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#structtype-object-oriented-programming","title":"<code>StructType</code> object oriented programming","text":"<p>The <code>StructType</code> object mixes in the <code>Seq</code> trait to access a bunch of collection methods.</p> <p>Here's how <code>StructType</code> is defined:</p> <pre><code>case class StructType(fields: Array[StructField])\n  extends DataType\n  with Seq[StructField]\n</code></pre> <p>Here's the StructType source code.</p> <p>The Scala <code>Seq</code> trait is defined as follows:</p> <pre><code>trait Seq[+A]\n  extends PartialFunction[Int, A]\n  with Iterable[A]\n  with GenSeq[A]\n  with GenericTraversableTemplate[A, Seq]\n  with SeqLike[A, Seq[A]]\n</code></pre> <p>By inheriting from the <code>Seq</code> trait, the <code>StructType</code> class gets access to collection methods like <code>collect()</code> and <code>foldLeft()</code>.</p> <p>Let's create a DataFrame schema and use the <code>foldLeft()</code> method to create a sequence of all the column names.</p> <pre><code>val data = Seq(\n  Row(8, \"bat\"),\n  Row(64, \"mouse\"),\n  Row(-27, \"horse\")\n)\n\nval schema = StructType(\n  List(\n    StructField(\"number\", IntegerType, true),\n    StructField(\"word\", StringType, true)\n  )\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n\nval columns = schema.foldLeft(Seq.empty[String]) {(memo: Seq[String], s: StructField) =&gt;\n  memo ++ Seq(s.name)\n}\n</code></pre> <p>If we really wanted to get a list of all the column names, we could just run <code>df.columns</code>, but the <code>foldLeft()</code> method is clearly more powerful - it let's us perform arbitrary collection operations on our DataFrame schemas.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#flattening-dataframes-with-structtype-columns","title":"Flattening DataFrames with <code>StructType</code> columns","text":"<p>In the previous section, we created a DataFrame with a <code>StructType</code> column. Let's expand the two columns in the nested <code>StructType</code> column to be two separate fields.</p> <p>We will leverage a <code>flattenSchema</code> method from spark-daria to make this easy.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DataFrameExt._\n\nval flattenedDF = df.flattenSchema(delimiter = \"_\")\n</code></pre> <pre><code>flattenedDF.show()\n\n+----+-----------------------------+------------------+\n|name|person_details_favorite_color|person_details_age|\n+----+-----------------------------+------------------+\n| bob|                         blue|                45|\n|mary|                          red|                64|\n+----+-----------------------------+------------------+\n</code></pre> <p>Take a look at the spark-daria source code to see how this code works.</p>"},{"location":"apache-spark/dataframe-schema-structfield-structtype/#next-steps","title":"Next steps","text":"<p>You'll be defining a lot of schemas in your test suites so make sure to master all the concepts covered in this blog post.</p>"},{"location":"apache-spark/dataframe-summary-describe/","title":"Exploring DataFrames with summary and describe","text":"<p>The <code>summary</code> and <code>describe</code> methods make it easy to explore the contents of a DataFrame at a high level.</p> <p>This post shows you how to use these methods.</p> <p>TL;DR - <code>summary</code> is more useful than <code>describe</code>. You can get the same result with <code>agg</code>, but <code>summary</code> will save you from writing a lot of code.</p>"},{"location":"apache-spark/dataframe-summary-describe/#describe","title":"describe","text":"<p>Suppose you have the following DataFrame.</p> <pre><code>+----+-------+\n|num1|letters|\n+----+-------+\n|   1|     aa|\n|   2|     aa|\n|   9|     bb|\n|   5|     cc|\n+----+-------+\n</code></pre> <p>Use <code>describe</code> to compute some summary statistics on the DataFrame.</p> <pre><code>df.describe().show()\n</code></pre> <pre><code>+-------+-----------------+-------+\n|summary|             num1|letters|\n+-------+-----------------+-------+\n|  count|                4|      4|\n|   mean|             4.25|   null|\n| stddev|3.593976442141304|   null|\n|    min|                1|     aa|\n|    max|                9|     cc|\n+-------+-----------------+-------+\n</code></pre> <p>You can limit the <code>describe</code> statistics for a subset of columns:</p> <pre><code>df.describe(\"num1\").show()\n</code></pre> <pre><code>+-------+-----------------+\n|summary|             num1|\n+-------+-----------------+\n|  count|                4|\n|   mean|             4.25|\n| stddev|3.593976442141304|\n|    min|                1|\n|    max|                9|\n+-------+-----------------+\n</code></pre> <p>This option isn't very useful. <code>df.select(\"num1\").describe().show()</code> would give the same result and is more consistent with the rest of the Spark API.</p> <p>Let's turn out attention to <code>summary</code>, a better designed method that provides more useful options.</p>"},{"location":"apache-spark/dataframe-summary-describe/#summary","title":"summary","text":"<p>Suppose you have the same starting DataFrame from before.</p> <pre><code>+----+-------+\n|num1|letters|\n+----+-------+\n|   1|     aa|\n|   2|     aa|\n|   9|     bb|\n|   5|     cc|\n+----+-------+\n</code></pre> <p>Calculate the summary statistics for all columns in the DataFrame.</p> <pre><code>df.summary().show()\n</code></pre> <pre><code>+-------+-----------------+-------+\n|summary|             num1|letters|\n+-------+-----------------+-------+\n|  count|                4|      4|\n|   mean|             4.25|   null|\n| stddev|3.593976442141304|   null|\n|    min|                1|     aa|\n|    25%|                1|   null|\n|    50%|                2|   null|\n|    75%|                5|   null|\n|    max|                9|     cc|\n+-------+-----------------+-------+\n</code></pre> <p>Let's customize the output to return the count, 33rd percentile, 50th percentile, and 66th percentile.</p> <pre><code>df.summary(\"count\", \"33%\", \"50%\", \"66%\").show()\n</code></pre> <pre><code>+-------+----+-------+\n|summary|num1|letters|\n+-------+----+-------+\n|  count|   4|      4|\n|    33%|   2|   null|\n|    50%|   2|   null|\n|    66%|   5|   null|\n+-------+----+-------+\n</code></pre> <p>Limit the custom summary to the <code>num1</code> column cause it doesn't make sense to compute percentiles for string columns.</p> <pre><code>df.select(\"num1\").summary(\"count\", \"33%\", \"50%\", \"66%\").show()\n</code></pre> <pre><code>+-------+----+\n|summary|num1|\n+-------+----+\n|  count|   4|\n|    33%|   2|\n|    50%|   2|\n|    66%|   5|\n+-------+----+\n</code></pre> <p>I worked with the Spark core team to add some additional options to this method, so as of Spark 3.2, you'll also be able to compute the exact and approximate count distinct.</p> <p>Here's how to get the exact count and distinct count for each column:</p> <pre><code>df.summary(\"count\", \"count_distinct\").show()\n</code></pre> <p>Here's how to get the approximate count distinct, which will run faster:</p> <pre><code>df.summary(\"count\", \"approx_count_distinct\").show()\n</code></pre>"},{"location":"apache-spark/dataframe-summary-describe/#agg","title":"agg","text":"<p>We can use <code>agg</code> to manually compute the summary statistics for columns in the DataFrame. Here's how to calculate the distinct count for each column in the DataFrame.</p> <pre><code>df.agg(countDistinct(\"num1\"), countDistinct(\"letters\")).show()\n</code></pre> <pre><code>+-----------+--------------+\n|count(num1)|count(letters)|\n+-----------+--------------+\n|          4|             3|\n+-----------+--------------+\n</code></pre> <p>Here's how to calculate the distinct count and the max for each column in the DataFrame:</p> <pre><code>val counts = df.agg(\n  lit(\"countDistinct\").as(\"colName\"),\n  countDistinct(\"num1\").as(\"num1\"),\n  countDistinct(\"letters\").as(\"letters\"))\nval maxes = df.agg(\n  lit(\"max\").as(\"colName\"),\n  max(\"num1\").as(\"num1\"),\n  max(\"letters\").as(\"letters\"))\ncounts.union(maxes).show()\n</code></pre> <pre><code>+-------------+----+-------+\n|      colName|num1|letters|\n+-------------+----+-------+\n|countDistinct|   4|      3|\n|          max|   9|     cc|\n+-------------+----+-------+\n</code></pre> <p>The code gets verbose quick. <code>summary</code> is great cause it prevents you from writing a lot of code.</p>"},{"location":"apache-spark/dataframe-summary-describe/#conclusion","title":"Conclusion","text":"<p><code>summary</code> is great for high level exploratory data analysis.</p> <p>For more detailed exploratory data analysis, see the deequ library.</p> <p>Ping me if you're interested and I'll add an extensible version of <code>summary</code> to spark-daria. We should have a <code>dariaSummary</code> method that can be invoked like this <code>df.dariaSummary(countDistinct, max)</code> and automatically generate a custom summary, without forcing the user to write tons of code.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/","title":"Spark Datasets: Advantages and Limitations","text":"<p>Datasets are available to Spark Scala/Java users and offer more type safety than DataFrames.</p> <p>Python and R infer types during runtime, so these APIs cannot support the Datasets.</p> <p>This post demonstrates how to create Datasets and describes the advantages of this data structure.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#tods","title":"toDS","text":"<p>Create a <code>City</code> case class, instantiate some objects, and then build a Dataset:</p> <pre><code>case class City(englishName: String, continent: String)\n\nval cities = Seq(\n  City(\"bejing\", \"asia\"),\n  City(\"new york\", \"north america\"),\n  City(\"paris\", \"europe\")\n).toDS()\n\ncities.show()\n</code></pre> <pre><code>+-----------+-------------+\n|englishName|    continent|\n+-----------+-------------+\n|     bejing|         asia|\n|   new york|north america|\n|      paris|       europe|\n+-----------+-------------+\n</code></pre> <p>The <code>cities</code> Dataset is of type <code>org.apache.spark.sql.Dataset[City]</code>.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#createdataset","title":"createDataset","text":"<p>The <code>cities</code> Dataset can also be created with the <code>createDataset</code> method:</p> <pre><code>case class City(englishName: String, continent: String)\n\nval cities2 = spark.createDataset(\n  Seq(\n    City(\"bejing\", \"asia\"),\n    City(\"new york\", \"north america\"),\n    City(\"paris\", \"europe\")\n  )\n)\n</code></pre> <p><code>cities2</code> is also of type <code>org.apache.spark.sql.Dataset[City]</code>.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#converting-from-dataframe-to-dataset","title":"Converting from DataFrame to Dataset","text":"<p>Let's create a DataFrame of trees and then convert it to a Dataset. Start by creating the DataFrame.</p> <pre><code>val treesDF = Seq(\n  (\"Oak\", \"deciduous\"),\n  (\"Hemlock\", \"evergreen\"),\n  (\"Apple\", \"angiosperms\")\n).toDF(\"tree_name\", \"tree_type\")\n\ntreesDF.show()\n</code></pre> <pre><code>+---------+-----------+\n|tree_name|  tree_type|\n+---------+-----------+\n|      Oak|  deciduous|\n|  Hemlock|  evergreen|\n|    Apple|angiosperms|\n+---------+-----------+\n</code></pre> <p><code>treesDF</code> is a <code>org.apache.spark.sql.DataFrame</code>.</p> <p>Define a case class and use <code>as</code> to convert the DataFrame to a Dataset.</p> <pre><code>case class Tree(tree_name: String, tree_type: String)\n\nval treesDS = treesDF.as[Tree]\n</code></pre> <p><code>treesDS</code> is a <code>org.apache.spark.sql.Dataset[Tree]</code>.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#dataframe-is-an-alias-for-datasetrow","title":"DataFrame is an alias for Dataset[Row]","text":"<p>DataFrame is defined as a Dataset[Row] in the Spark codebase with this line: <code>type DataFrame = Dataset[Row]</code>.</p> <p><code>org.apache.spark.sql.Row</code> is a generic object that can be instantiated with any arguments.</p> <pre><code>import org.apache.spark.sql.Row\n\nval oneRow = Row(\"hi\", 34)\nval anotherRow = Row(34.2, \"cool\", 4)\n</code></pre> <p>case classes cannot be instantiated with any arguments. This code will error out:</p> <pre><code>case class Furniture(furniture_type: String, color: String)\nFurniture(\"bed\", 33)\n</code></pre> <p>Here's the error message:</p> <pre><code>error: type mismatch;\n found   : Int(33)\n required: String\nFurniture(\"bed\", 33)\n                 ^\n</code></pre> <p>Scala throws a compile-time error when you try to instantiate an object with the wrong type.</p> <p>Your text editor will complain about this code, so you don't need to wait until runtime to discover the error.</p> <p></p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#runtime-vs-compile-time-errors","title":"Runtime vs compile-time errors","text":"<p>Here's some invalid code to create a DataFrame that'll error-out at runtime:</p> <pre><code>val shoesDF = Seq(\n  (\"nike\", \"black\"),\n  (\"puma\", 42)\n).toDF(\"brand\", \"color\")\n</code></pre> <p>Here's the runtime error: <code>java.lang.ClassNotFoundException: scala.Any</code>.</p> <p>Note that the runtime error is not descriptive, so the bug is hard to trace. The runtime error isn't caught by your text editor either.</p> <p>Let's write some similarly invalid code to create a Dataset.</p> <pre><code>case class Shoe(brand: String, color: String)\n\nval shoesDS = Seq(\n  Shoe(\"nike\", \"black\"),\n  Shoe(\"puma\", 42)\n).toDS()\n</code></pre> <p>The Dataset API gives a much better error message:</p> <pre><code>error: type mismatch;\n found   : Int(42)\n required: String\n      Shoe(\"puma\", 42)\n                   ^\n</code></pre> <p>This is a compile-time error, so it'll be caught by your text editor.</p> <p></p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#advantages-of-datasets","title":"Advantages of Datasets","text":"<p>Datasets catch some bugs at compile-time that aren't caught by DataFrames till runtime.</p> <p>Runtime bugs can be a real nuisance for big data jobs.</p> <p>You don't want to run a job for 4 hours, only to have it error out with a silly runtime bug. It's better to catch bugs in your text editor, before they become production job errors.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#disadvantages-of-datasets","title":"Disadvantages of Datasets","text":"<p>Spark automatically converts Datasets to DataFrames when performing operations like adding columns.</p> <p>Adding columns is a common operation. You can go through the effort of defining a case class to build a Dataset, but all that type safety is lost with a simple <code>withColumn</code> operation.</p> <p>Here's an example:</p> <pre><code>case class Sport(name: String, uses_ball: Boolean)\n\nval sportsDS = Seq(\n  Sport(\"basketball\", true),\n  Sport(\"polo\", true),\n  Sport(\"hockey\", false)\n).toDS()\n</code></pre> <p><code>sportsDS</code> is of type <code>org.apache.spark.sql.Dataset[Sport]</code>.</p> <p>Append a <code>short_name</code> column to the Dataset and view the results.</p> <pre><code>import org.apache.spark.sql.functions._\nval res = sportsDS.withColumn(\"short_name\", substring($\"name\", 1, 3))\nres.show()\n</code></pre> <pre><code>+----------+---------+----------+\n|      name|uses_ball|short_name|\n+----------+---------+----------+\n|basketball|     true|       bas|\n|      polo|     true|       pol|\n|    hockey|    false|       hoc|\n+----------+---------+----------+\n</code></pre> <p><code>res</code> is of type <code>org.apache.spark.sql.DataFrame</code>.</p> <p>We'd need to define another case class and convert <code>res</code> back to a Dataset if we'd like to get the type safety benefits back.</p> <p>Not all operations convert Datasets to DataFrames. For example, filtering does not convert Datasets to DataFrames:</p> <pre><code>val nonBallSports = sportsDS.where($\"uses_ball\" === false)\n</code></pre> <p><code>nonBallSports</code> is still of type <code>org.apache.spark.sql.Dataset[Sport]</code>.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#typed-datasets","title":"Typed Datasets","text":"<p>The frameless library offers typed Datasets that are even more type safe than Spark Datasets, but typed Datasets face even more limitations.</p> <p>Lots of Spark workflows operate on wide tables with transformations that append tens or hundreds of columns. Creating a new case class whenever a new column is added isn't practical for most Spark workflows.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#why-pyspark-doesnt-have-datasets","title":"Why PySpark doesn't have Datasets","text":"<p>We've demonstrated that Scala will throw compile-time errors when case classes are instantiated with invalid arguments.</p> <p>Python is not compile-time type safe, so it throws runtime exceptions when classes are instantiated with invalid arguments. The Dataset API cannot be added to PySpark because of this Python language limitation.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#spark-datasets-arent-so-type-safe","title":"Spark Datasets aren't so type safe","text":""},{"location":"apache-spark/dataset-tods-createdataset-advantages/#some-nonsensical-operations-are-caught-at-runtime","title":"Some nonsensical operations are caught at runtime","text":"<p>Create a Dataset with an integer column and try to add four months to the integer.</p> <pre><code>case class Cat(name: String, favorite_number: Int)\n\nval catsDS = Seq(\n  Cat(\"fluffy\", 45)\n).toDS()\n\ncatsDS.withColumn(\"meaningless\", add_months($\"favorite_number\", 4)).show()\n</code></pre> <p>Here's the error message: org.apache.spark.sql.AnalysisException: cannot resolve 'add_months(<code>favorite_number</code>, 4)' due to data type mismatch: argument 1 requires date type, however, '<code>favorite_number</code>' is of int type.;;</p> <p>AnalysisExceptions are thrown at runtime, so this isn't a compile-time error that you'd expect from a type safe API.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#other-nonsensical-operations-return-null","title":"Other nonsensical operations return null","text":"<p>Let's run the <code>date_trunc</code> function on a <code>StringType</code> column and observe the result.</p> <pre><code>catsDS.withColumn(\"meaningless\", date_trunc(\"name\", lit(\"cat\"))).show()\n</code></pre> <pre><code>+------+---------------+-----------+\n|  name|favorite_number|meaningless|\n+------+---------------+-----------+\n|fluffy|             45|       null|\n+------+---------------+-----------+\n</code></pre> <p>Some Spark functions just return <code>null</code> when the operation is meaningless. <code>lit(\"cat\")</code> isn't a valid format, so this operation will always return <code>null</code>.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#some-operations-return-meaningless-results","title":"Some operations return meaningless results","text":"<p>Let's create a Dataset with a date column and then reverse the date:</p> <pre><code>import java.sql.Date\n\ncase class Birth(hospitalName: String, birthDate: Date)\n\nval birthsDS = Seq(\n  Birth(\"westchester\", Date.valueOf(\"2014-01-15\"))\n).toDS()\n\nbirthsDS.withColumn(\"meaningless\", reverse($\"birthDate\")).show()\n</code></pre> <pre><code>+------------+----------+-----------+\n|hospitalName| birthDate|meaningless|\n+------------+----------+-----------+\n| westchester|2014-01-15| 51-10-4102|\n+------------+----------+-----------+\n</code></pre> <p>At the very least, we'd expect this code to error out at runtime with a <code>org.apache.spark.sql.AnalysisException</code>.</p> <p>A type-safe implementation would throw a compile time error when the <code>reverse</code> function is passed a <code>DateType</code> value.</p>"},{"location":"apache-spark/dataset-tods-createdataset-advantages/#conclusion","title":"Conclusion","text":"<p>Spark Datasets offer more type safety than DataFrames, but they're hard to stick with. Spark will automatically convert your Datasets to DataFrames when you perform common operations, like adding a column.</p> <p>Even when you're using Datasets, you don't get much type safety. The <code>org.apache.spark.sql.Column</code> objects don't have type information, so they're akin to Scala <code>Any</code> values.</p> <p>Some <code>org.apache.spark.sql.functions</code> throw <code>AnalysisException</code> when they're supplied with meaningless input, but others just return junk responses. Diligently test your Spark code to catch bugs before making production deploys.</p>"},{"location":"apache-spark/dates-times/","title":"Working with dates and times in Spark","text":"<p>Spark supports <code>DateType</code> and <code>TimestampType</code> columns and defines a rich API of functions to make working with dates and times easy. This blog post will demonstrates how to make DataFrames with <code>DateType</code> / <code>TimestampType</code> columns and how to leverage Spark's functions for working with these columns.</p>"},{"location":"apache-spark/dates-times/#complex-spark-column-types","title":"Complex Spark Column types","text":"<p>Spark supports ArrayType,\u00a0MapType\u00a0and\u00a0StructType\u00a0columns in addition to the DateType / TimestampType columns covered in this post.</p> <p>Check out\u00a0Writing Beautiful Spark Code\u00a0for a detailed overview of the different complex column types and how they should be used when architecting Spark applications.</p>"},{"location":"apache-spark/dates-times/#creating-datetype-columns","title":"Creating DateType columns","text":"<p>Import the <code>java.sql.Date</code> library to create a DataFrame with a <code>DateType</code> column.</p> <pre><code>import java.sql.Date\nimport org.apache.spark.sql.types.{DateType, IntegerType}\n\nval sourceDF = spark.createDF(\n  List(\n    (1, Date.valueOf(\"2016-09-30\")),\n    (2, Date.valueOf(\"2016-12-14\"))\n  ), List(\n    (\"person_id\", IntegerType, true),\n    (\"birth_date\", DateType, true)\n  )\n)\n</code></pre> <pre><code>sourceDF.show()\n\n+---------+----------+\n|person_id|birth_date|\n+---------+----------+\n|        1|2016-09-30|\n|        2|2016-12-14|\n+---------+----------+\n\nsourceDF.printSchema()\n\nroot\n |-- person_id: integer (nullable = true)\n |-- birth_date: date (nullable = true)\n</code></pre> <p>The <code>cast()</code> method can create a <code>DateType</code> column by converting a <code>StringType</code> column into a date.</p> <pre><code>val sourceDF = spark.createDF(\n  List(\n    (1, \"2013-01-30\"),\n    (2, \"2012-01-01\")\n  ), List(\n    (\"person_id\", IntegerType, true),\n    (\"birth_date\", StringType, true)\n  )\n).withColumn(\n  \"birth_date\",\n  col(\"birth_date\").cast(\"date\")\n)\n</code></pre> <pre><code>sourceDF.show()\n\n+---------+----------+\n|person_id|birth_date|\n+---------+----------+\n|        1|2013-01-30|\n|        2|2012-01-01|\n+---------+----------+\n\nsourceDF.printSchema()\n\nroot\n |-- person_id: integer (nullable = true)\n |-- birth_date: date (nullable = true)\n</code></pre>"},{"location":"apache-spark/dates-times/#year-month-dayofmonth","title":"year(), month(), dayofmonth()","text":"<p>Let's create a DataFrame with a <code>DateType</code> column and use built in Spark functions to extract the year, month, and day from the date.</p> <pre><code>val sourceDF = spark.createDF(\n  List(\n    (1, Date.valueOf(\"2016-09-30\")),\n    (2, Date.valueOf(\"2016-12-14\"))\n  ), List(\n    (\"person_id\", IntegerType, true),\n    (\"birth_date\", DateType, true)\n  )\n)\n\nsourceDF.withColumn(\n  \"birth_year\",\n  year(col(\"birth_date\"))\n).withColumn(\n  \"birth_month\",\n  month(col(\"birth_date\"))\n).withColumn(\n  \"birth_day\",\n  dayofmonth(col(\"birth_date\"))\n).show()\n</code></pre> <pre><code>+---------+----------+----------+-----------+---------+\n|person_id|birth_date|birth_year|birth_month|birth_day|\n+---------+----------+----------+-----------+---------+\n|        1|2016-09-30|      2016|          9|       30|\n|        2|2016-12-14|      2016|         12|       14|\n+---------+----------+----------+-----------+---------+\n</code></pre>"},{"location":"apache-spark/dates-times/#minute-second","title":"minute(), second()","text":"<p>Let's create a DataFrame with a <code>TimestampType</code> column and use built in Spark functions to extract the minute and second from the timestamp.</p> <pre><code>import java.sql.Timestamp\n\nval sourceDF = spark.createDF(\n  List(\n    (1, Timestamp.valueOf(\"2017-12-02 03:04:00\")),\n    (2, Timestamp.valueOf(\"1999-01-01 01:45:20\"))\n  ), List(\n    (\"person_id\", IntegerType, true),\n    (\"fun_time\", TimestampType, true)\n  )\n)\n\nsourceDF.withColumn(\n  \"fun_minute\",\n  minute(col(\"fun_time\"))\n).withColumn(\n  \"fun_second\",\n  second(col(\"fun_time\"))\n).show()\n</code></pre> <pre><code>+---------+-------------------+----------+----------+\n|person_id|           fun_time|fun_minute|fun_second|\n+---------+-------------------+----------+----------+\n|        1|2017-12-02 03:04:00|         4|         0|\n|        2|1999-01-01 01:45:20|        45|        20|\n+---------+-------------------+----------+----------+\n</code></pre>"},{"location":"apache-spark/dates-times/#datediff","title":"datediff()","text":"<p>The <code>datediff()</code> and <code>current_date()</code> functions can be used to calculate the number of days between today and a date in a <code>DateType</code> column. Let's use these functions to calculate someone's age in days.</p> <pre><code>val sourceDF = spark.createDF(\n  List(\n    (1, Date.valueOf(\"1990-09-30\")),\n    (2, Date.valueOf(\"2001-12-14\"))\n  ), List(\n    (\"person_id\", IntegerType, true),\n    (\"birth_date\", DateType, true)\n  )\n)\n\nsourceDF.withColumn(\n  \"age_in_days\",\n  datediff(current_timestamp(), col(\"birth_date\"))\n).show()\n</code></pre> <pre><code>+---------+----------+-----------+\n|person_id|birth_date|age_in_days|\n+---------+----------+-----------+\n|        1|1990-09-30|       9946|\n|        2|2001-12-14|       5853|\n+---------+----------+-----------+\n</code></pre>"},{"location":"apache-spark/dates-times/#date_add","title":"date_add()","text":"<p>The <code>date_add()</code> function can be used to add days to a date. Let's add 15 days to a date column.</p> <pre><code>val sourceDF = spark.createDF(\n  List(\n    (1, Date.valueOf(\"1990-09-30\")),\n    (2, Date.valueOf(\"2001-12-14\"))\n  ), List(\n    (\"person_id\", IntegerType, true),\n    (\"birth_date\", DateType, true)\n  )\n)\n\nsourceDF.withColumn(\n  \"15_days_old\",\n  date_add(col(\"birth_date\"), 15)\n).show()\n</code></pre> <pre><code>+---------+----------+-----------+\n|person_id|birth_date|15_days_old|\n+---------+----------+-----------+\n|        1|1990-09-30| 1990-10-15|\n|        2|2001-12-14| 2001-12-29|\n+---------+----------+-----------+\n</code></pre>"},{"location":"apache-spark/dates-times/#next-steps","title":"Next steps","text":"<p>Look at the Spark SQL functions for the full list of methods available for working with dates and times in Spark.</p> <p>The Spark date functions aren't comprehensive and Java / Scala datetime libraries are notoriously difficult to work with. We should think about filling in the gaps in the native Spark datetime libraries by adding functions to spark-daria.</p>"},{"location":"apache-spark/dealing-with-null/","title":"Dealing with null in Spark","text":"<p>Spark Datasets / DataFrames are filled with null values and you should write code that gracefully handles these null values.</p> <p>You don't want to write code that thows <code>NullPointerExceptions</code> - yuck!</p> <p>If you're using PySpark, see this post on Navigating None and null in PySpark.</p> <p>Writing Beautiful Spark Code outlines all of the advanced tactics for making null your best friend when you work with Spark.</p> <p>This post outlines when null should be used, how native Spark functions handle null input, and how to simplify null logic by avoiding user defined functions. This post is a great start, but it doesn't provide all the detailed context discussed in Writing Beautiful Spark Code.</p>"},{"location":"apache-spark/dealing-with-null/#what-is-null","title":"What is null?","text":"<p>In SQL databases, \"null means that some value is unknown, missing, or irrelevant.\" The SQL concept of null is different than null in programming languages like JavaScript or Scala. Spark DataFrame best practices are aligned with SQL best practices, so DataFrames should use null for values that are unknown, missing or irrelevant.</p>"},{"location":"apache-spark/dealing-with-null/#spark-uses-null-by-default-sometimes","title":"Spark uses null by default sometimes","text":"<p>Let's look at the following file as an example of how Spark considers blank and empty CSV fields as <code>null</code> values.</p> <pre><code>name,country,zip_code\njoe,usa,89013\nravi,india,\n\"\",,12389\n</code></pre> <p>All the blank values and empty strings are read into a DataFrame as null by the Spark CSV library (after Spark 2.0.1 at least).</p> <pre><code>val peopleDf = spark.read.option(\"header\", \"true\").csv(path)\n</code></pre> <pre><code>peopleDf.show()\n\n+----+-------+--------+\n|name|country|zip_code|\n+----+-------+--------+\n| joe|    usa|   89013|\n|ravi|  india|    null|\n|null|   null|   12389|\n+----+-------+--------+\n</code></pre> <p>The Spark <code>csv()</code> method demonstrates that null is used for values that are unknown or missing when files are read into DataFrames.</p>"},{"location":"apache-spark/dealing-with-null/#nullable-columns","title":"nullable Columns","text":"<p>Let's create a DataFrame with a <code>name</code> column that isn't nullable and an <code>age</code> column that is nullable. The <code>name</code> column cannot take null values, but the <code>age</code> column can take null values. The nullable property is the third argument when instantiating a <code>StructField</code>.</p> <pre><code>val schema = List(\n  StructField(\"name\", StringType, false),\n  StructField(\"age\", IntegerType, true)\n)\n\nval data = Seq(\n  Row(\"miguel\", null),\n  Row(\"luisa\", 21)\n)\n\nval df = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  StructType(schema)\n)\n</code></pre> <p>If we try to create a DataFrame with a <code>null</code> value in the <code>name</code> column, the code will blow up with this error: \"Error while encoding: java.lang.RuntimeException: The 0th field 'name' of input row cannot be null\".</p> <p>Here's some code that would cause the error to be thrown:</p> <pre><code>val data = Seq(\n  Row(\"phil\", 44),\n  Row(null, 21)\n)\n</code></pre> <p>You can keep null values out of certain columns by setting <code>nullable</code> to <code>false</code>.</p> <p>You won't be able to set <code>nullable</code> to <code>false</code> for all columns in a DataFrame and pretend like <code>null</code> values don't exist. For example, when joining DataFrames, the join column will return <code>null</code> when a match cannot be made.</p> <p>You can run, but you can't hide!</p>"},{"location":"apache-spark/dealing-with-null/#native-spark-code","title":"Native Spark code","text":"<p>Native Spark code handles <code>null</code> gracefully.</p> <p>Let's create a DataFrame with numbers so we have some data to play with.</p> <pre><code>val schema = List(\n  StructField(\"number\", IntegerType, true)\n)\n\nval data = Seq(\n  Row(1),\n  Row(8),\n  Row(12),\n  Row(null)\n)\n\nval numbersDF = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  StructType(schema)\n)\n</code></pre> <p>Now let's add a column that returns <code>true</code> if the number is even, <code>false</code> if the number is odd, and <code>null</code> otherwise.</p> <pre><code>numbersDF\n  .withColumn(\"is_even\", $\"number\" % 2 === 0)\n  .show()\n</code></pre> <pre><code>+------+-------+\n|number|is_even|\n+------+-------+\n|     1|  false|\n|     8|   true|\n|    12|   true|\n|  null|   null|\n+------+-------+\n</code></pre> <p>The Spark <code>%</code> function returns <code>null</code> when the input is <code>null</code>. Actually all Spark functions return <code>null</code> when the input is <code>null</code>. All of your Spark functions should return <code>null</code> when the input is <code>null</code> too!</p>"},{"location":"apache-spark/dealing-with-null/#scala-null-conventions","title":"Scala null Conventions","text":"<p>Native Spark code cannot always be used and sometimes you'll need to fall back on Scala code and User Defined Functions. The Scala best practices for null are different than the Spark null best practices.</p> <p>David Pollak, the author of Beginning Scala, stated \"Ban null from any of your code. Period.\" Alvin Alexander, a prominent Scala blogger and author, explains why Option is better than null in this blog post. The Scala community clearly prefers Option to avoid the pesky null pointer exceptions that have burned them in Java.</p> <p>Some developers erroneously interpret these Scala best practices to infer that null should be banned from DataFrames as well! Remember that DataFrames are akin to SQL databases and should generally follow SQL best practices. Scala best practices are completely different.</p> <p>The Databricks Scala style guide does not agree that null should always be banned from Scala code and says: \"For performance sensitive code, prefer null over Option, in order to avoid virtual method calls and boxing.\"</p> <p>The Spark source code uses the Option keyword 821 times, but it also refers to null directly in code like <code>if (ids != null)</code>. Spark may be taking a hybrid approach of using Option when possible and falling back to null when necessary for performance reasons.</p> <p>I think Option should be used wherever possible and you should only fall back on null when necessary for performance reasons.</p> <p>Let's dig into some code and see how null and Option can be used in Spark user defined functions.</p>"},{"location":"apache-spark/dealing-with-null/#user-defined-functions","title":"User Defined Functions","text":"<p>Let's create a user defined function that returns true if a number is even and false if a number is odd.</p> <pre><code>def isEvenSimple(n: Integer): Boolean = {\n  n % 2 == 0\n}\n\nval isEvenSimpleUdf = udf[Boolean, Integer](isEvenSimple)\n</code></pre> <p>Suppose we have the following <code>sourceDf</code> DataFrame:</p> <pre><code>+------+\n|number|\n+------+\n|     1|\n|     8|\n|    12|\n|  null|\n+------+\n</code></pre> <p>Our UDF does not handle <code>null</code> input values. Let's run the code and observe the error.</p> <pre><code>numbersDF.withColumn(\n  \"is_even\",\n  isEvenSimpleUdf(col(\"number\"))\n)\n</code></pre> <p>Here is the error message:</p> <p>SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 1 times, most recent failure: Lost task 2.0 in stage 16.0 (TID 41, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (int) =&gt; boolean)</p> <p>Caused by: java.lang.NullPointerException</p> <p>We can use the <code>isNotNull</code> method to work around the <code>NullPointerException</code> that's caused when <code>isEvenSimpleUdf</code> is invoked.</p> <pre><code>val actualDf = sourceDf.withColumn(\n  \"is_even\",\n  when(\n    col(\"number\").isNotNull,\n    isEvenSimpleUdf(col(\"number\"))\n  ).otherwise(lit(null))\n)\n</code></pre> <pre><code>actualDf.show()\n\n+------+-------+\n|number|is_even|\n+------+-------+\n|     1|  false|\n|     8|   true|\n|    12|   true|\n|  null|   null|\n+------+-------+\n</code></pre> <p>It's better to write user defined functions that gracefully deal with null values and don't rely on the <code>isNotNull</code> work around\u200a-\u200alet's try again.</p>"},{"location":"apache-spark/dealing-with-null/#dealing-with-null-badly","title":"Dealing with null\u00a0badly","text":"<p>Let's refactor the user defined function so it doesn't error out when it encounters a null value.</p> <pre><code>def isEvenBad(n: Integer): Boolean = {\n  if (n == null) {\n    false\n  } else {\n    n % 2 == 0\n  }\n}\n\nval isEvenBadUdf = udf[Boolean, Integer](isEvenBad)\n</code></pre> <p>We can run the <code>isEvenBadUdf</code> on the same <code>sourceDf</code> as earlier.</p> <pre><code>val actualDf = sourceDf.withColumn(\n  \"is_even\",\n  isEvenBadUdf(col(\"number\"))\n)\n</code></pre> <pre><code>actualDf.show()\n\n+------+-------+\n|number|is_even|\n+------+-------+\n|     1|  false|\n|     8|   true|\n|    12|   true|\n|  null|  false|\n+------+-------+\n</code></pre> <p>This code works, but is terrible because it returns false for odd numbers and null numbers. Remember that null should be used for values that are irrelevant. null is not even or odd\u200a-\u200areturning false for null numbers implies that null is odd!</p> <p>Let's refactor this code and correctly return null when number is null.</p>"},{"location":"apache-spark/dealing-with-null/#dealing-with-null-better","title":"Dealing with null\u00a0better","text":"<p>The <code>isEvenBetterUdf</code> returns <code>true</code> / <code>false</code> for numeric values and <code>null</code> otherwise.</p> <pre><code>def isEvenBetter(n: Integer): Option[Boolean] = {\n  if (n == null) {\n    None\n  } else {\n    Some(n % 2 == 0)\n  }\n}\n\nval isEvenBetterUdf = udf[Option[Boolean], Integer](isEvenBetter)\n</code></pre> <p>The <code>isEvenBetter</code> method returns an <code>Option[Boolean]</code>. When the input is null, <code>isEvenBetter</code> returns <code>None</code>, which is converted to null in DataFrames.</p> <p>Let's run the <code>isEvenBetterUdf</code> on the same <code>sourceDf</code> as earlier and verify that null values are correctly added when the number column is null.</p> <pre><code>val actualDf = sourceDf.withColumn(\n  \"is_even\",\n  isEvenBetterUdf(col(\"number\"))\n)\n</code></pre> <pre><code>actualDf.show()\n\n+------+-------+\n|number|is_even|\n+------+-------+\n|     1|  false|\n|     8|   true|\n|    12|   true|\n|  null|   null|\n+------+-------+\n</code></pre> <p>The <code>isEvenBetter</code> function is still directly referring to null. Let's do a final refactoring to fully remove null from the user defined function.</p>"},{"location":"apache-spark/dealing-with-null/#best-scala-style-solution-what-about-performance","title":"Best Scala Style Solution (What about performance?)","text":"<p>We'll use <code>Option</code> to get rid of null once and for all!</p> <pre><code>def isEvenOption(n: Integer): Option[Boolean] = {\n  val num = Option(n).getOrElse(return None)\n  Some(num % 2 == 0)\n}\n\nval isEvenOptionUdf = udf[Option[Boolean], Integer](isEvenOption)\n</code></pre> <p>The <code>isEvenOption</code> function converts the integer to an <code>Option</code> value and returns <code>None</code> if the conversion cannot take place. This code does not use <code>null</code> and follows the purist advice: \"Ban null from any of your code. Period.\"</p> <p>A smart commenter pointed out that returning in the middle of a function is a Scala antipattern and this code is even more elegant:</p> <pre><code>def isEvenOption(n:Int): Option[Boolean] = {\n  Option(n).map( _ % 2 == 0)\n}\n</code></pre> <p>Both solution Scala option solutions are less performant than directly referring to null, so a refactoring should be considered if performance becomes a bottleneck.</p>"},{"location":"apache-spark/dealing-with-null/#user-defined-functions-cannot-take-options-as-params","title":"User Defined Functions Cannot Take Options as\u00a0Params","text":"<p>User defined functions surprisingly cannot take an <code>Option</code> value as a parameter, so this code won't work:</p> <pre><code>def isEvenBroke(n: Option[Integer]): Option[Boolean] = {\n  val num = n.getOrElse(return None)\n  Some(num % 2 == 0)\n}\n\nval isEvenBrokeUdf = udf[Option[Boolean], Option[Integer]](isEvenBroke)\n</code></pre> <p>If you run this code, you'll get the following error:</p> <pre><code>org.apache.spark.SparkException: Failed to execute user defined function\n\nCaused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.Option\n</code></pre>"},{"location":"apache-spark/dealing-with-null/#spark-rules-for-dealing-with-null","title":"Spark Rules for Dealing with\u00a0null","text":"<p>Use native Spark code whenever possible to avoid writing <code>null</code> edge case logic</p> <ol> <li> <p>If UDFs are needed, follow these rules:</p> </li> <li> <p>Scala code should deal with null values gracefully and shouldn't error out if there are null values.</p> </li> <li>Scala code should return <code>None</code> (or null) for values that are unknown, missing, or irrelevant. DataFrames should also use null for for values that are unknown, missing, or irrelevant.</li> <li>Use <code>Option</code> in Scala code and fall back on null if <code>Option</code> becomes a performance bottleneck.</li> </ol>"},{"location":"apache-spark/deduplicating-and-collapsing/","title":"Deduplicating and Collapsing Records in Spark DataFrames","text":"<p>This blog post explains how to filter duplicate records from Spark DataFrames with the <code>dropDuplicates()</code> and <code>killDuplicates()</code> methods. It also demonstrates how to collapse duplicate records into a single row with the <code>collect_list()</code> and <code>collect_set()</code> functions.</p> <p>Make sure to read\u00a0Writing Beautiful Spark Code\u00a0for a detailed overview of how to deduplicate production datasets and for background information on the ArrayType columns that are returned when DataFrames are collapsed.</p>"},{"location":"apache-spark/deduplicating-and-collapsing/#deduplicating-dataframes","title":"Deduplicating DataFrames","text":"<p>Let's create a DataFrame with <code>letter1</code>, <code>letter2</code>, and <code>number1</code> columns.</p> <pre><code>val df = Seq(\n  (\"a\", \"b\", 1),\n  (\"a\", \"b\", 2),\n  (\"a\", \"b\", 3),\n  (\"z\", \"b\", 4),\n  (\"a\", \"x\", 5)\n).toDF(\"letter1\", \"letter2\", \"number1\")\n\ndf.show()\n</code></pre> <pre><code>+-------+-------+-------+\n|letter1|letter2|number1|\n+-------+-------+-------+\n|      a|      b|      1|\n|      a|      b|      2|\n|      a|      b|      3|\n|      z|      b|      4|\n|      a|      x|      5|\n+-------+-------+-------+\n</code></pre> <p>Some rows in the <code>df</code> DataFrame have the same <code>letter1</code> and <code>letter2</code> values. Let's use the <code>Dataset#dropDuplicates()</code> method to remove duplicates from the DataFrame.</p> <pre><code>df.dropDuplicates(\"letter1\", \"letter2\").show()\n</code></pre> <pre><code>+-------+-------+-------+\n|letter1|letter2|number1|\n+-------+-------+-------+\n|      a|      x|      5|\n|      z|      b|      4|\n|      a|      b|      1|\n+-------+-------+-------+\n</code></pre> <p>The <code>dropDuplicates</code> method chooses one record from the duplicates and drops the rest. This is useful for simple use cases, but collapsing records is better for analyses that can't afford to lose any valuable data.</p>"},{"location":"apache-spark/deduplicating-and-collapsing/#killing-duplicates","title":"Killing duplicates","text":"<p>We can use the spark-daria <code>killDuplicates()</code> method to completely remove all duplicates from a DataFrame.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DataFrameExt._\n\ndf.killDuplicates(\"letter1\", \"letter2\").show()\n</code></pre> <pre><code>+-------+-------+-------+\n|letter1|letter2|number1|\n+-------+-------+-------+\n|      a|      x|      5|\n|      z|      b|      4|\n+-------+-------+-------+\n</code></pre> <p>Killing duplicates is similar to dropping duplicates, just a little more aggressive.</p>"},{"location":"apache-spark/deduplicating-and-collapsing/#collapsing-records","title":"Collapsing records","text":"<p>Let's use the <code>collect_list()</code> method to eliminate all the rows with duplicate <code>letter1</code> and <code>letter2</code> rows in the DataFrame and collect all the <code>number1</code> entries as a list.</p> <pre><code>df\n  .groupBy(\"letter1\", \"letter2\")\n  .agg(collect_list(\"number1\") as \"number1s\")\n  .show()\n</code></pre> <pre><code>+-------+-------+---------+\n|letter1|letter2| number1s|\n+-------+-------+---------+\n|      a|      x|      [5]|\n|      z|      b|      [4]|\n|      a|      b|[1, 2, 3]|\n+-------+-------+---------+\n</code></pre> <p>Let's create a more realitic example of credit card transactions and use <code>collect_set()</code> to aggregate unique records and eliminate pure duplicates.</p> <pre><code>val ccTransactionsDF = Seq(\n  (\"123\", \"20180102\", 10.49),\n  (\"123\", \"20180102\", 10.49),\n  (\"123\", \"20180102\", 77.33),\n  (\"555\", \"20180214\", 99.99),\n  (\"888\", \"20180214\", 1.23)\n).toDF(\"person_id\", \"transaction_date\", \"amount\")\n\nccTransactionsDF.show()\n</code></pre> <pre><code>+---------+----------------+------+\n|person_id|transaction_date|amount|\n+---------+----------------+------+\n|      123|        20180102| 10.49|\n|      123|        20180102| 10.49|\n|      123|        20180102| 77.33|\n|      555|        20180214| 99.99|\n|      888|        20180214|  1.23|\n+---------+----------------+------+\n</code></pre> <p>Let's eliminate the duplicates with <code>collect_set()</code>.</p> <pre><code>ccTransactionsDF\n  .groupBy(\"person_id\", \"transaction_date\")\n  .agg(collect_set(\"amount\") as \"amounts\")\n  .show()\n</code></pre> <pre><code>+---------+----------------+--------------+\n|person_id|transaction_date|       amounts|\n+---------+----------------+--------------+\n|      555|        20180214|       [99.99]|\n|      888|        20180214|        [1.23]|\n|      123|        20180102|[10.49, 77.33]|\n+---------+----------------+--------------+\n</code></pre> <p><code>collect_set()</code> let's us retain all the valuable information and delete the duplicates. The best of both worlds!</p>"},{"location":"apache-spark/deduplicating-and-collapsing/#collapsing-records-to-datamarts","title":"Collapsing records to datamarts","text":"<p>Let's examine a DataFrame of with data on hockey players and how many goals they've scored in each game.</p> <pre><code>val playersDF = Seq(\n  (\"123\", 11, \"20180102\", 0),\n  (\"123\", 11, \"20180102\", 0),\n  (\"123\", 13, \"20180105\", 3),\n  (\"555\", 11, \"20180214\", 1),\n  (\"888\", 22, \"20180214\", 2)\n).toDF(\"player_id\", \"game_id\", \"game_date\", \"goals_scored\")\n\nplayersDF.show()\n</code></pre> <pre><code>+---------+-------+---------+------------+\n|player_id|game_id|game_date|goals_scored|\n+---------+-------+---------+------------+\n|      123|     11| 20180102|           0|\n|      123|     11| 20180102|           0|\n|      123|     13| 20180105|           3|\n|      555|     11| 20180214|           1|\n|      888|     22| 20180214|           2|\n+---------+-------+---------+------------+\n</code></pre> <p>Let's create a <code>StructType</code> column that encapsulates all the columns in the DataFrame and then collapse all records on the <code>player_id</code> column to create a player datamart.</p> <pre><code>playersDF\n  .withColumn(\"as_struct\", struct(\"game_id\", \"game_date\", \"goals_scored\"))\n  .groupBy(\"player_id\")\n  .agg(collect_set(\"as_struct\") as \"as_structs\")\n  .show(false)\n</code></pre> <pre><code>+---------+----------------------------------+\n|player_id|as_structs                        |\n+---------+----------------------------------+\n|888      |[[22,20180214,2]]                 |\n|555      |[[11,20180214,1]]                 |\n|123      |[[11,20180102,0], [13,20180105,3]]|\n+---------+----------------------------------+\n</code></pre> <p>A player datamart like this can simplify a lot of queries. We don't need to write window functions if all the data is already aggregated in a single row.</p>"},{"location":"apache-spark/deduplicating-and-collapsing/#next-steps","title":"Next steps","text":"<p>Deduplicating DataFrames is relatively straightforward. Collapsing records is more complicated, but worth the effort.</p> <p>Data lakes are notoriously granular and programmers often write window functions to analyze historical results.</p> <p>Collapsing records into datamarts is the best way to simplify your code logic.</p>"},{"location":"apache-spark/environment-specific-configuration/","title":"Environment Specific Config in Spark Scala Projects","text":"<p>Environment config files return different values for the test, development, staging, and production environments.</p> <p>In Spark projects, you will often want a variable to point to a local CSV file in the test environment and a CSV file in S3 in the production environment.</p> <p>This episode will demonstrate how to add environment config to your projects and how to set environment variables to change the environment.</p>"},{"location":"apache-spark/environment-specific-configuration/#basic-use-case","title":"Basic use case","text":"<p>Let's create a <code>Config</code> object with one <code>Map[String, String]</code> with test configuration and another <code>Map[String, String]</code> with production config.</p> <pre><code>package com.github.mrpowers.spark.spec.sql\n\nobject Config {\n\n  var test: Map[String, String] = {\n    Map(\n      \"libsvmData\" -&gt; new java.io.File(\"./src/test/resources/sample_libsvm_data.txt\").getCanonicalPath,\n      \"somethingElse\" -&gt; \"hi\"\n    )\n  }\n\n  var production: Map[String, String] = {\n    Map(\n      \"libsvmData\" -&gt; \"s3a://my-cool-bucket/fun-data/libsvm.txt\",\n      \"somethingElse\" -&gt; \"whatever\"\n    )\n  }\n\n  var environment = sys.env.getOrElse(\"PROJECT_ENV\", \"production\")\n\n  def get(key: String): String = {\n    if (environment == \"test\") {\n      test(key)\n    } else {\n      production(key)\n    }\n  }\n\n}\n</code></pre> <p>The <code>Config.get()</code> method will grab values from the <code>test</code> or <code>production</code> map depending on the <code>PROJECT_ENV</code> value.</p> <p>Let's use the <code>sbt console</code> command to demonstrate this.</p> <pre><code>$ PROJECT_ENV=test sbt console\nscala&gt; com.github.mrpowers.spark.spec.sql.Config.get(\"somethingElse\")\nres0: String = hi\n</code></pre> <p>Let's restart the SBT console and run the same code in the production environment.</p> <pre><code>$ PROJECT_ENV=production sbt console\nscala&gt; com.github.mrpowers.spark.spec.sql.Config.get(\"somethingElse\")\nres0: String = whatever\n</code></pre> <p>Here is how the <code>Config</code> object can be used to fetch a file in your GitHub repository in the test environment and also fetch a file from S3 in the production environment.</p> <pre><code>val training = spark\n  .read\n  .format(\"libsvm\")\n  .load(Config.get(\"libsvmData\"))\n</code></pre> <p>This solution is elegant and does not clutter our application code with environment logic.</p>"},{"location":"apache-spark/environment-specific-configuration/#environment-specific-code-anitpattern","title":"Environment specific code anitpattern","text":"<p>Here is an example of how you should not add environment paths to your code.</p> <pre><code>var environment = sys.env.getOrElse(\"PROJECT_ENV\", \"production\")\nval training = if (environment == \"test\") {\n  spark\n    .read\n    .format(\"libsvm\")\n    .load(new java.io.File(\"./src/test/resources/sample_libsvm_data.txt\").getCanonicalPath)\n} else {\n  spark\n    .read\n    .format(\"libsvm\")\n    .load(\"s3a://my-cool-bucket/fun-data/libsvm.txt\")\n}\n</code></pre> <p>An anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive. - source</p> <p>You should never write code with different execution paths in the production and test environments because then your test suite won't really be testing the actual code that's run in production.</p>"},{"location":"apache-spark/environment-specific-configuration/#overriding-config","title":"Overriding config","text":"<p>The <code>Config.test</code> and <code>Config.production</code> maps are defined as variables (with the <code>var</code> keyword), so they can be overridden.</p> <pre><code>scala&gt; import com.github.mrpowers.spark.spec.sql.Config\nscala&gt; Config.get(\"somethingElse\")\nres1: String = hi\n\nscala&gt; Config.test = Config.test ++ Map(\"somethingElse\" -&gt; \"give me clean air\")\nscala&gt; Config.get(\"somethingElse\")\nres2: String = give me clean air\n</code></pre> <p>Giving users the ability to swap out config on the fly makes your codebase more flexible for a variety of use cases.</p>"},{"location":"apache-spark/environment-specific-configuration/#setting-the-project_env-variable-for-test-runs","title":"Setting the <code>PROJECT_ENV</code> variable for test runs","text":"<p>The <code>Config</code> object uses the production environment by default. You're not going to want to have to remember to set the <code>PROJECT_ENV</code> to test everytime you run your test suite (e.g. you don't want to type <code>PROJECT_ENV=test sbt test</code>).</p> <p>You can update your <code>build.sbt</code> file as follows to set <code>PROJECT_ENV</code> to test whenever the test suite is run.</p> <pre><code>fork in Test := true\nenvVars in Test := Map(\"PROJECT_ENV\" -&gt; \"test\")\n</code></pre> <p>Big thanks to the StackOverflow community for helping me figure this out.</p>"},{"location":"apache-spark/environment-specific-configuration/#other-implementations","title":"Other implementations","text":"<p>This StackOverflow thread discusses other solutions.</p> <p>One answer relies on an external library, one is in Java, and one doesn't allow for overrides. I will add an answer with the implementation discussed in this blog post now.</p>"},{"location":"apache-spark/environment-specific-configuration/#next-steps","title":"Next steps","text":"<p>Feel free to extend this solution to account for other environments. For example, you might want to add a staging environment that uses different paths to test code before it's run in production.</p> <p>Just remember to follow best practices and avoid the config anti-pattern that can litter your codebase and reduce the protection offered by your test suite.</p> <p>Adding <code>Config</code> objects to your functions adds a dependency you might not want. In a future blog post, we'll discuss how dependency injection can abstract these <code>Config</code> depencencies and how the <code>Config</code> object can be leveraged to access smart defaults - the best of both worlds!</p>"},{"location":"apache-spark/exact-percentile-approx-median/","title":"Calculating Percentile, Approximate Percentile, and Median with Spark","text":"<p>This blog post explains how to compute the percentile, approximate percentile and median of a column in Spark.</p> <p>There are a variety of different ways to perform these computations and it's good to know all the approaches because they touch different important sections of the Spark API.</p>"},{"location":"apache-spark/exact-percentile-approx-median/#percentile","title":"Percentile","text":"<p>You can calculate the exact percentile with the <code>percentile</code> SQL function.</p> <p>Suppose you have the following DataFrame:</p> <pre><code>+--------+\n|some_int|\n+--------+\n|       0|\n|      10|\n+--------+\n</code></pre> <p>Calculate the 50th percentile:</p> <pre><code>df\n  .agg(expr(\"percentile(some_int, 0.5)\").as(\"50_percentile\"))\n  .show()\n</code></pre> <pre><code>+-------------+\n|50_percentile|\n+-------------+\n|          5.0|\n+-------------+\n</code></pre> <p>Using <code>expr</code> to write SQL strings when using the Scala API isn't ideal. It's better to invoke Scala functions, but the <code>percentile</code> function isn't defined in the Scala API.</p> <p>The bebe library fills in the Scala API gaps and provides easy access to functions like percentile.</p> <pre><code>df\n  .agg(bebe_percentile(col(\"some_int\"), lit(0.5)).as(\"50_percentile\"))\n  .show()\n</code></pre> <pre><code>+-------------+\n|50_percentile|\n+-------------+\n|          5.0|\n+-------------+\n</code></pre> <p><code>bebe_percentile</code> is implemented as a Catalyst expression, so it's just as performant as the SQL percentile function.</p>"},{"location":"apache-spark/exact-percentile-approx-median/#approximate-percentile","title":"Approximate Percentile","text":"<p>Create a DataFrame with the integers between 1 and 1,000.</p> <pre><code>val df1 = (1 to 1000).toDF(\"some_int\")\n</code></pre> <p>Use the <code>approx_percentile</code> SQL method to calculate the 50th percentile:</p> <pre><code>df1\n  .agg(expr(\"approx_percentile(some_int, array(0.5))\").as(\"approx_50_percentile\"))\n  .show()\n</code></pre> <pre><code>+--------------------+\n|approx_50_percentile|\n+--------------------+\n|               [500]|\n+--------------------+\n</code></pre> <p>This <code>expr</code> hack isn't ideal. We don't like including SQL strings in our Scala code.</p> <p>Let's use the <code>bebe_approx_percentile</code> method instead.</p> <pre><code>df1\n  .select(bebe_approx_percentile(col(\"some_int\"), array(lit(0.5))).as(\"approx_50_percentile\"))\n  .show()\n</code></pre> <pre><code>+--------------------+\n|approx_50_percentile|\n+--------------------+\n|               [500]|\n+--------------------+\n</code></pre> <p>bebe lets you write code that's a lot nicer and easier to reuse.</p>"},{"location":"apache-spark/exact-percentile-approx-median/#median","title":"Median","text":"<p>The median is the value where fifty percent or the data values fall at or below it. Therefore, the median is the 50th percentile.</p> <p>Source</p> <p>We've already seen how to calculate the 50th percentile, or median, both exactly and approximately.</p>"},{"location":"apache-spark/exact-percentile-approx-median/#conclusion","title":"Conclusion","text":"<p>The Spark percentile functions are exposed via the SQL API, but aren't exposed via the Scala or Python APIs.</p> <p>Invoking the SQL functions with the expr hack is possible, but not desirable. Formatting large SQL strings in Scala code is annoying, especially when writing code that's sensitive to special characters (like a regular expression).</p> <p>It's best to leverage the bebe library when looking for this functionality. The bebe functions are performant and provide a clean interface for the user.</p>"},{"location":"apache-spark/expr-eval/","title":"Executing Spark code with expr and eval","text":"<p>You can execute Spark column functions with a genius combination of <code>expr</code> and <code>eval()</code>.</p> <p>This technique lets you execute Spark functions without having to create a DataFrame.</p> <p>This makes it easier to run code in the console and to run tests faster.</p>"},{"location":"apache-spark/expr-eval/#simple-example","title":"Simple example","text":"<p>Open up the Spark console and let's evaluate some code!</p> <p>Use the <code>lower</code> method defined in <code>org.apache.spark.sql.functions</code> to downcase the string \"HI THERE\".</p> <pre><code>import org.apache.spark.sql.functions._\n\nlower(lit(\"HI THERE\")).expr.eval() // hi there\n</code></pre> <p>Here's how this looks in a console:</p> <p></p> <p>Note that this code returns an Any value. It does not return a string.</p>"},{"location":"apache-spark/expr-eval/#array-example","title":"Array example","text":"<p>Let's use <code>array_contains</code> to see if <code>Array(\"this\", \"is\", \"cool\")</code> contains the string \"cool\":</p> <pre><code>val myArr = Array(\"this\", \"is\", \"cool\")\narray_contains(lit(myArr), \"cool\").expr.eval() // true\n</code></pre> <p>Let's check to make sure <code>myArr</code> doesn't contain the word \"blah\":</p> <pre><code>array_contains(lit(myArr), \"blah\").expr.eval() // false\n</code></pre> <p>What a nice way to play around with Spark functions!!</p>"},{"location":"apache-spark/expr-eval/#executing-column-methods","title":"Executing Column methods","text":"<p>Column methods are defined in org.apache.spark.sql.Column.</p> <p>Let's execute the <code>contains()</code> method defined in the Column class with expr and eval.</p> <pre><code>lit(\"i like tacos\").contains(\"tacos\").expr.eval() // true\n</code></pre> <p>Let's verify that \"i like tacos\" does not contain the word \"beans\".</p> <pre><code>lit(\"i like tacos\").contains(\"beans\").expr.eval() // false\n</code></pre>"},{"location":"apache-spark/expr-eval/#lower-in-a-dataframe","title":"lower in a DataFrame","text":"<p>Creating a DataFrames requires more typing than expr / eval.</p> <p>Let's execute the <code>lower</code> function in a DataFrame:</p> <pre><code>val df = Seq(\"HI THERE\").toDF(\"col1\")\n\ndf.withColumn(\"lower_col1\", lower($\"col1\")).show()\n\n+--------+----------+\n|    col1|lower_col1|\n+--------+----------+\n|HI THERE|  hi there|\n+--------+----------+\n</code></pre> <p>expr / eval is easier when you'd like to quickly execute a function.</p>"},{"location":"apache-spark/expr-eval/#scala-functions-vs-expr-eval","title":"Scala functions vs. expr / eval","text":"<p>Scala functions return typed values whereas expr / eval returns Any type values.</p> <p><code>lower(lit(\"HI THERE\")).expr.eval()</code> returns an <code>Any</code> value.</p> <p><code>\"HI THERE\".toLowerCase()</code> returns a <code>String</code> value.</p> <p></p> <p>We don't normally want to return Any type values. Let's create some helper methods.</p>"},{"location":"apache-spark/expr-eval/#abstracting","title":"Abstracting","text":"<p>Let's define an <code>evalString()</code> method that'll take a Column argument and return a String.</p> <pre><code>import org.apache.spark.sql.Column\n\ndef evalString(col: Column) = {\n  col.expr.eval().toString\n}\n</code></pre> <p>This lets us do less typing:</p> <pre><code>evalString(lower(lit(\"HI THERE\"))) // hi there\n</code></pre> <p>A regular function isn't nearly hacky enough! Let's use an implicit class to monkey patch the <code>Column</code> class, so we can call <code>evalString</code> directly on column objects.</p>"},{"location":"apache-spark/expr-eval/#hacking","title":"Hacking","text":"<p>Here's a code snippet from the spark-daria repo that extends the <code>Column</code> class with an <code>evalString()</code> method.</p> <pre><code>object ColumnExt {\n  implicit class ColumnMethods(col: Column) {\n    def evalString(): String = {\n      col.expr.eval().toString\n    }\n  }\n}\n</code></pre> <p>Let's <code>cd</code> into the spark-daria project directory and run <code>sbt console</code> to fire up a console with all the spark-daria code loaded.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.ColumnExt._\n\nlower(lit(\"HI THERE\")).evalString() // hi there\n</code></pre> <p>It's normally bad practice to extend Spark core classes. Only extend core classes when you feel like the extension adds a method that's missing from the API.</p>"},{"location":"apache-spark/expr-eval/#testing","title":"Testing","text":"<p>We typically need to create DataFrames to test column methods. With <code>evalString</code>, we can test column methods with standard Scala comparison operators.</p> <p>Here's a <code>myLowerClean</code> Column function that removes all whitespace and downcases a string:</p> <pre><code>def myLowerClean(col: Column): Column = {\n  lower(regexp_replace(col, \"\\\\s+\", \"\"))\n}\n</code></pre> <p>Here's how we can test <code>myLowerClean</code> with <code>evalString()</code>.</p> <pre><code>it(\"runs tests with evalString\") {\n  assert(myLowerClean(lit(\"  BOO     \")).evalString() === \"boo\")\n  assert(myLowerClean(lit(\" HOO   \")).evalString() === \"hoo\")\n}\n</code></pre> <p>This test is slower and more verbose when the spark-fast-tests <code>assertColumnEquality</code> method is used.</p> <pre><code>it(\"assertColumnEquality approach\") {\n  val df = spark.createDF(\n    List(\n      (\"  BOO     \", \"boo\"),\n      (\" HOO   \", \"hoo\"),\n      (null, null)\n    ), List(\n      (\"cry\", StringType, true),\n      (\"expected\", StringType, true)\n    )\n  ).withColumn(\n    \"clean_cry\",\n    myLowerClean(col(\"cry\"))\n  )\n\n  assertColumnEquality(df, \"clean_cry\", \"expected\")\n}\n</code></pre> <p>You should read Testing Spark Code if you'd like to learn more about how the expr / eval design pattern can be used in Spark test suites.</p> <p>The spark-fast-tests README contains more detailed benchmarking results.</p> <p>expr / eval can be a powerful testing technique, but users need to know the limitations.</p>"},{"location":"apache-spark/expr-eval/#picky-parens","title":"Picky parens","text":"<p>You may have noticed that <code>.expr</code> doesn't have parens and <code>eval()</code> does have parens.</p> <pre><code>lower(lit(\"HI THERE\")).expr.eval()\n</code></pre> <p>The code will error out if <code>expr</code> is given parens (e.g. <code>lower(lit(\"HI THERE\")).expr().eval()</code>).</p> <p>The code will also error out if <code>eval()</code> doesn't have parens (e.g. <code>lower(lit(\"HI THERE\")).expr.eval</code>).</p> <p>You need to be super careful about your paren placement for this design pattern!</p>"},{"location":"apache-spark/expr-eval/#sparks-object-model","title":"Spark's object model","text":"<p>You should always dig into Spark's object model to understand return values at every step of a long method chain to better understand how Spark works.</p> <p>Let's break down the objects in the code snippet we keep using: <code>lower(lit(\"HI THERE\")).expr.eval()</code>.</p> <ul> <li><code>lit(\"HI THERE\")</code> returns a Column object. <code>lit()</code> is defined in the functions object.</li> <li><code>lower()</code> is also defined in the functions object and also returns a Column</li> <li>The Column documentation page has this note: \"The internal Catalyst expression can be accessed via expr, but this method is for debugging purposes only and can change in any future Spark releases.\" This design pattern isn't as robust as I thought ;)</li> <li>We're deep in Spark now. Think <code>eval()</code> is defined somewhere in this file. Not sure.</li> </ul> <p>Always keep digging into Spark's object model when you encounter a magical code snippet to learn more about how Spark works.</p>"},{"location":"apache-spark/expr-eval/#conclusion","title":"Conclusion","text":"<p>The expr / eval design pattern lets you easily evaluate code via the console and provides a powerful testing pattern because it's so fast.</p> <p>Testing Spark Applications is the best resource to learn more about how this design pattern can be used in production.</p>"},{"location":"apache-spark/filter-where/","title":"Important Considerations when filtering in Spark with filter and where","text":"<p>This blog post explains how to filter in Spark and discusses the vital factors to consider when filtering.</p> <p>Poorly executed filtering operations are a common bottleneck in Spark analyses.</p> <p>You need to make sure your data is stored in a format that is efficient for Spark to query. You also need to make sure the number of memory partitions after filtering is appropriate for your dataset.</p> <p>Executing a filtering query is easy\u2026 filtering well is difficult. Read the Beautiful Spark book if you want to learn how to create create data lakes that are optimized for performant filtering operations.</p> <p>Read this blog post closely. Filtering properly will make your analyses run faster and save your company money. It's the easiest way to become a better Spark programmer.</p>"},{"location":"apache-spark/filter-where/#filter-basics","title":"Filter basics","text":"<p>Let's create a DataFrame and view the contents:</p> <pre><code>val df = Seq(\n  (\"famous amos\", true),\n  (\"oreo\", true),\n  (\"ginger snaps\", false)\n).toDF(\"cookie_type\", \"contains_chocolate\")\n</code></pre> <pre><code>df.show()\n\n+------------+------------------+\n| cookie_type|contains_chocolate|\n+------------+------------------+\n| famous amos|              true|\n|        oreo|              true|\n|ginger snaps|             false|\n+------------+------------------+\n</code></pre> <p>Now let's filter the DataFrame to only include the rows with <code>contains_chocolate</code> equal to <code>true</code>.</p> <pre><code>val filteredDF = df.where(col(\"contains_chocolate\") === lit(true))\n\nfilteredDF.show()\n\n+-----------+------------------+\n|cookie_type|contains_chocolate|\n+-----------+------------------+\n|famous amos|              true|\n|       oreo|              true|\n+-----------+------------------+\n</code></pre> <p>There are various alternate syntaxes that give you the same result and same performance.</p> <ul> <li><code>df.where(\"contains_chocolate = true\")</code></li> <li><code>df.where($\"contains_chocolate\" === true)</code></li> <li><code>df.where('contains_chocolate === true)</code></li> </ul> <p>A separate section towards the end of this blog post demonstrates that all of these syntaxes generate the same execution plan, so they'll all perform equally.</p> <p><code>where</code> is an alias for <code>filter</code>, so all these work as well:</p> <ul> <li><code>df.filter(col(\"contains_chocolate\") === lit(true))</code></li> <li><code>df.filter(\"contains_chocolate = true\")</code></li> <li><code>df.filter($\"contains_chocolate\" === true)</code></li> <li><code>df.filter('contains_chocolate === true)</code></li> </ul>"},{"location":"apache-spark/filter-where/#empty-partition-problem","title":"Empty partition problem","text":"<p>A filtering operation does not change the number of memory partitions in a DataFrame.</p> <p>Suppose you have a data lake with 25 billion rows of data and 60,000 memory partitions. Suppose you run a filtering operation that results in a DataFrame with 10 million rows. After filtering, you'll still have 60,000 memory partitions, many of which will be empty. You'll need to run <code>repartition()</code> or <code>coalesce()</code> to spread the data on an appropriate number of memory partitions.</p> <p>Let's look at some pseudocode:</p> <pre><code>val df = spark.read.parquet(\"/some/path\") // 60,000 memory partitions\nval filteredDF = df.filter(col(\"age\") &gt; 98) // still 60,000 memory partitions\n// at this point, any operations performed on filteredDF will be super inefficient\nval repartitionedDF = filtereDF.repartition(200) // down to 200 memory partitions\n</code></pre> <p>Let's use the <code>person_data.csv</code> file that contains 100 rows of data and <code>person_name</code> and <code>person_country</code> columns to demonstrate this on a real dataset.</p> <p>80 people are from China, 15 people are from France, and 5 people are from Cuba.</p> <p>This code reads in the <code>person_data.csv</code> file and repartitions the data into 200 memory partitions.</p> <pre><code>val path = new java.io.File(\"./src/test/resources/person_data.csv\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .csv(path)\n  .repartition(200)\n\nprintln(df.rdd.partitions.size) // 200\n</code></pre> <p>Let's filter the DataFrame and verify that the number of memory partitions does not change:</p> <pre><code>val filteredDF = df.filter(col(\"person_country\") === \"Cuba\")\nprintln(filteredDF.rdd.partitions.size) // 200\n</code></pre> <p>There are only 5 rows of Cuba data and 200 memory partitions, so we know that at least 195 memory partitions are empty.</p> <p>Having a lot of empty memory partitions significantly slows down analyses on production-sized datasets.</p>"},{"location":"apache-spark/filter-where/#selecting-an-appropriate-number-of-memory-partitions","title":"Selecting an appropriate number of memory partitions","text":"<p>Choosing the right number of memory partitions after filtering is difficult.</p> <p>You can follow the 1GB per memory partition rule of thumb to estimate the number of memory partitions that'll be appropriate for a filtered dataset.</p> <p>Suppose you have 25 billion rows of data, which is 10 terabytes on disk (10,000 GB).</p> <p>An extract with 500 million rows (2% of the total data) is probably around 200 GB of data (0.02 * 10,000), so 200 memory partitions should work well.</p>"},{"location":"apache-spark/filter-where/#underlying-data-stores","title":"Underlying data stores","text":"<p>Filtering operations execute completely differently depending on the underlying data store.</p> <p>Spark attempts to \"push down\" filtering operations to the database layer whenever possible because databases are optimized for filtering. This is called predicate pushdown filtering.</p> <p>An operation like <code>df.filter(col(\"person_country\") === \"Cuba\")</code> is executed differently depending on if the data store supports predicate pushdown filtering.</p> <ul> <li>A parquet lake will send all the data to the Spark cluster, and perform the filtering operation on the Spark cluster</li> <li>A Postgres database table will perform the filtering operation in Postgres, and then send the resulting data to the Spark cluster.</li> </ul> <p>N.B. using a data lake that doesn't allow for query pushdown is a common, and potentially massive bottleneck.</p>"},{"location":"apache-spark/filter-where/#column-pruning","title":"Column pruning","text":"<p>Spark will use the minimal number of columns possible to execute a query.</p> <p>The <code>df.select(\"person_country\").distinct()</code> query will be executed differently depending on the file format:</p> <ul> <li>A Postgres database will perform the filter at the database level and only send a subset of the <code>person_country</code> column to the cluster</li> <li>A Parquet data store will send the entire <code>person_country</code> column to the cluster and perform the filtering on the cluster (it doesn't send the <code>person_name</code> column - that column is \"pruned\")</li> <li>A CSV data store will send the entire dataset to the cluster. CSV is a row based file format and row based file formats don't support column pruning.</li> </ul> <p>You almost always want to work with a file format or database that supports column pruning for your Spark analyses.</p>"},{"location":"apache-spark/filter-where/#cluster-sizing-after-filtering","title":"Cluster sizing after filtering","text":"<p>Depending on the data store, the cluster size needs might be completely different before and after performing a filtering operation.</p> <p>Let's say your 25 billion row dataset is stored in a parquet data lake and you need to perform a big filter and then do some advanced NLP on 1 million rows. You'll need a big cluster to perform the initial filtering operation and a smaller cluster to perform the NLP analysis on the comparatively tiny dataset. For workflows like these, it's often better to perform the filtering operation on a big cluster, repartition the data, write it to disk, and then perform the detailed analysis with a separate, smaller cluster on the extract.</p> <p>Transferring big datasets from cloud storage to a cloud cluster and performing a big filtering operation is slow and expensive. You will generate a huge cloud compute bill with these types of workflows.</p> <p>The pre / post filtering cluster requirements don't change when you're using a data storage that allows for query pushdown. The filtering operation is not performed in the Spark cluster. So you only need to use a cluster that can handle the size of the filtered dataset.</p>"},{"location":"apache-spark/filter-where/#partition-filters","title":"Partition filters","text":"<p>Data lakes can be partitioned on disk with partitionBy.</p> <p>If the data lake is partitioned, Spark can use PartitionFilters, as long as the filter is using the partition key.</p> <p>In our example, we could make a partitioned data lake with the <code>person_country</code> partition key as follows:</p> <pre><code>val path = new java.io.File(\"./src/test/resources/person_data.csv\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .csv(path)\n  .repartition(col(\"person_country\"))\n\ndf\n  .write\n  .partitionBy(\"person_country\")\n  .option(\"header\", \"true\")\n  .csv(\"tmp/person_data_partitioned\")\n</code></pre> <p>This'll write out the data as follows:</p> <pre><code>person_data_partitioned/\n  person_country=China/\n    part-00059-dd8849eb-4e7d-4b6c-9536-59f94ea56412.c000.csv\n  person_country=Cuba/\n    part-00086-dd8849eb-4e7d-4b6c-9536-59f94ea56412.c000.csv\n  person_country=France/\n    part-00030-dd8849eb-4e7d-4b6c-9536-59f94ea56412.c000.csv\n</code></pre> <p>The \"partition key\" is <code>person_country</code>. Let's use <code>explain</code> to verify that PartitionFilters are used when filtering on the partition key.</p> <pre><code>val partitionedPath = new java.io.File(\"tmp/person_data_partitioned\").getCanonicalPath\nspark\n  .read\n  .csv(partitionedPath)\n  .filter(col(\"person_country\") === \"Cuba\")\n  .explain()\n</code></pre> <pre><code>FileScan csv [_c0#132,person_country#133]\n  Batched: false,\n  Format: CSV,\n  Location: InMemoryFileIndex[file:/Users/matthewpowers/Documents/code/my_apps/mungingdata/spark2/tmp/person_...,\n  PartitionCount: 1,\n  PartitionFilters: [isnotnull(person_country#133), (person_country#133 = Cuba)],\n  PushedFilters: [],\n  ReadSchema: struct&lt;_c0:string&gt;\n</code></pre> <p>Check out Beautiful Spark Code for a full description on how to build, update, and filter partitioned data lakes.</p>"},{"location":"apache-spark/filter-where/#explain-with-different-filter-syntax","title":"Explain with different filter syntax","text":"<p><code>filter</code> and <code>where</code> are executed the same, regardless of whether column arguments or SQL strings are used.</p> <p>Let's verify that all the different filter syntaxes generate the same physical plan.</p> <p>All of these code snippets generate the same physical plan:</p> <pre><code>df.where(\"person_country = 'Cuba'\").explain()\ndf.where($\"person_country\" === \"Cuba\").explain()\ndf.where('person_country === \"Cuba\").explain()\ndf.filter(\"person_country = 'Cuba'\").explain()\n</code></pre> <p>Here's the generated physical plan:</p> <pre><code>== Physical Plan ==\n(1) Project [person_name#152, person_country#153]\n+- (1) Filter (isnotnull(person_country#153) &amp;&amp; (person_country#153 = Cuba))\n   +- (1) FileScan csv [person_name#152,person_country#153]\n          Batched: false,\n      Format: CSV,\n          Location: InMemoryFileIndex[file:/Users/matthewpowers/Documents/code/my_apps/mungingdata/spark2/src/test/re...,\n          PartitionFilters: [],\n          PushedFilters: [IsNotNull(person_country),\n          EqualTo(person_country,Cuba)],\n          ReadSchema: struct&lt;person_name:string,person_country:string&gt;\n</code></pre>"},{"location":"apache-spark/filter-where/#incremental-updates-with-filter","title":"Incremental updates with filter","text":"<p>Some filtering operations are easy to incrementally update with Structured Streaming + Trigger.Once.</p> <p>See this blog post for more details.</p> <p>Incrementally updating a dataset is often 100 times faster than rerunning the query on the entire dataset.</p>"},{"location":"apache-spark/filter-where/#conclusion","title":"Conclusion","text":"<p>There are different syntaxes for filtering Spark DataFrames that are executed the same under the hood.</p> <p>Optimizing filtering operations depends on the underlying data store. Your queries will be a lot more performant if the data store supports predicate pushdown filters.</p> <p>If you're working with a data storage format that doesn't support predicate pushdown filters, try to create a partitioned data lake and leverages partition filters.</p> <p>Transferring large datasets to the Spark cluster and performing the filtering in Spark is generally the slowest and most costly option. Avoid this query pattern whenever possible.</p> <p>Filtering a Spark dataset is easy, but filtering in a performant, cost efficient manner is surprisingly hard. Filtering is a common bottleneck in Spark analyses.</p>"},{"location":"apache-spark/frameless-typed-datasets/","title":"Expressively Typed Spark Datasets with Frameless","text":"<p>frameless is a great library for writing Datasets with expressive types. The library helps users write correct code with descriptive compile time errors instead of runtime errors with long stack traces.</p> <p>This blog post shows how to build typed datasets with frameless. It demonstrates the improved error messages, explains how to add columns / run functions, and discusses how the library could be improved.</p>"},{"location":"apache-spark/frameless-typed-datasets/#create-datasets","title":"Create datasets","text":"<p>Let's create a regular Spark dataset using the built-in functions.</p> <pre><code>import org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nval conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Frameless repl\").set(\"spark.ui.enabled\", \"false\")\nimplicit val spark = SparkSession.builder().config(conf).appName(\"REPL\").getOrCreate()\nspark.sparkContext.setLogLevel(\"WARN\")\nimport spark.implicits._\n\ncase class City(name: String, population: Double)\nval cities = Seq(\n  City(\"Manila\", 12.8),\n  City(\"Brasilia\", 2.5),\n  City(\"Lagos\", 14.4)\n)\nval citiesDS = spark.createDataset(cities)\n</code></pre> <p>Let's display the dataset contents:</p> <pre><code>citiesDS.show()\n\n+--------+----------+\n|    name|population|\n+--------+----------+\n|  Manila|      12.8|\n|Brasilia|       2.5|\n|   Lagos|      14.4|\n+--------+----------+\n</code></pre> <p>We can build a typed dataset with the frameless API in a similar manner.</p> <pre><code>import frameless.TypedDataset\nimport frameless.syntax._\nval citiesTDS = TypedDataset.create(cities)\n</code></pre> <p><code>citiesDS</code> is a \"regular\" Spark dataset and <code>citiesTDS</code> is a frameless typed dataset.</p> <p>Let's display the contents of the typed dataset.</p> <pre><code>citiesTDS.dataset.show()\n\n+--------+----------+\n|    name|population|\n+--------+----------+\n|  Manila|      12.8|\n|Brasilia|       2.5|\n|   Lagos|      14.4|\n+--------+----------+\n</code></pre>"},{"location":"apache-spark/frameless-typed-datasets/#selecting-a-column","title":"Selecting a column","text":"<p>Select a column from the Spark Dataset and display the contents to the screen.</p> <pre><code>val cities = citiesDS.select(\"population\")\ncities.show()\n\n+----------+\n|population|\n+----------+\n|      12.8|\n|       2.5|\n|      14.4|\n+----------+\n</code></pre> <p>Now select a column from the typed dataset and display the contents to the screen.</p> <pre><code>val cities: TypedDataset[Double] = citiesTDS.select(citiesTDS('population))\ncities.dataset.show()\n</code></pre> <pre><code>+----+\n|  _1|\n+----+\n|12.8|\n| 2.5|\n|14.4|\n+----+\n</code></pre> <p>Typed datasets provide better error messages if you try to select columns that are not present. Here's the error message if you try to select a <code>continent</code> column from the regular Spark dataset with <code>citiesDS.select(\"continent\")</code>.</p> <pre><code>[info]   org.apache.spark.sql.AnalysisException: cannot resolve '`continent`' given input columns: [name, population];;\n[info] 'Project ['continent]\n[info] +- LocalRelation [name#30, population#31]\n[info]   at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n[info]   at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n[info]   at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n[info]   at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n</code></pre> <p>This is a runtime error. The Spark Dataset API does not catch this error at compile time.</p> <p>Trying to access a column that doesn't exist with the frameless API yields a compile error. Here's the error for <code>citiesTDS.select(DatasetCreator.citiesTDS('continent))</code>:</p> <pre><code>[info] Compiling 1 Scala source to /Users/powers/Documents/code/my_apps/spark-frameless/target/scala-2.11/test-classes ...\n[error] /Users/powers/Documents/code/my_apps/spark-frameless/src/test/scala/mrpowers/spark/frameless/CitiesSpec.scala:36:46: No column Symbol with shapeless.tag.Tagged[String(\"continent\")] of type A in mrpowers.spark.frameless.DatasetCreator.City\n[error]     citiesTDS.select(DatasetCreator.citiesTDS('continent))\n[error]                                              ^\n[error] one error found\n[error] (Test / compileIncremental) Compilation failed\n</code></pre> <p>frameless gives a descriptive compile time error message that's easier to decipher than the standard runtime error.</p>"},{"location":"apache-spark/frameless-typed-datasets/#why-compile-time-error-messages-are-better","title":"Why compile time error messages are better","text":"<p>Compile time errors are better than runtime errors.</p> <p>Lots of Spark jobs are run with this workflow:</p> <ul> <li>Write some code</li> <li>Compile the code</li> <li>Attach the JAR file to a cluster</li> <li>Run the code in production</li> </ul> <p>Our previous example demonstrates that the native Spark Dataset API will let you compile code that references columns that aren't in the underlying dataset. You can easily compile code that's not correct and not notice till you run your job in production.</p> <p>Spark programmers try to minimize the risk of runtime errors with spark-daria DataFrame validation checks or by Testing Spark Applications.</p> <p>The frameless philosophy is to rely on automated compile time checks rather than manually checking the correctness of all aspects of the program. We can already see how frameless helps you write better code. Let's check out more cool features!</p>"},{"location":"apache-spark/frameless-typed-datasets/#adding-columns","title":"Adding columns","text":"<p>You can add a column to a typed dataset with <code>withColumn</code>, but the entire dataset schema must be supplied.</p> <pre><code>import frameless.functions._\n\ncase class City2(name: String, population: Double, greeting: String)\nval tds2 = citiesTDS.withColumn[City2](lit(\"hi\"))\ntds2.dataset.show()\n</code></pre> <pre><code>+--------+----------+--------+\n|    name|population|greeting|\n+--------+----------+--------+\n|  Manila|      12.8|      hi|\n|Brasilia|       2.5|      hi|\n|   Lagos|      14.4|      hi|\n+--------+----------+--------+\n</code></pre> <p>Supplying an entirely new schema when adding a single column isn't easy, especially for datasets with a lot of columns. You can also add a column with <code>withColumnTupled</code>.</p> <pre><code>val tds2 = citiesTDS.withColumnTupled(lit(\"hi\"))\ntds2.dataset.show()\n</code></pre> <pre><code>+--------+----+---+\n|      _1|  _2| _3|\n+--------+----+---+\n|  Manila|12.8| hi|\n|Brasilia| 2.5| hi|\n|   Lagos|14.4| hi|\n+--------+----+---+\n</code></pre> <p>You can inspect the schema with <code>tds2.dataset.printSchema()</code> to see that all the dataset column names are now _1, _2, _n.</p> <pre><code>root\n |-- _1: string (nullable = false)\n |-- _2: double (nullable = false)\n |-- _3: string (nullable = false)\n</code></pre> <p>This is far from ideal. We don't want to lose existing column names when adding a new column. Let's hope a new method is added to the API to make it a bit easier to add columns.</p> <p>The docs discuss using <code>asCol</code> as a potential workaround for this issue.</p>"},{"location":"apache-spark/frameless-typed-datasets/#functions","title":"Functions","text":"<p>Let's append \"is fun\" to all the city names.</p> <pre><code>import frameless.functions.nonAggregate._\n\nval cities = citiesTDS.select(\n  concat(citiesTDS('name), lit(\" is fun\")),\n  citiesTDS('population)\n)\ncities.dataset.show()\n</code></pre> <p>Here's what's printed:</p> <pre><code>+---------------+----+\n|             _1|  _2|\n+---------------+----+\n|  Manila is fun|12.8|\n|Brasilia is fun| 2.5|\n|   Lagos is fun|14.4|\n+---------------+----+\n</code></pre>"},{"location":"apache-spark/frameless-typed-datasets/#conclusion","title":"Conclusion","text":"<p>frameless is a really cool library that's still being actively developed and is already used by many companies for their production workflows.</p> <p>Catching errors at compile time is always better than compiling code and dealing with production runtime issues.</p> <p>See this repo for all the code from this post.</p> <p>Hopefully a new method will be added to the API that'll make it easier to add columns to a typed dataset.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/","title":"Using HyperLogLog for count distinct computations with Spark","text":"<p>This blog post explains how to use the HyperLogLog algorithm to perform fast count distinct operations.</p> <p>HyperLogLog sketches can be generated with spark-alchemy, loaded into Postgres databases, and queried with millisecond response times.</p> <p>Let's start by exploring the built-in Spark approximate count functions and explain why it's not useful in most situations.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#simple-example-with-approx_count_distinct","title":"Simple example with <code>approx_count_distinct</code>","text":"<p>Suppose we have the following <code>users1.csv</code> file:</p> <pre><code>user_id,first_name\n1,bob\n1,bob\n2,cathy\n2,cathy\n3,ming\n</code></pre> <p>Let's use the <code>approx_count_distinct</code> function to estimate the unique number of distinct <code>user_id</code> values in the dataset.</p> <pre><code>val path1 = new java.io.File(\"./src/test/resources/users1.csv\").getCanonicalPath\n\nval df1 = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path1)\n\ndf1\n  .agg(approx_count_distinct(\"user_id\").as(\"approx_user_id_count\"))\n  .show()\n\n+--------------------+\n|approx_user_id_count|\n+--------------------+\n|                   3|\n+--------------------+\n</code></pre> <p><code>approx_count_distinct</code> uses the HyperLogLog algorithm under the hood and will return a result faster than a precise count of the distinct tokens (i.e. <code>df1.select(\"user_id\").distinct().count()</code> will run slower).</p> <p><code>approx_count_distinct</code> is good for an ad-hoc query, but won't help you build a system with HyperLogLog sketches that can be queried with milisecond response times.</p> <p>Let's review the count distinct problem at a high level and dive into an open source library that does allow for millisecond response times.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#overview-of-count-distinct-problem","title":"Overview of count distinct problem","text":"<p>Precise distinct counts can not be reaggregated and updated incrementally.</p> <p>If you have 5 unique visitors to a website and 3 unique visitors on day two, how many total visitors have you had to your site? Between 5 and 8.</p> <p>Suppose the unique visitors at the end of day 1 is stored as an integer (5). On day 2, you won't be able to use the day 1 unique count when calculating the updated number of unique visitors. You'll need to rerun the entire count distinct query on the entire dataset.</p> <p>HyperLogLog sketches are reaggregatable and can be incrementally updated.</p> <p>The native Spark approx_count_distinct function does not expose the underlying HLL sketch, so it does allow users to build systems with incremental updates.</p> <p>Sim has a great talk on HyperLogLogs and reaggregation with more background information.</p> <p>Let's turn to an open source library that exposes HLL sketches.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#simple-example-with-spark-alchemy","title":"Simple example with spark-alchemy","text":"<p>The open source spark-alchemy library makes it easy to create, merge, and calculate the number of distinct items in a HyperLogLog sketch.</p> <p>Let's use the <code>hll_init</code> function to append a HyperLogLog sketch to each row of data in a DataFrame.</p> <pre><code>import com.swoop.alchemy.spark.expressions.hll.functions._\n\nval path1 = new java.io.File(\"./src/test/resources/users1.csv\").getCanonicalPath\n\nval df1 = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path1)\n\ndf1\n  .withColumn(\"user_id_hll\", hll_init(\"user_id\"))\n  .show()\n\n+-------+----------+--------------------+\n|user_id|first_name|         user_id_hll|\n+-------+----------+--------------------+\n|      1|       bob|[FF FF FF FE 09 0...|\n|      1|       bob|[FF FF FF FE 09 0...|\n|      2|     cathy|[FF FF FF FE 09 0...|\n|      2|     cathy|[FF FF FF FE 09 0...|\n|      3|      ming|[FF FF FF FE 09 0...|\n+-------+----------+--------------------+\n</code></pre> <p>Let's verify that the <code>user_id_hll</code> is a <code>BinaryType</code> column:</p> <pre><code>df1\n  .withColumn(\"user_id_hll\", hll_init(\"user_id\"))\n  .printSchema()\n\nroot\n |-- user_id: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- user_id_hll: binary (nullable = true)\n</code></pre> <p>Let's use the <code>hll_merge</code> function to merge all of the HLL sketckes into a single row of data:</p> <pre><code>df1\n  .withColumn(\"user_id_hll\", hll_init(\"user_id\"))\n  .select(hll_merge(\"user_id_hll\").as(\"user_id_hll\"))\n  .show()\n\n+--------------------+\n|         user_id_hll|\n+--------------------+\n|[FF FF FF FE 09 0...|\n+--------------------+\n</code></pre> <p>Write out the HyperLogLog sketch to disk and use the <code>hll_cardinality()</code> function to estimate the number of unique <code>user_id</code> values in the sketch.</p> <pre><code>val sketchPath1 = new java.io.File(\"./tmp/sketches/file1\").getCanonicalPath\n\ndf1\n  .withColumn(\"user_id_hll\", hll_init(\"user_id\"))\n  .select(hll_merge(\"user_id_hll\").as(\"user_id_hll\"))\n  .write\n  .parquet(sketchPath1)\n\nval sketch1 = spark.read.parquet(sketchPath1)\n\nsketch1\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           3|\n+----------------------------+\n</code></pre> <p>Let's look at how we can incrementally update the HyperLogLog sketch and rerun the <code>hll_cardinality</code> computation.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#incrementally-updating-hyperloglog-sketch","title":"Incrementally updating HyperLogLog sketch","text":"<p>Suppose we have the following <code>users2.csv</code> file:</p> <pre><code>user_id,first_name\n1,bob\n1,bob\n1,bob\n1,bob\n2,cathy\n8,camilo\n9,maria\n</code></pre> <p>Our new data file has two new <code>user_id</code> values (8 and 9) and two existing <code>user_id</code> values (<code>user_id</code> 1 and 2).</p> <p>Let's build a HyperLogLog sketch with the new data, merge the new HLL sketch with the existing HyperLogLog sketch we wrote to disk, and rerun the <code>hll_cardinality</code> computation.</p> <pre><code>val path2 = new java.io.File(\"./src/test/resources/users2.csv\").getCanonicalPath\n\nval df2 = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path2)\n\ndf2\n  .withColumn(\"user_id_hll\", hll_init(\"user_id\"))\n  .select(\"user_id_hll\")\n  .union(sketch1)\n  .select(hll_merge(\"user_id_hll\").as(\"user_id_hll\"))\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           5|\n+----------------------------+\n</code></pre> <p>We don't need to rebuild the HyperLogLog sketch for the <code>users1.csv</code> file - we can use the existing HLL sketch. HyperLogLogs are reaggregatable can be incrementally updated quickly.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#building-cohorts","title":"Building cohorts","text":"<p>Suppose we have the following data on some gamers:</p> <pre><code>user_id,favorite_game,age\nsean,halo,36\npowers,smash,34\ncohen,smash,33\nangel,pokemon,32\nmadison,portal,24\npete,mario_maker,8\nnora,smash,7\n</code></pre> <p>The business would like a count of all the gamers that are adults.</p> <pre><code>val path = new java.io.File(\"./src/test/resources/gamers.csv\").getCanonicalPath\n\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval adults = df\n  .where(col(\"age\") &gt;= 18)\n  .select(hll_init_agg(\"user_id\").as(\"user_id_hll\"))\n\nadults\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           5|\n+----------------------------+\n</code></pre> <p>This result makes sense: sean, powers, cohen, angel, and madison are all adults in our dataset.</p> <p>The business would also like a count of all the gamers with a favorite game of smash.</p> <pre><code>val favoriteGameSmash = df\n  .where(col(\"favorite_game\") === \"smash\")\n  .select(hll_init_agg(\"user_id\").as(\"user_id_hll\"))\n\nfavoriteGameSmash\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           3|\n+----------------------------+\n</code></pre> <p>powers, cohen, and nora like smash in our dataset.</p> <p>The business would also like a count of all the gamers that are adults or have a smash as their favorite game (this is called a cohort).</p> <pre><code>adults\n  .union(favoriteGameSmash)\n  .select(hll_merge(\"user_id_hll\").as(\"user_id_hll\"))\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           6|\n+----------------------------+\n</code></pre> <p>We can easily union cohorts without reprocessing data.</p> <p>Suppose the adults HLL sketch is stored in one row and the smash favorite game HLL sketch is stored in another row of data. We only need to process two rows of data to derive the count of adults or gamers that like smash.</p> <p>In a real world application, you can generate a bunch of HLL sketches that are incrementally updated. You can union these HLL sketches as you wish with millisecond response times.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#interoperability-and-web-speed-response-times","title":"Interoperability and web speed response times","text":"<p>You can use <code>hll_convert</code> to load the HLL sketches in a Postgres database so they can be queried rapidly.</p> <p>We could create a database table with two columns:</p> <pre><code>| cohort_name  | hll_sketch         |\n|--------------|--------------------|\n| adults       | binary_data        |\n| smash_lovers | binary_data        |\n</code></pre> <p>Generating counts is a simple matter of querying a few rows of data.</p> <p>Querying fewer rows of data is how to make big data distinct counts fast.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#hyperloglog-sketches-with-groupby","title":"HyperLogLog sketches with groupBy","text":"<p>We can use the <code>hll_init_agg</code> function to compute the number of adult and non-adult smash fans.</p> <pre><code>val path = new java.io.File(\"./src/test/resources/gamers.csv\").getCanonicalPath\n\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n  .withColumn(\"is_adult\", col(\"age\") &gt;= 18)\n\nval resDF = df\n  .groupBy(\"is_adult\", \"favorite_game\")\n  .agg(hll_init_agg(\"user_id\").as(\"user_id_hll\"))\n\n\n// number of adults that like smash\nresDF\n  .where(col(\"is_adult\") &amp;&amp; col(\"favorite_game\") === \"smash\")\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           2|\n+----------------------------+\n\n// number of children that like smash\nresDF\n  .where(!col(\"is_adult\") &amp;&amp; col(\"favorite_game\") === \"smash\")\n  .select(hll_cardinality(\"user_id_hll\"))\n  .show()\n\n+----------------------------+\n|hll_cardinality(user_id_hll)|\n+----------------------------+\n|                           1|\n+----------------------------+\n</code></pre> <p>The <code>hll_init_agg</code> functions allows for data to be grouped and opens up new options for how distinct counts can be sliced and diced.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#incremental-updates-with-delta-lake","title":"Incremental updates with Delta lake","text":"<p>Let me know if you're interested in learning more about how Delta lake makes it easier to incrementally update HLL sketches and I'll populate this section.</p>"},{"location":"apache-spark/hyperloglog-count-distinct/#conclusion","title":"Conclusion","text":"<p>Distinct counts are expensive to compute and difficult incrementally update.</p> <p>The HyperLogLog algorithm makes it easy to quickly compute distinct counts.</p> <p>Businesses often want to build cohorts with web-speed response times.</p> <p>You can use the spark-alchemy library to precompute HLL sketches, store the HLL sketches in a Postgres database, and return cohort counts with millisecond response times.</p>"},{"location":"apache-spark/incrementally-updating-extracts/","title":"Incrementally Updating Extracts with Spark","text":"<p>Spark Structured Streaming and <code>Trigger.Once</code> can be used to incrementally update Spark extracts with ease.</p> <p>An extract that updates incrementally will take the same amount of time as a normal extract for the initial run, but subsequent runs will execute much faster.</p> <p>I recently used this technology to refactor a batch job that took 11.5 hours to execute with an incrementally updating extract that only takes 8 minutes to run (on a cluster that's three times smaller)!</p> <p>Incremental updates save a lot of time and money!</p>"},{"location":"apache-spark/incrementally-updating-extracts/#batch-extracts","title":"Batch Extracts","text":"<p>Here's how to extract all rows with an age less than 18 from a data lake.</p> <p>We'll use the spark-daria <code>EtlDefinition</code> object to wire up the extract, as described in this blog post.</p> <pre><code>val lakeDF = spark.read.parquet(\"s3a://some-bucket/my-data-lake\")\n\ndef filterMinors()(df: DataFrame): DataFrame = {\n  df\n    .filter(col(\"age\") &lt; 18)\n    .repartition(2000)\n}\n\ndef exampleWriter()(df: DataFrame): Unit = {\n  val path = \"s3a://some-bucket/extracts/adults\"\n  df.write.mode(SaveMode.Overwrite).parquet(path)\n}\n\nval etl = new EtlDefinition(\n  sourceDF = lakeDF,\n  transform = filterMinors(),\n  write = exampleWriter()\n)\n\netl.process()\n</code></pre> <p>In batch mode, you always need to manually repartition an extract after filtering from the data lake, as described in this blog post.</p> <p>It's hard to determine the optimal number of partitions to use for each extract. You can estimate the number of optimal partitions by measuring the size of your entire data lake and assuming each row contains the same amount of data.</p> <p>Let's say your data lake contains 10 billion rows and 2,000 GB of data. Let's say you run an extract that contains 1 billion rows and you'd like each partition of the extract to contain 100 MB (0.1 GB) of data. You can assume the extract will contain 200 GB of data (because the extract is only 10% the size of the entire lake) and you'll need 2,000 partitions for each partition to contain 100 MB of data.</p> <p>Manually specifying the number of partitions is annoying and requires a bit of trial and error to get right. You're chasing a moving target as the data lake grows and the number of optimal partitions increases.</p> <p>Whenever the dataset is filtered, repartitioning is critical or your extract will contain a lot of files with no data. Empty partitions cause a lot of unnecessary network traffic and cause Spark to run slowly.</p> <p>Thankfully incremental update technology removes the need to manually specify the number of partitions.</p>"},{"location":"apache-spark/incrementally-updating-extracts/#incrementally-updating-extracts","title":"Incrementally Updating Extracts","text":"<p>Spark Structured Streaming coupled with <code>Trigger.Once</code> can be used to create extracts that incrementally update, as described in this blog post.</p> <p>The Structured Streaming API is similar to the batch API, so we only need to make minor refactorings to our code.</p> <pre><code>val schema = StructType(\n  List(\n    StructField(\"first_name\", StringType, true),\n    StructField(\"age\", IntegerType, true)\n  )\n)\n\nval streamingLakeDF = spark\n  .readStream\n  .format(\"parquet\")\n  .schema(schema)\n  .load(\"s3a://some-bucket/my-data-lake\")\n\ndef filterMinors()(df: DataFrame): DataFrame = {\n  df\n    .filter(col(\"age\") &lt; 18)\n}\n\ndef parquetStreamWriter(dataPath: String, checkpointPath: String)(df: DataFrame): Unit = {\n  df\n    .writeStream\n    .trigger(Trigger.Once)\n    .format(\"parquet\")\n    .option(\"checkpointLocation\", checkpointPath)\n    .start(dataPath)\n}\n\nval etl = new EtlDefinition(\n  sourceDF = streamingLakeDF,\n  transform = filterMinors(),\n  write = parquetStreamWriter(\n    \"s3a://some-bucket/incremental_extracts/data/adults\",\n    \"s3a://some-bucket/incremental_extracts/checkpoints/adults\"\n  )\n)\n\netl.process()\n</code></pre> <p>Key differences with the new code:</p> <ul> <li>We need to explicitly specify the schema of our data lake (the spark-daria <code>printSchemaInCodeFormat</code> method makes this easy)</li> <li>We don't need to specify how the extract will be repartioned, Spark Structured Streaming does this for us automatically</li> <li>We need to specify a checkpoint directory when writing out the data</li> </ul> <p>The initial run of an incremental extract needs to be run on a big cluster (initial runs need clusters that are the same as clusters for batch runs). On subsequent runs, the checkpoint directory will keep track of the data lake files that have already been analyzed. The subsequent runs will analyse files that were added to the data lake since the last run. That's why subsequent runs can be executed on clusters that are much smaller and will take less times to complete.</p>"},{"location":"apache-spark/incrementally-updating-extracts/#full-refresh-of-an-incremental-extract","title":"Full refresh of an incremental extract","text":"<p>Whenever the transformation logic is modified, you'll need to do a full refresh of the incremental extract. For example, if the transformation is changed from an age of 18 to 16, then a full refresh is required.</p> <pre><code>def filterMinors()(df: DataFrame): DataFrame = {\n  df\n    .filter(col(age) &lt; 16)\n}\n</code></pre> <p>You can simply delete the data folder and the checkpoint folder and run the exact same code to do a full refresh.</p> <p>Here are the AWS CLI commands to delete those folders.</p> <pre><code>aws s3 rm s3a://some-bucket/incremental_extracts/data/adults --recursive\naws s3 rm s3a://some-bucket/incremental_extracts/checkpoints/adults --recursive\n</code></pre> <p>Remember that you'll need a bigger cluster for full refreshes than incremental updates.</p>"},{"location":"apache-spark/incrementally-updating-extracts/#incremental-complications","title":"Incremental Complications","text":"<p>It's hard to incrementally update analyses that perform aggregations Spark supports watermarking, but I haven't figured out how to use it yet. I'll publish another blog when I figure it out ;)</p>"},{"location":"apache-spark/incrementally-updating-extracts/#next-steps","title":"Next Steps","text":"<p>Incrementally updating analyses are necessary to keep processing times low and control costs.</p> <p>Data extracts are a great place to add Structured Streaming and <code>Trigger.Once</code> to your ETL stack so you can avoid the complications of watermarking.</p> <p>Incrementally updating extracts will save you a ton of time and money!</p>"},{"location":"apache-spark/introduction-to-dataframes/","title":"Introduction to Spark DataFrames","text":"<p>Spark DataFrames are similar to tables in relational databases - they store data in columns and rows and support a variety of operations to manipulate the data.</p> <p>Here's an example of a DataFrame that contains information about cities.</p> city country population Boston USA 0.67 Dubai UAE 3.1 Cordoba Argentina 1.39 <p>This blog post will discuss creating DataFrames, defining schemas, adding columns, and filtering rows.</p>"},{"location":"apache-spark/introduction-to-dataframes/#creating-dataframes","title":"Creating DataFrames","text":"<p>You can import spark implicits and create a DataFrame with the <code>toDF()</code> method.</p> <pre><code>import spark.implicits._\n\nval df = Seq(\n  (\"Boston\", \"USA\", 0.67),\n  (\"Dubai\", \"UAE\", 3.1),\n  (\"Cordoba\", \"Argentina\", 1.39)\n).toDF(\"city\", \"country\", \"population\")\n</code></pre> <p>You can view the contents of a DataFrame with the <code>show()</code> method.</p> <pre><code>df.show()\n</code></pre> <pre><code>+-------+---------+----------+\n|   city|  country|population|\n+-------+---------+----------+\n| Boston|      USA|      0.67|\n|  Dubai|      UAE|       3.1|\n|Cordoba|Argentina|      1.39|\n+-------+---------+----------+\n</code></pre> <p>Each DataFrame column has <code>name</code>, <code>dataType</code> and <code>nullable</code> properties. The column can contain <code>null</code> values if the <code>nullable</code> property is set to <code>true</code>.</p> <p>The <code>printSchema()</code> method provides an easily readable view of the DataFrame schema.</p> <pre><code>df.printSchema()\n</code></pre> <pre><code>root\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- population: double (nullable = false)\n</code></pre>"},{"location":"apache-spark/introduction-to-dataframes/#adding-columns","title":"Adding columns","text":"<p>Columns can be added to a DataFrame with the <code>withColumn()</code> method.</p> <p>Let's add an <code>is_big_city</code> column to the DataFrame that returns <code>true</code> if the city contains more than one million people.</p> <pre><code>import org.apache.spark.sql.functions.col\n\nval df2 = df.withColumn(\"is_big_city\", col(\"population\") &gt; 1)\ndf2.show()\n</code></pre> <pre><code>+-------+---------+----------+-----------+\n|   city|  country|population|is_big_city|\n+-------+---------+----------+-----------+\n| Boston|      USA|      0.67|      false|\n|  Dubai|      UAE|       3.1|       true|\n|Cordoba|Argentina|      1.39|       true|\n+-------+---------+----------+-----------+\n</code></pre> <p>DataFrames are immutable, so the <code>withColumn()</code> method returns a new DataFrame. <code>withColumn()</code> does not mutate the original DataFrame. Let's confirm that <code>df</code> is still the same with <code>df.show()</code>.</p> <pre><code>+-------+---------+----------+\n|   city|  country|population|\n+-------+---------+----------+\n| Boston|      USA|      0.67|\n|  Dubai|      UAE|       3.1|\n|Cordoba|Argentina|      1.39|\n+-------+---------+----------+\n</code></pre> <p><code>df</code> does not contain the <code>is_big_city</code> column, so we've confirmed that <code>withColumn()</code> did not mutate <code>df</code>.</p>"},{"location":"apache-spark/introduction-to-dataframes/#filtering-rows","title":"Filtering rows","text":"<p>The <code>filter()</code> method removes rows from a DataFrame.</p> <pre><code>df.filter(col(\"population\") &gt; 1).show()\n</code></pre> <pre><code>+-------+---------+----------+\n|   city|  country|population|\n+-------+---------+----------+\n|  Dubai|      UAE|       3.1|\n|Cordoba|Argentina|      1.39|\n+-------+---------+----------+\n</code></pre> <p>It's a little hard to read code with multiple method calls on the same line, so let's break this code up on multiple lines.</p> <pre><code>df\n  .filter(col(\"population\") &gt; 1)\n  .show()\n</code></pre> <p>We can also assign the filtered DataFrame to a separate variable rather than chaining method calls.</p> <pre><code>val filteredDF = df.filter(col(\"population\") &gt; 1)\nfilteredDF.show()\n</code></pre>"},{"location":"apache-spark/introduction-to-dataframes/#more-on-schemas","title":"More on schemas","text":"<p>As previously discussed, the DataFrame schema can be pretty printed to the console with the <code>printSchema()</code> method. The <code>schema</code> method returns a code representation of the DataFrame schema.</p> <pre><code>df.schema\n</code></pre> <pre><code>StructType(\n  StructField(city, StringType, true),\n  StructField(country, StringType, true),\n  StructField(population, DoubleType, false)\n)\n</code></pre> <p>Each column of a Spark DataFrame is modeled as a <code>StructField</code> object with name, columnType, and nullable properties. The entire DataFrame schema is modeled as a <code>StructType</code>, which is a collection of <code>StructField</code> objects.</p> <p>Let's create a schema for a DataFrame that has <code>first_name</code> and <code>age</code> columns.</p> <pre><code>import org.apache.spark.sql.types._\n\nStructType(\n  Seq(\n    StructField(\"first_name\", StringType, true),\n    StructField(\"age\", DoubleType, true)\n  )\n)\n</code></pre> <p>Spark's programming interface makes it easy to define the exact schema you'd like for your DataFrames.</p>"},{"location":"apache-spark/introduction-to-dataframes/#creating-dataframes-with-createdataframe","title":"Creating DataFrames with createDataFrame()","text":"<p>The <code>toDF()</code> method for creating Spark DataFrames is quick, but it's limited because it doesn't let you define your schema (it infers the schema for you). The <code>createDataFrame()</code> method lets you define your DataFrame schema.</p> <pre><code>import org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\n\nval animalData = Seq(\n  Row(30, \"bat\"),\n  Row(2, \"mouse\"),\n  Row(25, \"horse\")\n)\n\nval animalSchema = List(\n  StructField(\"average_lifespan\", IntegerType, true),\n  StructField(\"animal_type\", StringType, true)\n)\n\nval animalDF = spark.createDataFrame(\n  spark.sparkContext.parallelize(animalData),\n  StructType(animalSchema)\n)\n\nanimalDF.show()\n</code></pre> <pre><code>+----------------+-----------+\n|average_lifespan|animal_type|\n+----------------+-----------+\n|              30|        bat|\n|               2|      mouse|\n|              25|      horse|\n+----------------+-----------+\n</code></pre> <p>Read this blog post if you'd like more information on different approaches to create Spark DataFrames.</p> <p>We can use the <code>animalDF.printSchema()</code> method to confirm that the schema was created as specified.</p> <pre><code>root\n |-- average_lifespan: integer (nullable = true)\n |-- animal_type: string (nullable = true)\n</code></pre>"},{"location":"apache-spark/introduction-to-dataframes/#next-steps","title":"Next Steps","text":"<p>DataFrames are the fundamental building blocks of Spark. All machine learning and streaming analyses are built on top of the DataFrame API. Make sure you master DataFrames before diving in to more advanced parts of the Spark API.</p>"},{"location":"apache-spark/introduction-to-sbt/","title":"Introduction to SBT for Spark Programmers","text":"<p>SBT is an interactive build tool that is used to run tests and package your projects as JAR files.</p> <p>SBT lets you create a project in a text editor and package it, so it can be run in a cloud cluster computing environment (like Databricks).</p> <p>SBT has a comprehensive Getting started guide, but let's be honest - who wants to read a book on a build tool?</p> <p>This guide teaches Spark programmers what they need to know about SBT and skips all the other details!</p>"},{"location":"apache-spark/introduction-to-sbt/#sample-code","title":"Sample code","text":"<p>I recommend cloning the spark-daria project on your local machine, so you can run the SBT commands as you read this post.</p>"},{"location":"apache-spark/introduction-to-sbt/#running-sbt-commands","title":"Running SBT commands","text":"<p>SBT commands can be run from the command line or from the SBT shell.</p> <p>For example, here's how to run the test suite from Bash: <code>sbt test</code>.</p> <p>Alternatively, we can open the SBT shell by running <code>sbt</code> in Bash and then simply run <code>test</code>.</p> <p>Run <code>exit</code> to leave the SBT shell.</p>"},{"location":"apache-spark/introduction-to-sbt/#buildsbt","title":"<code>build.sbt</code>","text":"<p>The SBT build definition is specified in the <code>build.sbt</code> file.</p> <p>This is where you'll add code to specify your dependencies, the Scala version, how to build your JAR files, how to manage memory, etc.</p> <p>One of the only things that's not specified in the <code>build.sbt</code> file is the SBT version itself. The SBT version is specified in the <code>project/build.properties</code> file, for example:</p> <pre><code>sbt.version=1.2.8\n</code></pre>"},{"location":"apache-spark/introduction-to-sbt/#librarydependencies","title":"<code>libraryDependencies</code>","text":"<p>You can specify <code>libraryDependencies</code> in your <code>build.sbt</code> file to fetch libraries from Maven or JitPack.</p> <p>Here's how to add Spark SQL and Spark ML to a project:</p> <pre><code>libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.4.0\" % \"provided\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"2.4.0\" % \"provided\"\n</code></pre> <p>SBT provides shortcut sytax if we'd like to clean up our <code>build.sbt</code> file a bit.</p> <pre><code>libraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.0\" % \"provided\",\n  \"org.apache.spark\" %% \"spark-mllib\" % \"2.4.0\" % \"provided\"\n)\n</code></pre> <p><code>\"provided\"</code> dependencies are already included in the environment where we run our code.</p> <p>Here's an example of some test dependencies that are only used when we run our test suite:</p> <pre><code>libraryDependencies += \"com.lihaoyi\" %% \"utest\" % \"0.6.3\" % \"test\"\nlibraryDependencies += \"MrPowers\" % \"spark-fast-tests\" % \"0.17.1-s_2.11\" % \"test\"\n</code></pre> <p>Read this post on Building JAR files for a more detailed discussion on <code>provided</code> and <code>test</code> dependencies.</p>"},{"location":"apache-spark/introduction-to-sbt/#sbt-test","title":"<code>sbt test</code>","text":"<p>You can run your test suite with the <code>sbt test</code> command.</p> <p>You can set environment variables in your test suite by adding this line to your <code>build.sbt</code> file: <code>envVars in Test := Map(\"PROJECT_ENV\" -&gt; \"test\")</code>. Read the blog post on Environment Specific Config in Spark Projects for more details about this design pattern.</p> <p>You can run a single test file when using Scalatest with this command:</p> <pre><code>sbt \"test:testOnly *LoginServiceSpec\"\n</code></pre> <p>This command is easier to run from the SBT shell:</p> <pre><code>&gt; testOnly *LoginServiceSpec\n</code></pre> <p>Here is how to run a single test file when using uTest:</p> <pre><code>&gt; testOnly -- com.github.mrpowers.spark.daria.sql.DataFrameExtTest\n</code></pre> <p>Complicated SBT commands are generally easier to run from the SBT shell, so you don't need to think about proper quoting.</p> <p>Read this Stackoverflow thread if you'd like to run a single test with Scalatest and this blog post if you'd like to run a single test with uTest.</p>"},{"location":"apache-spark/introduction-to-sbt/#sbt-doc","title":"<code>sbt doc</code>","text":"<p>The <code>sbt doc</code> command generates HTML documentation for your project.</p> <p>You can open the documentation on your local machine with <code>open target/scala-2.11/api/index.html</code> after it's been generated.</p> <p>You should diligently mark functions and objects as private if they're not part of the API. <code>sbt doc</code> won't generate documentation for private members.</p> <p>Codebases are always easier to understand when the public API is clearly defined.</p> <p>For more information, read the spark-style-guide Documentation guidelines and the Documenting Spark Code with Scaladoc blog post.</p>"},{"location":"apache-spark/introduction-to-sbt/#sbt-console","title":"<code>sbt console</code>","text":"<p>The <code>sbt console</code> command starts the Scala interpreter with easy access to all your project files.</p> <p>Let's run <code>sbt console</code> in the <code>spark-daria</code> project and then invoke the <code>StringHelpers.snakify()</code> method.</p> <pre><code>scala&gt; com.github.mrpowers.spark.daria.utils.StringHelpers.snakify(\"FunStuff\") // fun_stuff\n</code></pre> <p>Running <code>sbt console</code> is similar to running the Spark shell with the <code>spark-daria</code> JAR file attached. Here's how to start the Spark shell with the <code>spark-daria</code> JAR file attached.</p> <pre><code>./spark-shell --jars ~/Documents/code/my_apps/spark-daria/target/scala-2.11/spark-daria-assembly-0.28.0.jar\n</code></pre> <p>The same code from before also works in the Spark shell:</p> <pre><code>scala&gt; com.github.mrpowers.spark.daria.utils.StringHelpers.snakify(\"FunStuff\") // fun_stuff\n</code></pre> <p>This blog post provides more details on how to use the Spark shell.</p> <p>The <code>sbt console</code> is sometimes useful for playing around with code, but the test suite is usually better. Don't \"test\" your code in the console and neglect writing real tests.</p>"},{"location":"apache-spark/introduction-to-sbt/#sbt-package-sbt-assembly","title":"<code>sbt package</code> / <code>sbt assembly</code>","text":"<p><code>sbt package</code> builds a thin JAR file (only includes the project files). For <code>spark-daria</code>, the <code>sbt package</code> command builds the <code>target/scala-2.11/spark-daria-0.28.0.jar</code> file.</p> <p><code>sbt assembly</code> builds a fat JAR file (includes all the project and dependency files). For <code>spark-daria</code>, the <code>sbt assembly</code> command builds the <code>target/scala-2.11/spark-daria-assembly-0.28.0.jar</code> file.</p> <p>Read this blog post on Building Spark JAR files for a detailed discussion on how <code>sbt package</code> and <code>sbt assembly</code> differ. To further customize JAR files, read this blog post on shading dependencies.</p> <p>You should be comfortable with developing Spark code in a text editor, packaging your project as a JAR file, and attaching your JAR file to a cloud cluster for production analyses.</p>"},{"location":"apache-spark/introduction-to-sbt/#sbt-clean","title":"<code>sbt clean</code>","text":"<p>The <code>sbt clean</code> command deletes all of the generated files in the <code>target/</code> directory.</p> <p>This command will delete the documentation generated by <code>sbt doc</code> and will delete the JAR files generated by <code>sbt package</code> / <code>sbt assembly</code>.</p> <p>It's good to run <code>sbt clean</code> frequently, so you don't accumlate a lot of legacy clutter in the <code>target/</code> directory.</p>"},{"location":"apache-spark/introduction-to-sbt/#next-steps","title":"Next steps","text":"<p>SBT is a great build tool for Spark projects.</p> <p>It lets you easily run tests, generate documentation, and package code as JAR files.</p> <p>In a future post, we'll investigate how Mill can be used as a build tool for Spark projects.</p>"},{"location":"apache-spark/just-enough-scala/","title":"Just Enough Scala for Spark Programmers","text":"<p>Spark programmers only need to know a small subset of the Scala API to be productive.</p> <p>Scala has a reputation for being a difficult language to learn and that scares some developers away from Spark. This guide covers the Scala language features needed for Spark programmers.</p> <p>Spark programmers need to know how to write Scala functions, encapsulate functions in objects, and namespace objects in packages. It's not a lot to learn - I promise!</p>"},{"location":"apache-spark/just-enough-scala/#scala-function-basics","title":"Scala function basics","text":"<p>This section describes how to write vanilla Scala functions and Spark SQL functions.</p> <p>Here is a Scala function that adds two numbers:</p> <pre><code>def sum(num1: Int, num2: Int): Int = {\n  num1 + num2\n}\n</code></pre> <p>We can invoke this function as follows:</p> <pre><code>sum(10, 5) // returns 15\n</code></pre> <p>Let's write a Spark SQL function that adds two numbers together:</p> <pre><code>import org.apache.spark.sql.Column\n\ndef sumColumns(num1: Column, num2: Column): Column = {\n  num1 + num2\n}\n</code></pre> <p>Let's create a DataFrame in the Spark shell and run the <code>sumColumns()</code> function.</p> <pre><code>val numbersDF = Seq(\n  (10, 4),\n  (3, 4),\n  (8, 4)\n).toDF(\"some_num\", \"another_num\")\n\nnumbersDF\n  .withColumn(\n    \"the_sum\",\n    sumColumns(col(\"some_num\"), col(\"another_num\"))\n  )\n  .show()\n</code></pre> <pre><code>+--------+-----------+-------+\n|some_num|another_num|the_sum|\n+--------+-----------+-------+\n|      10|          4|     14|\n|       3|          4|      7|\n|       8|          4|     12|\n+--------+-----------+-------+\n</code></pre> <p>Spark SQL functions take <code>org.apache.spark.sql.Column</code> arguments whereas vanilla Scala functions take native Scala data type arguments like <code>Int</code> or <code>String</code>.</p>"},{"location":"apache-spark/just-enough-scala/#currying-functions","title":"Currying functions","text":"<p>Scala allows for functions to take multiple parameter lists, which is formally known as currying. This section explains how to use currying with vanilla Scala functions and why currying is important for Spark programmers.</p> <pre><code>def myConcat(word1: String)(word2: String): String = {\n  word1 + word2\n}\n</code></pre> <p>Here's how to invoke the <code>myConcat()</code> function.</p> <pre><code>myConcat(\"beautiful \")(\"picture\") // returns \"beautiful picture\"\n</code></pre> <p><code>myConcat()</code> is invoked with two sets of arguments.</p> <p>Spark has a <code>Dataset#transform()</code> method that makes it easy to chain DataFrame transformations.</p> <p>Here's an example of a DataFrame transformation function:</p> <pre><code>import org.apache.spark.sql.DataFrame\n\ndef withCat(name: String)(df: DataFrame): DataFrame = {\n  df.withColumn(\"cat\", lit(s\"$name meow\"))\n}\n</code></pre> <p>DataFrame transformation functions can take an arbitrary number of arguments in the first parameter list and must take a single DataFrame argument in the second parameter list.</p> <p>Let's create a DataFrame in the Spark shell and run the <code>withCat()</code> function.</p> <pre><code>val stuffDF = Seq(\n  (\"chair\"),\n  (\"hair\"),\n  (\"bear\")\n).toDF(\"thing\")\n\nstuffDF\n  .transform(withCat(\"darla\"))\n  .show()\n</code></pre> <pre><code>+-----+----------+\n|thing|       cat|\n+-----+----------+\n|chair|darla meow|\n| hair|darla meow|\n| bear|darla meow|\n+-----+----------+\n</code></pre> <p>Most Spark code can be organized as Spark SQL functions or as custom DataFrame transformations.</p>"},{"location":"apache-spark/just-enough-scala/#object","title":"<code>object</code>","text":"<p>Spark functions can be stored in objects.</p> <p>Let's create a <code>SomethingWeird</code> object that defines a vanilla Scala function, a Spark SQL function, and a custom DataFrame transformation.</p> <pre><code>import org.apache.spark.sql.functions._\n\nobject SomethingWeird {\n\n  // vanilla Scala function\n  def hi(): String = {\n    \"welcome to planet earth\"\n  }\n\n  // Spark SQL function\n  def trimUpper(col: Column) = {\n    trim(upper(col))\n  }\n\n  // custom DataFrame transformation\n  def withScary()(df: DataFrame): DataFrame = {\n    df.withColumn(\"scary\", lit(\"boo!\"))\n  }\n\n}\n</code></pre> <p>Let's create a DataFrame in the Spark shell and run the <code>trimUpper()</code> and <code>withScary()</code> functions.</p> <pre><code>val wordsDF = Seq(\n  (\"niCE\"),\n  (\"  CaR\"),\n  (\"BAR  \")\n).toDF(\"word\")\n\nwordsDF\n  .withColumn(\"trim_upper_word\", SomethingWeird.trimUpper(col(\"word\")))\n  .transform(SomethingWeird.withScary())\n  .show()\n</code></pre> <pre><code>+-----+---------------+-----+\n| word|trim_upper_word|scary|\n+-----+---------------+-----+\n| niCE|           NICE| boo!|\n|  CaR|            CAR| boo!|\n|BAR  |            BAR| boo!|\n+-----+---------------+-----+\n</code></pre> <p>Objects are useful for grouping related Spark functions.</p>"},{"location":"apache-spark/just-enough-scala/#trait","title":"<code>trait</code>","text":"<p>Traits can be mixed into objects to add commonly used methods or values. We can define a <code>SparkSessionWrapper</code> <code>trait</code> that defines a <code>spark</code> variable to give objects easy access to the <code>SparkSession</code> object.</p> <pre><code>import org.apache.spark.sql.SparkSession\n\ntrait SparkSessionWrapper extends Serializable {\n\n  lazy val spark: SparkSession = {\n    SparkSession.builder().master(\"local\").appName(\"spark session\").getOrCreate()\n  }\n\n}\n</code></pre> <p>The <code>Serializable</code> trait is mixed into the <code>SparkSessionWrapper</code> trait.</p> <p>Let's create a <code>SpecialDataLake</code> object that mixes in the <code>SparkSessionWrapper</code> trait to provide easy access to a data lake.</p> <pre><code>object SpecialDataLake extends SparkSessionWrapper {\n\n  def dataLake(): DataFrame = {\n    spark.read.parquet(\"some_secret_s3_path\")\n  }\n\n}\n</code></pre>"},{"location":"apache-spark/just-enough-scala/#package","title":"<code>package</code>","text":"<p>Packages are used to namespace Scala code. Per the Databricks Scala style guide, packages should follow Java naming conventions.</p> <p>For example, the Databricks spark-redshift project uses the <code>com.databricks.spark.redshift</code> namespace.</p> <p>The Spark project used the <code>org.apache.spark</code> namespace. spark-daria uses the <code>com.github.mrpowers.spark.daria</code> namespace.</p> <p>Here an example of code that's defined in a package in <code>spark-daria</code>:</p> <pre><code>package com.github.mrpowers.spark.daria.sql\n\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions._\n\nobject functions {\n\n  def singleSpace(col: Column): Column = {\n    trim(regexp_replace(col, \" +\", \" \"))\n  }\n\n}\n</code></pre> <p>The package structure should mimic the file structure of the project.</p>"},{"location":"apache-spark/just-enough-scala/#implicit-classes","title":"Implicit classes","text":"<p>Implicit classes can be used to extend Spark core classes with additional methods.</p> <p>Let's add a <code>lower()</code> method to the <code>Column</code> class that converts all the strings in a column to lower case.</p> <pre><code>package com.github.mrpowers.spark.daria.sql\n\nimport org.apache.spark.sql.Column\n\nobject FunctionsAsColumnExt {\n\n  implicit class ColumnMethods(col: Column) {\n\n    def lower(): Column = {\n      org.apache.spark.sql.functions.lower(col)\n    }\n\n  }\n\n}\n</code></pre> <p>After running <code>import com.github.mrpowers.spark.daria.sql.FunctionsAsColumnExt._</code>, you can run the <code>lower()</code> method directly on column objects.</p> <pre><code>col(\"some_string\").lower()\n</code></pre> <p>Implicit classes should be avoided in general. I only monkey patch core classes in the spark-daria project. Feel free to send pull requests if you have any good ideas for other extensions.</p>"},{"location":"apache-spark/just-enough-scala/#next-steps","title":"Next steps","text":"<p>There are a couple of other Scala features that are useful when writing Spark code, but this blog post covers 90%+ of common use cases.</p> <p>You don't need to understand functional programming or advanced Scala language features to be a productive Spark programmer.</p> <p>In fact, staying away from UDFs and native Scala code is a best practice.</p> <p>Focus on mastering the native Spark API and you'll be a productive big data engineer in no time!</p>"},{"location":"apache-spark/limiting-function-order-dependencies/","title":"Limiting Order Dependencies in Spark Functions","text":"<p>Spark codebases can easily become a collection of order dependent custom transformations (see this blog post for background on custom transformations). Your library will be difficult to use if many functions need to be run in a specific order. This post shows how to run functions to add intermediate columns when necessary and limit order dependencies. This design pattern will provider your users a better experience.</p>"},{"location":"apache-spark/limiting-function-order-dependencies/#example-of-order-dependent-transformations","title":"Example of order dependent transformations","text":"<p>Suppose we have a DataFrame with a <code>city</code> column. We have one transformation to add a <code>country</code> column to the DataFrame and another transformation to add a <code>hemisphere</code> column to the DataFrame (to indicate if the country is in the Northern Hemisphere or Southern Hemisphere).</p> <pre><code>def withCountry()(df: DataFrame): DataFrame = {\n  df.withColumn(\n    \"country\",\n    when(col(\"city\") === \"Calgary\", \"Canada\")\n      .when(col(\"city\") === \"Buenos Aires\", \"Argentina\")\n      .when(col(\"city\") === \"Cape Town\", \"South Africa\")\n  )\n}\n\ndef withHemisphere()(df: DataFrame): DataFrame = {\n  df.withColumn(\n    \"hemisphere\",\n    when(col(\"country\") === \"Canada\", \"Northern Hemisphere\")\n      .when(col(\"country\") === \"Argentina\", \"Southern Hemisphere\")\n      .when(col(\"country\") === \"South Africa\", \"Southern Hemisphere\")\n  )\n}\n</code></pre> <p>Let's create a DataFrame with cities and run our transformations.</p> <pre><code>val df = spark.createDF(\n  List(\n    (\"Calgary\"),\n    (\"Buenos Aires\"),\n    (\"Cape Town\")\n  ), List(\n    (\"city\", StringType, true)\n  )\n)\n\ndf\n  .transform(withCountry())\n  .transform(withHemisphere())\n  .show()\n</code></pre> <p>This DataFrame is printed to the console:</p> <pre><code>+------------+------------+-------------------+\n|        city|     country|         hemisphere|\n+------------+------------+-------------------+\n|     Calgary|      Canada|Northern Hemisphere|\n|Buenos Aires|   Argentina|Southern Hemisphere|\n|   Cape Town|South Africa|Southern Hemisphere|\n+------------+------------+-------------------+\n</code></pre> <p>The <code>withCountry()</code> and <code>withHemisphere()</code> transformations are order dependent because <code>withCountry()</code> must be run before <code>withHemisphere()</code>. If <code>withHemisphere()</code> is run first, the code will error out.</p> <pre><code>df\n  .transform(withHemisphere())\n  .transform(withCountry())\n  .show()\n</code></pre> <p>Here is the error message:</p> <pre><code>org.apache.spark.sql.AnalysisException: cannot resolve 'country' given input columns: [city];;\n\n'Project \\[city#1, CASE WHEN ('country = Canada) THEN Northern Hemisphere WHEN ('country = Argentina) THEN Southern Hemisphere WHEN ('country = South Africa) THEN Southern Hemisphere END AS hemisphere#4\\]\n</code></pre>"},{"location":"apache-spark/limiting-function-order-dependencies/#intelligently-adding-dependencies-based-on-dataframe-columns","title":"Intelligently adding dependencies based on DataFrame columns","text":"<p>Let's write a <code>withHemisphereRefactored()</code> method that intelligently runs the <code>withCountry()</code> method if the underlying DataFrame does not contain a <code>country</code> column.</p> <pre><code>def withHemisphereRefactored()(df: DataFrame): DataFrame = {\n  if (df.schema.fieldNames.contains(\"country\")) {\n    df.withColumn(\n      \"hemisphere\",\n      when(col(\"country\") === \"Canada\", \"Northern Hemisphere\")\n        .when(col(\"country\") === \"Argentina\", \"Southern Hemisphere\")\n        .when(col(\"country\") === \"South Africa\", \"Southern Hemisphere\")\n    )\n  } else {\n    df\n      .transform(withCountry())\n      .withColumn(\n      \"hemisphere\",\n      when(col(\"country\") === \"Canada\", \"Northern Hemisphere\")\n        .when(col(\"country\") === \"Argentina\", \"Southern Hemisphere\")\n        .when(col(\"country\") === \"South Africa\", \"Southern Hemisphere\")\n    )\n  }\n}\n</code></pre> <p>We can run <code>withHemisphereRefactored()</code> directly on a DataFrame that only contains a <code>city</code> column and it will now work:</p> <pre><code>val df = spark.createDF(\n  List(\n    (\"Calgary\"),\n    (\"Buenos Aires\"),\n    (\"Cape Town\")\n  ), List(\n    (\"city\", StringType, true)\n  )\n)\n\ndf\n  .transform(withHemisphereRefactored())\n  .show()\n</code></pre> <p>We can use the <code>explain()</code> method to inspect the physical plan and see how this code intelligently adds the <code>country</code> column:</p> <pre><code>Project [city#19, CASE\n  WHEN (city#19 = Calgary) THEN Canada\n  WHEN (city#19 = Buenos Aires) THEN Argentina\n  WHEN (city#19 = Cape Town) THEN South Africa\nEND AS country#22,\nCASE\n  WHEN (CASE WHEN (city#19 = Calgary) THEN Canada WHEN (city#19 = Buenos Aires) THEN Argentina WHEN (city#19 = Cape Town) THEN South Africa END = Canada) THEN Northern Hemisphere\n  WHEN (CASE WHEN (city#19 = Calgary) THEN Canada WHEN (city#19 = Buenos Aires) THEN Argentina WHEN (city#19 = Cape Town) THEN South Africa END = Argentina) THEN Southern Hemisphere\n  WHEN (CASE WHEN (city#19 = Calgary) THEN Canada WHEN (city#19 = Buenos Aires) THEN Argentina WHEN (city#19 = Cape Town) THEN South Africa END = South Africa) THEN Southern Hemisphere\nEND AS hemisphere#26]\n</code></pre> <p><code>withHemisphereRefactored()</code> does not run the <code>withCountry()</code> code if the DataFrame already contains a <code>country</code> column. We don't want to write code that unnecessarily executes functions because that would slow down our codebase. Let's verify this by running <code>withHemisphereRefactored()</code> on a DataFrame with a <code>country</code> column and inspecting the physical plan.</p> <pre><code>val df = spark.createDF(\n  List(\n    (\"Canada\"),\n    (\"Argentina\"),\n    (\"South Africa\")\n  ), List(\n    (\"country\", StringType, true)\n  )\n)\n\ndf\n  .transform(withHemisphereRefactored())\n  .show()\n</code></pre> <pre><code>+------------+-------------------+\n|     country|         hemisphere|\n+------------+-------------------+\n|      Canada|Northern Hemisphere|\n|   Argentina|Southern Hemisphere|\n|South Africa|Southern Hemisphere|\n+------------+-------------------+\n</code></pre> <p>Let's inspect the physical plan to confirm that <code>withHemisphereRefactored()</code> is not unnecessarily running the <code>withCountry()</code> related logic when the DataFrame already contains a <code>country</code> column.</p> <pre><code>df\n  .transform(withHemisphereRefactored())\n  .explain()\n</code></pre> <pre><code>Project [country#37, CASE\n  WHEN (country#37 = Canada) THEN Northern Hemisphere\n  WHEN (country#37 = Argentina) THEN Southern Hemisphere\n  WHEN (country#37 = South Africa) THEN Southern Hemisphere\nEND AS hemisphere#48]\n</code></pre> <p>Let's leverage the spark-daria <code>containsColumn()</code> DataFrame extension and abstract the transformation logic to a subfunction to express this code more elegantly:</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DataFrameExt._\n\ndef withHemisphereElegant()(df: DataFrame): DataFrame = {\n  def hemTransformation()(df: DataFrame): DataFrame = {\n    df.withColumn(\n      \"hemisphere\",\n      when(col(\"country\") === \"Canada\", \"Northern Hemisphere\")\n        .when(col(\"country\") === \"Argentina\", \"Southern Hemisphere\")\n        .when(col(\"country\") === \"South Africa\", \"Southern Hemisphere\")\n    )\n  }\n\n  if (df.containsColumn(\"country\")) {\n    df.transform(hemTransformation())\n  } else {\n    df\n      .transform(withCountry())\n      .transform(hemTransformation())\n  }\n}\n</code></pre> <p>You can also pass the <code>containsColumn()</code> method a <code>StructField</code> argument if you'd like to validate the column name, type, and nullable property.</p> <pre><code>df.containsColumn(StructField(\"country\", StringType, true))\n</code></pre>"},{"location":"apache-spark/limiting-function-order-dependencies/#taking-it-to-the-next-level","title":"Taking it to the next level","text":"<p>We can leverage the spark-daria <code>CustomTransform</code> case class to encapsulate the DataFrame columns that are required, added, and removed by each DataFrame transformation function.</p> <p>Here's the <code>CustomTransform</code> case class definition:</p> <pre><code>case class CustomTransform(\n  transform: (DataFrame =&gt; DataFrame),\n  requiredColumns: Seq[String] = Seq.empty[String],\n  addedColumns: Seq[String] = Seq.empty[String],\n  removedColumns: Seq[String] = Seq.empty[String]\n)\n</code></pre> <p>Let's define <code>countryCT</code> and <code>hemisphereCT</code> objects:</p> <pre><code>val countryCT = new CustomTransform(\n  transform = withCountry(),\n  requiredColumns = Seq(\"city\"),\n  addedColumns = Seq(\"country\")\n)\n\nval hemisphereCT = new CustomTransform(\n  transform = withHemisphere(),\n  requiredColumns = Seq(\"country\"),\n  addedColumns = Seq(\"hemisphere\")\n)\n</code></pre> <p>We can immediately identify how <code>countryCT</code> and <code>hemisphereCT</code> are related - the column that's added by <code>countryCT</code> is the same as the column that's required by <code>hemisphereCT</code>.</p> <p>Custom transformation objects can be executed by a <code>trans()</code> method that's also defined in spark-daria.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DataFrameExt._\n\nval df = spark.createDF(\n  List(\n    (\"Calgary\"),\n    (\"Buenos Aires\"),\n    (\"Cape Town\")\n  ), List(\n    (\"city\", StringType, true)\n  )\n)\n\ndf\n  .trans(countryCT)\n  .trans(hemisphereCT)\n  .show()\n</code></pre> <pre><code>+------------+------------+-------------------+\n|        city|     country|         hemisphere|\n+------------+------------+-------------------+\n|     Calgary|      Canada|Northern Hemisphere|\n|Buenos Aires|   Argentina|Southern Hemisphere|\n|   Cape Town|South Africa|Southern Hemisphere|\n+------------+------------+-------------------+\n</code></pre> <p>The spark-daria <code>composeTrans()</code> method runs a list of CustomTransforms intelligently - the DataFrame transformation is only run if the columns that will be added by the transformation are missing from the DataFrame.</p> <pre><code>df.composeTrans(List(countryCT, hemisphereCT))\n</code></pre>"},{"location":"apache-spark/limiting-function-order-dependencies/#a-path-forward","title":"A path forward","text":"<p>A Spark library can be modeled as a directed acyclic graph (DAG) of <code>CustomTransform</code> objects.</p> <p>This would allow users to specify the columns they want added to a DataFrame without worrying about running a bunch of functions in a specific order. The library would be responsible for intelligently running the required transformations to provide the user their desired DataFrame.</p>"},{"location":"apache-spark/limiting-function-order-dependencies/#next-steps","title":"Next steps","text":"<p>Spark libraries can quickly grow to 30+ order dependent functions that are difficult to use. When order dependencies are rampant in a codebase, users are forced to dig through the source code and often revert to a frustrating trial and error development workflow.</p> <p>Modeling transformations as <code>CustomTransform</code> objects forces you to document the columns that are added and removed by each transformation and will create a codebase that's easier to parse.</p> <p>Writing transformations that intelligently call other transformations if required columns are missing is another way to make it easier for users to leverage your public interface.</p> <p>Modeling a library of <code>CustomTransform</code> objects as a DAG is the holy grail for a library public interface. The library will be responsible for running functions in a certain order and making sure the Spark physical plan that's generated is optimal. This will provide users a better library experience.</p>"},{"location":"apache-spark/logistic-regressions/","title":"Running Logistic Regressions with Spark","text":"<p>Logistic regression models are a powerful way to predict binary outcomes (e.g. winning a game or surviving a shipwreck).</p> <p>Multiple explanatory variables (aka \"features\") are used to train the model that predicts the outcome.</p> <p>This episode shows how to train a Spark logistic regression model with the Titanic dataset and use the model to predict if a passenger survived or died.</p> <p>We'll run our model on a test dataset and demonstrate that the model predicts the passenger survivorship accurately 83% of the time.</p>"},{"location":"apache-spark/logistic-regressions/#titanic-dataset","title":"Titanic Dataset","text":"<p>The <code>train.csv</code> file contains 891 rows of data in this schema:</p> <pre><code>PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S\n5,0,3,\"Allen, Mr. William Henry\",male,35,0,0,373450,8.05,,S\n6,0,3,\"Moran, Mr. James\",male,,0,0,330877,8.4583,,Q\n7,0,1,\"McCarthy, Mr. Timothy J\",male,54,0,0,17463,51.8625,E46,S\n8,0,3,\"Palsson, Master. Gosta Leonard\",male,2,3,1,349909,21.075,,S\n</code></pre> <ul> <li><code>PassengerId</code> is a unique identifier for each passenger</li> <li><code>Survived</code> is <code>0</code> for passengers that died and <code>1</code> for passengers that survived</li> <li><code>Pclass</code> is the ticket class from 1 - 3 (1 is the highest class, 2 is the middle class, and 3 is the lowest class)</li> <li><code>Name</code> is the passenger's full name</li> <li><code>Sex</code> is male or female</li> <li><code>Age</code> is the passenger's age in years</li> <li><code>SibSp</code> is the number of siblings / spouses on board</li> <li><code>Parch</code> is the number of parents / children on board</li> <li><code>Ticket</code> is the ticket number</li> <li><code>Fare</code> is the passenger fare</li> <li><code>Cabin</code> is the cabin number</li> <li><code>Embarked</code> is the port where the passenger got on the ship</li> </ul>"},{"location":"apache-spark/logistic-regressions/#selecting-features","title":"Selecting Features","text":"<p>We need to select the explanatory variables that will be able to predict if passengers survived or died. It's always good to have a \"plain English\" reason why your explanatory variables are capable of predicting the outcome.</p> <ul> <li><code>Gender</code> because females were more likely to be put on lifeboats</li> <li><code>Age</code> because children were more likely to be saved</li> <li><code>SibSp</code> because families we more likely to be saved together</li> <li><code>Parch</code> because parent / children combinations were more likely to be saved</li> <li><code>Fare</code> because the richer passengers were more likely to be saved</li> </ul> <p>Explanatory variables are referred to as \"features\" in machine learning.</p>"},{"location":"apache-spark/logistic-regressions/#prepping-the-training-dataset","title":"Prepping the Training Dataset","text":"<p>We will create a <code>trainingDF()</code> method that returns a DataFrame with all of the features converted to floating point numbers, so they can be plugged into the machine learning model.</p> <pre><code>object TitanicData extends SparkSessionWrapper {\n\n  def trainingDF(\n    titanicDataDirName: String = \"./src/test/resources/titanic/\"\n  ): DataFrame = {\n    spark\n      .read\n      .option(\"header\", \"true\")\n      .csv(titanicDataDirName + \"train.csv\")\n      .withColumn(\n        \"Gender\",\n        when(\n          col(\"Sex\").equalTo(\"male\"), 0\n        )\n          .when(col(\"Sex\").equalTo(\"female\"), 1)\n          .otherwise(null)\n      )\n      .select(\n        col(\"Gender\").cast(\"double\"),\n        col(\"Survived\").cast(\"double\"),\n        col(\"Pclass\").cast(\"double\"),\n        col(\"Age\").cast(\"double\"),\n        col(\"SibSp\").cast(\"double\"),\n        col(\"Parch\").cast(\"double\"),\n        col(\"Fare\").cast(\"double\")\n      )\n      .filter(\n        col(\"Gender\").isNotNull &amp;&amp;\n          col(\"Survived\").isNotNull &amp;&amp;\n          col(\"Pclass\").isNotNull &amp;&amp;\n          col(\"Age\").isNotNull &amp;&amp;\n          col(\"SibSp\").isNotNull &amp;&amp;\n          col(\"Parch\").isNotNull &amp;&amp;\n          col(\"Fare\").isNotNull\n      )\n  }\n\n}\n</code></pre>"},{"location":"apache-spark/logistic-regressions/#train-the-model","title":"Train the Model","text":"<p>Let's write a function that will convert all of the features into a single vector.</p> <pre><code>def withVectorizedFeatures(\n  featureColNames: Array[String] = Array(\"Gender\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"),\n  outputColName: String = \"features\"\n)(df: DataFrame): DataFrame = {\n  val assembler: VectorAssembler = new VectorAssembler()\n    .setInputCols(featureColNames)\n    .setOutputCol(outputColName)\n  assembler.transform(df)\n}\n</code></pre> <p>Now let's write a function that will convert the <code>Survived</code> column into a label.</p> <pre><code>def withLabel(\n  inputColName: String = \"Survived\",\n  outputColName: String = \"label\"\n)(df: DataFrame) = {\n  val labelIndexer: StringIndexer = new StringIndexer()\n    .setInputCol(inputColName)\n    .setOutputCol(outputColName)\n\n  labelIndexer\n    .fit(df)\n    .transform(df)\n}\n</code></pre> <p>We can train the model by vectorizing the features, adding a label, and fitting a logistic regression model with a DataFrame that has <code>feature</code> and <code>label</code> columns.</p> <pre><code>def model(df: DataFrame = TitanicData.trainingDF()): LogisticRegressionModel = {\n  val trainFeatures: DataFrame = df\n    .transform(withVectorizedFeatures())\n    .transform(withLabel())\n    .select(\"features\", \"label\")\n\n  new LogisticRegression()\n    .fit(trainFeatures)\n}\n</code></pre> <p>The model has coefficients and a y-intercept.</p> <pre><code>println(model().coefficients) // [2.5334201606150444,-0.021514292982670942,-0.40830426011779103,-0.23251735366607038,0.017246642519055992]\n\nprintln(model().intercept) // -0.9761016366658759\n</code></pre>"},{"location":"apache-spark/logistic-regressions/#evaluating-model-accuracy","title":"Evaluating Model Accuracy","text":"<p>Let's create a method that does all the data munging and return a properly formatted test dataset so we can run our logistic regresssion model.</p> <pre><code>object TitanicData extends SparkSessionWrapper {\n\n  def testDF(\n    titanicDataDirName: String = \"./src/test/resources/titanic/\"\n  ): DataFrame = {\n    val rawTestDF = spark\n      .read\n      .option(\"header\", \"true\")\n      .csv(titanicDataDirName + \"test.csv\")\n\n    val genderSubmissionDF = spark\n      .read\n      .option(\"header\", \"true\")\n      .csv(titanicDataDirName + \"gender_submission.csv\")\n\n    rawTestDF\n      .join(\n        genderSubmissionDF,\n        Seq(\"PassengerId\")\n      )\n      .withColumn(\n        \"Gender\",\n        when(col(\"Sex\").equalTo(\"male\"), 0)\n          .when(col(\"Sex\").equalTo(\"female\"), 1)\n          .otherwise(null)\n      )\n      .select(\n        col(\"Gender\").cast(\"double\"),\n        col(\"Survived\").cast(\"double\"),\n        col(\"Pclass\").cast(\"double\"),\n        col(\"Age\").cast(\"double\"),\n        col(\"SibSp\").cast(\"double\"),\n        col(\"Parch\").cast(\"double\"),\n        col(\"Fare\").cast(\"double\")\n      )\n      .filter(\n        col(\"Gender\").isNotNull &amp;&amp;\n          col(\"Pclass\").isNotNull &amp;&amp;\n          col(\"Age\").isNotNull &amp;&amp;\n          col(\"SibSp\").isNotNull &amp;&amp;\n          col(\"Parch\").isNotNull &amp;&amp;\n          col(\"Fare\").isNotNull\n      )\n\n  }\n\n}\n</code></pre> <p>Just like we did with the training dataset, let's add one column with the vectorized features and another column with the label.</p> <pre><code>val testDF: DataFrame = TitanicData\n  .testDF()\n  .transform(withVectorizedFeatures())\n  .transform(withLabel())\n  .select(\"features\", \"label\")\n</code></pre> <p>Now we're ready to run the logistic regression model.</p> <pre><code>val predictions: DataFrame = TitanicLogisticRegression\n  .model()\n  .transform(testDF)\n  .select(\n    col(\"label\"),\n    col(\"rawPrediction\"),\n    col(\"prediction\")\n  )\n</code></pre> <p>We can use the <code>BinaryClassificationEvaluator</code> class to test the accuracy of our model.</p> <pre><code>new BinaryClassificationEvaluator().evaluate(predictions) // 0.83\n</code></pre>"},{"location":"apache-spark/logistic-regressions/#persisting-the-model","title":"Persisting the model","text":"<p>We can write the logistic regression model to disc with this command.</p> <pre><code>model().save(\"./tmp/titanic_model/\")\n</code></pre> <p>The predictions can be generated with the model that's been persisted in the filesystem.</p> <pre><code>val predictions: DataFrame = LogisticRegressionModel\n  .load(\"./tmp/titanic_model/\")\n  .transform(testDF)\n  .select(\n    col(\"label\"),\n    col(\"rawPrediction\"),\n    col(\"prediction\")\n  )\n</code></pre> <p>Training a model can be expensive and you'll want to persist your model rather than generating it on the fly every time it's needed.</p>"},{"location":"apache-spark/logistic-regressions/#next-steps","title":"Next steps","text":"<p>Spark makes it easy to run logistic regression analyses at scale.</p> <p>From a code organization standpoint, it's easier to separate the data munging and machine learning code in separate objects.</p> <p>You'll need to understand how Spark executes programs to performance tune your models. Training a logistic regression model once and persisting the results will obviously be a lot faster than retraining the model every time it's run.</p>"},{"location":"apache-spark/maptype-columns/","title":"Working with Spark MapType Columns","text":"<p>Spark DataFrame columns support maps, which are great for key / value pairs with an arbitrary length.</p> <p>This blog post describes how to create MapType columns, demonstrates built-in functions to manipulate MapType columns, and explain when to use maps in your analyses.</p> <p>Make sure to read\u00a0Writing Beautiful Spark Code\u00a0for a detailed overview of how to use MapType columns in production applications.</p>"},{"location":"apache-spark/maptype-columns/#scala-maps","title":"Scala maps","text":"<p>Let's begin with a little refresher on Scala maps.</p> <p>Create a Scala map that connects some English and Spanish words.</p> <pre><code>val wordMapping = Map(\"one\" -&gt; \"uno\", \"dog\" -&gt; \"perro\")\n</code></pre> <p>Fetch the value associated with the <code>dog</code> key:</p> <pre><code>wordMapping(\"dog\") // \"perro\"\n</code></pre>"},{"location":"apache-spark/maptype-columns/#creating-maptype-columns","title":"Creating MapType columns","text":"<p>Let's create a DataFrame with a MapType column.</p> <pre><code>val singersDF = spark.createDF(\n  List(\n    (\"sublime\", Map(\n      \"good_song\" -&gt; \"santeria\",\n      \"bad_song\" -&gt; \"doesn't exist\")\n    ),\n    (\"prince_royce\", Map(\n      \"good_song\" -&gt; \"darte un beso\",\n      \"bad_song\" -&gt; \"back it up\")\n    )\n  ), List(\n    (\"name\", StringType, true),\n    (\"songs\", MapType(StringType, StringType, true), true)\n  )\n)\n</code></pre> <pre><code>singersDF.show(false)\n\n+------------+----------------------------------------------------+\n|name        |songs                                               |\n+------------+----------------------------------------------------+\n|sublime     |[good_song -&gt; santeria, bad_song -&gt; doesn't exist]  |\n|prince_royce|[good_song -&gt; darte un beso, bad_song -&gt; back it up]|\n+------------+----------------------------------------------------+\n</code></pre> <p>Let's examine the DataFrame schema and verify that the <code>songs</code> column has a <code>MapType</code>:</p> <pre><code>singersDF.printSchema()\n\nroot\n |-- name: string (nullable = true)\n |-- songs: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n</code></pre> <p>We can see that <code>songs</code> is a MapType column.</p> <p>Let's explore some built-in Spark methods that make it easy to work with MapType columns.</p>"},{"location":"apache-spark/maptype-columns/#fetching-values-from-maps-with-element_at","title":"Fetching values from maps with element_at()","text":"<p>Let's use the <code>singersDF</code> DataFrame and append <code>song_to_love</code> as a column.</p> <pre><code>singersDF\n  .withColumn(\"song_to_love\", element_at(col(\"songs\"), \"good_song\"))\n  .show(false)\n</code></pre> <pre><code>+------------+----------------------------------------------------+-------------+\n|name        |songs                                               |song_to_love |\n+------------+----------------------------------------------------+-------------+\n|sublime     |[good_song -&gt; santeria, bad_song -&gt; doesn't exist]  |santeria     |\n|prince_royce|[good_song -&gt; darte un beso, bad_song -&gt; back it up]|darte un beso|\n+------------+----------------------------------------------------+-------------+\n</code></pre> <p>The <code>element_at()</code> function fetches a value from a MapType column.</p>"},{"location":"apache-spark/maptype-columns/#appending-maptype-columns","title":"Appending MapType columns","text":"<p>We can use the <code>map()</code> method defined in <code>org.apache.spark.sql.functions</code> to append a <code>MapType</code> column to a DataFrame.</p> <pre><code>val countriesDF = spark.createDF(\n  List(\n    (\"costa_rica\", \"sloth\"),\n    (\"nepal\", \"red_panda\")\n  ), List(\n    (\"country_name\", StringType, true),\n    (\"cute_animal\", StringType, true)\n  )\n).withColumn(\n  \"some_map\",\n  map(col(\"country_name\"), col(\"cute_animal\"))\n)\n</code></pre> <pre><code>countriesDF.show(false)\n\n+------------+-----------+---------------------+\n|country_name|cute_animal|some_map             |\n+------------+-----------+---------------------+\n|costa_rica  |sloth      |[costa_rica -&gt; sloth]|\n|nepal       |red_panda  |[nepal -&gt; red_panda] |\n+------------+-----------+---------------------+\n</code></pre> <p>Let's verify that <code>some_map</code> is a <code>MapType</code> column:</p> <pre><code>countriesDF.printSchema()\n\nroot\n |-- country_name: string (nullable = true)\n |-- cute_animal: string (nullable = true)\n |-- some_map: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n</code></pre>"},{"location":"apache-spark/maptype-columns/#creating-maptype-columns-from-two-arraytype-columns","title":"Creating MapType columns from two ArrayType columns","text":"<p>We can create a <code>MapType</code> column from two <code>ArrayType</code> columns.</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(\"a\", \"b\"), Array(1, 2)),\n    (Array(\"x\", \"y\"), Array(33, 44))\n  ), List(\n    (\"letters\", ArrayType(StringType, true), true),\n    (\"numbers\", ArrayType(IntegerType, true), true)\n  )\n).withColumn(\n  \"strange_map\",\n  map_from_arrays(col(\"letters\"), col(\"numbers\"))\n)\n</code></pre> <pre><code>df.show(false)\n\n+-------+--------+------------------+\n|letters|numbers |strange_map       |\n+-------+--------+------------------+\n|[a, b] |[1, 2]  |[a -&gt; 1, b -&gt; 2]  |\n|[x, y] |[33, 44]|[x -&gt; 33, y -&gt; 44]|\n+-------+--------+------------------+\n</code></pre> <p>Let's take a look at the <code>df</code> schema and verify <code>strange_map</code> is a <code>MapType</code> column:</p> <pre><code>df.printSchema()\n\n |-- letters: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- numbers: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n |-- strange_map: map (nullable = true)\n |    |-- key: string\n |    |-- value: integer (valueContainsNull = true)\n</code></pre> <p>The Spark way of converting to arrays to a map is different that the \"regular Scala\" way of converting two arrays to a map.</p>"},{"location":"apache-spark/maptype-columns/#converting-arrays-to-maps-with-scala","title":"Converting Arrays to Maps with Scala","text":"<p>Here's how you'd convert two collections to a map with Scala.</p> <pre><code>val list1 = List(\"a\", \"b\")\nval list2 = List(1, 2)\n\nlist1.zip(list2).toMap // Map(a -&gt; 1, b -&gt; 2)\n</code></pre> <p>We could wrap this code in a User Defined Function and define our own <code>map_from_arrays</code> function if we wanted.</p> <p>In general, it's best to rely on the standard Spark library instead of defining our own UDFs.</p> <p>The key takeaway is that the Spark way of solving a problem is often different from the Scala way. Read the API docs and always try to solve your problems the Spark way.</p>"},{"location":"apache-spark/maptype-columns/#merging-maps-with-map_concat","title":"Merging maps with map_concat()","text":"<p><code>map_concat()</code> can be used to combine multiple MapType columns to a single MapType column.</p> <pre><code>val df = spark.createDF(\n  List(\n    (Map(\"a\" -&gt; \"aaa\", \"b\" -&gt; \"bbb\"), Map(\"c\" -&gt; \"ccc\", \"d\" -&gt; \"ddd\"))\n  ), List(\n    (\"some_data\", MapType(StringType, StringType, true), true),\n    (\"more_data\", MapType(StringType, StringType, true), true)\n  )\n)\n\ndf\n  .withColumn(\"all_data\", map_concat(col(\"some_data\"), col(\"more_data\")))\n  .show(false)\n</code></pre> <pre><code>+--------------------+--------------------+----------------------------------------+\n|some_data           |more_data           |all_data                                |\n+--------------------+--------------------+----------------------------------------+\n|[a -&gt; aaa, b -&gt; bbb]|[c -&gt; ccc, d -&gt; ddd]|[a -&gt; aaa, b -&gt; bbb, c -&gt; ccc, d -&gt; ddd]|\n+--------------------+--------------------+----------------------------------------+\n</code></pre>"},{"location":"apache-spark/maptype-columns/#using-structtype-columns-instead-of-maptype-columns","title":"Using StructType columns instead of MapType columns","text":"<p>Let's create a DataFrame that stores information about athletes.</p> <pre><code>val athletesDF = spark.createDF(\n  List(\n    (\"lebron\",\n      Map(\n        \"height\" -&gt; \"6.67\",\n        \"units\" -&gt; \"feet\"\n      )\n    ),\n    (\"messi\",\n      Map(\n        \"height\" -&gt; \"1.7\",\n        \"units\" -&gt; \"meters\"\n      )\n    )\n  ), List(\n    (\"name\", StringType, true),\n    (\"stature\", MapType(StringType, StringType, true), true)\n  )\n)\n\nathletesDF.show(false)\n</code></pre> <pre><code>+------+--------------------------------+\n|name  |stature                         |\n+------+--------------------------------+\n|lebron|[height -&gt; 6.67, units -&gt; feet] |\n|messi |[height -&gt; 1.7, units -&gt; meters]|\n+------+--------------------------------+\n</code></pre> <pre><code>athletesDF.printSchema()\n\nroot\n |-- name: string (nullable = true)\n |-- stature: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n</code></pre> <p><code>stature</code> is a MapType column, but we can also store stature as a StructType column.</p> <pre><code>val data = Seq(\n  Row(\"lebron\", Row(\"6.67\", \"feet\")),\n  Row(\"messi\", Row(\"1.7\", \"meters\"))\n)\n\nval schema = StructType(\n  List(\n    StructField(\"player_name\", StringType, true),\n    StructField(\n      \"stature\",\n      StructType(\n        List(\n          StructField(\"height\", StringType, true),\n          StructField(\"unit\", StringType, true)\n        )\n      ),\n      true\n    )\n  )\n)\n\nval athletesDF = spark.createDataFrame(\n  spark.sparkContext.parallelize(data),\n  schema\n)\n</code></pre> <pre><code>athletesDF.show(false)\n\n+-----------+-------------+\n|player_name|stature      |\n+-----------+-------------+\n|lebron     |[6.67, feet] |\n|messi      |[1.7, meters]|\n+-----------+-------------+\n</code></pre> <pre><code>athletesDF.printSchema()\n\nroot\n |-- player_name: string (nullable = true)\n |-- stature: struct (nullable = true)\n |    |-- height: string (nullable = true)\n |    |-- unit: string (nullable = true)\n</code></pre> <p>Sometimes both StructType and MapType columns can solve the same problem and you can choose between the two.</p>"},{"location":"apache-spark/maptype-columns/#writing-maptype-columns-to-disk","title":"Writing MapType columns to disk","text":"<p>The CSV file format cannot handle MapType columns.</p> <p>This code will error out.</p> <pre><code>val outputPath = new java.io.File(\"./tmp/csv_with_map/\").getCanonicalPath\n\nspark.createDF(\n  List(\n    (Map(\"a\" -&gt; \"aaa\", \"b\" -&gt; \"bbb\"))\n  ), List(\n    (\"some_data\", MapType(StringType, StringType, true), true)\n  )\n).write.csv(outputPath)\n</code></pre> <p>Here's the error message:</p> <pre><code>writing to disk\n- cannot write maps to disk with the CSV format *** FAILED ***\n  org.apache.spark.sql.AnalysisException: CSV data source does not support map&lt;string,string&gt; data type.;\n  at org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:69)\n  at org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:67)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:67)\n  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifyWriteSchema(DataSourceUtils.scala:34)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:100)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n</code></pre> <p>MapType columns can be written out with the Parquet file format. This code runs just fine:</p> <pre><code>val outputPath = new java.io.File(\"./tmp/csv_with_map/\").getCanonicalPath\n\nspark.createDF(\n  List(\n    (Map(\"a\" -&gt; \"aaa\", \"b\" -&gt; \"bbb\"))\n  ), List(\n    (\"some_data\", MapType(StringType, StringType, true), true)\n  )\n).write.parquet(outputPath)\n</code></pre>"},{"location":"apache-spark/maptype-columns/#conclusion","title":"Conclusion","text":"<p>MapType columns are a great way to store key / value pairs of arbitrary lengths in a DataFrame column.</p> <p>Spark 2.4 added a lot of native functions that make it easier to work with MapType columns. Prior to Spark 2.4, developers were overly reliant on UDFs for manipulating MapType columns.</p> <p>StructType columns can often be used instead of a MapType column. Study both of these column types closely so you can understand the tradeoffs and intelligently select the best column type for your analysis.</p>"},{"location":"apache-spark/mill-build-tool/","title":"Building Spark Projects with Mill","text":"<p>Mill is a SBT alternative that can be used to build Spark projects.</p> <p>This post explains how to create a Spark project with Mill and why you might want to use it instead of SBT.</p>"},{"location":"apache-spark/mill-build-tool/#project-structure","title":"Project structure","text":"<p>Here's is the directory structure of the <code>mill_spark_example</code> project:</p> <pre><code>mill_spark_example/\n  foo/\n    src/\n      Example.scala\n  test/\n    src/\n      ExampleTests.scala\n  out/\n  build.sc\n</code></pre> <p>The <code>build.sc</code> file specifies the project dependencies, similar to the <code>build.sbt</code> file for SBT projects.</p>"},{"location":"apache-spark/mill-build-tool/#running-a-test","title":"Running a test","text":"<p>Let's add a simple DataFrame transformation to the <code>foo/src/Example.scala</code> file:</p> <pre><code>package foo\n\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport com.github.mrpowers.spark.daria.sql.functions.removeAllWhitespace\n\nobject Example {\n\n  def withGreeting()(df: DataFrame): DataFrame = {\n    df.withColumn(\"greeting\", removeAllWhitespace(lit(\"hello YOU !\")))\n  }\n\n}\n</code></pre> <p>Now let's add a test in <code>foo/test/src/ExampleTests.scala</code>:</p> <pre><code>package foo\n\nimport utest._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport com.github.mrpowers.spark.fast.tests.DatasetComparer\nimport com.github.mrpowers.spark.daria.sql.SparkSessionExt._\n\nobject ExampleTests extends TestSuite with SparkSessionTestWrapper with DatasetComparer {\n\n  val tests = Tests {\n\n    import spark.implicits._\n\n    \"withGreeting\" - {\n\n      val sourceDF = spark.createDF(\n        List(\n          (\"jose\"),\n          (\"li\"),\n          (\"luisa\")\n        ), List(\n          (\"name\", StringType, true)\n        )\n      )\n\n      val actualDF = sourceDF.transform(Example.withGreeting())\n\n      val expectedDF = Seq(\n        (\"jose\", \"helloYOU!\"),\n        (\"li\", \"helloYOU!\"),\n        (\"luisa\", \"helloYOU!\")\n      ).toDF(\"name\", \"greeting\")\n\n      assertSmallDatasetEquality(actualDF, expectedDF, ignoreNullable = true)\n\n    }\n\n  }\n\n}\n</code></pre> <p>The <code>foo/src/Example.scala</code> needs access to Spark SQL and spark-daria. The <code>foo/test/src/ExampleTests.scala</code> file needs access to Spark SQL, uTest, spark-daria, and spark-fast-tests.</p> <p>Let's create a simple <code>build.sc</code> file that specifies the application and test dependencies.</p> <pre><code>import mill._\nimport mill.scalalib._\nimport coursier.maven.MavenRepository\n\nobject foo extends ScalaModule {\n  def scalaVersion = \"2.11.12\"\n\n  def repositories = super.repositories ++ Seq(\n    MavenRepository(\"http://dl.bintray.com/spark-packages/maven\")\n  )\n\n  def ivyDeps = Agg(\n    ivy\"org.apache.spark::spark-sql:2.3.0\",\n    ivy\"mrpowers:spark-daria:0.26.1-s_2.11\",\n  )\n\n  object test extends Tests{\n    def ivyDeps = Agg(\n      ivy\"org.apache.spark::spark-sql:2.3.0\",\n      ivy\"com.lihaoyi::utest:0.6.0\",\n      ivy\"MrPowers:spark-fast-tests:0.17.1-s_2.11\",\n      ivy\"mrpowers:spark-daria:0.26.1-s_2.11\",\n    )\n    def testFrameworks = Seq(\"utest.runner.Framework\")\n  }\n}\n</code></pre> <p>We can run our tests with the <code>mill foo.test</code> command.</p>"},{"location":"apache-spark/mill-build-tool/#thin-jar-file","title":"Thin JAR file","text":"<p>The <code>mill foo.jar</code> command builds a thin JAR file that gets written out to <code>out/foo/jar/dest/out.jar</code>.</p> <p>We can use the <code>jar</code> command to see that the JAR file only includes the <code>Example</code> code:</p> <pre><code>$ jar tvf out/foo/jar/dest/out.jar\n    49 Thu Apr 04 11:21:08 EDT 2019 META-INF/MANIFEST.MF\n  1265 Thu Apr 04 09:24:30 EDT 2019 foo/Example$.class\n   843 Thu Apr 04 09:24:30 EDT 2019 foo/Example.class\n</code></pre>"},{"location":"apache-spark/mill-build-tool/#fat-jar-file","title":"Fat JAR file","text":"<p>The <code>mill foo.assembly</code> command builds a fat JAR file that gets written out to <code>out/foo/assembly/dest/out.jar</code>.</p> <p>The fat JAR file contains all the Spark, Scala, spark-daria, and project classes.</p> <p>Let's update the <code>build.sc</code> file, so the assembly JAR file does not contain any Spark SQL or Scala classes.</p> <pre><code>import mill._\nimport mill.scalalib._\nimport mill.modules.Assembly\nimport coursier.maven.MavenRepository\n\nobject foo extends ScalaModule {\n  def scalaVersion = \"2.11.12\"\n\n  def repositories = super.repositories ++ Seq(\n    MavenRepository(\"http://dl.bintray.com/spark-packages/maven\")\n  )\n\n  def compileIvyDeps = Agg(\n    ivy\"org.apache.spark::spark-sql:2.3.0\"\n  )\n\n  def ivyDeps = Agg(\n    ivy\"mrpowers:spark-daria:0.26.1-s_2.11\"\n  )\n\n  def assemblyRules = Assembly.defaultRules ++\n    Seq(\"scala/.*\", \"org\\\\.apache\\\\.spark/.*\")\n      .map(Assembly.Rule.ExcludePattern.apply)\n\n  object test extends Tests{\n    def ivyDeps = Agg(\n      ivy\"org.apache.spark::spark-sql:2.3.0\",\n      ivy\"com.lihaoyi::utest:0.6.0\",\n      ivy\"MrPowers:spark-fast-tests:0.17.1-s_2.11\",\n      ivy\"mrpowers:spark-daria:0.26.1-s_2.11\",\n    )\n    def testFrameworks = Seq(\"utest.runner.Framework\")\n  }\n}\n</code></pre>"},{"location":"apache-spark/mill-build-tool/#is-mill-better","title":"Is Mill better?","text":"<p>TODO</p> <ul> <li>Figure out if Mill runs a test suite faster</li> <li>Figure out if Mill can generate JAR files faster</li> </ul>"},{"location":"apache-spark/optimizing-data-lakes/","title":"Optimizing Data Lakes for Apache Spark","text":"<p>Spark code will run faster with certain data lakes than others.</p> <p>For example, Spark will run slowly if the data lake uses gzip compression and has unequally sized files (especially if there are a lot of small files). The code will run fast if the data lake contains equally sized 1GB Parquet files that use snappy compression.</p> <p>This blog post outlines the data lake characteristics that are desirable for Spark analyses.</p> <ol> <li>File formats</li> <li>Compression algorithms</li> <li>Small file problem</li> <li>Partitioning schemes</li> </ol> <p>TL;DR:</p> <ul> <li>Use 1GB Parquet files with Snappy compression</li> <li>Solving the small file problem is important</li> <li>Partitioning data lakes is important</li> </ul>"},{"location":"apache-spark/optimizing-data-lakes/#file-formats","title":"File formats","text":"<p>Spark works with many file formats including Parquet, CSV, JSON, OCR, Avro, and text files.</p> <p>TL;DR Use Apache Parquet instead of CSV or JSON whenever possible, because it's faster and better.</p> <p>JSON is the worst file format for distributed systems and should be avoided whenever possible.</p>"},{"location":"apache-spark/optimizing-data-lakes/#row-vs-column-oriented-formats","title":"Row vs. Column oriented formats","text":"<p>CSV, JSON, and Avro are row oriented data formats.</p> <p>OCR and Parquet are column oriented data formats.</p> <p>Row oriented file formats require data from all the rows to be transmitted over the wire for every analysis.</p> <p>Suppose you have the following DataFrame (<code>df</code>) and would like to query the <code>city</code> column.</p> <pre><code>+-------+---------+----------+\n|   city|  country|population|\n+-------+---------+----------+\n| Boston|      USA|      0.67|\n|  Dubai|      UAE|       3.1|\n|Cordoba|Argentina|      1.39|\n+-------+---------+----------+\n</code></pre> <p>If the data is persisted as a Parquet file, then <code>df.select(\"city\")</code> will only have to transmit one column worth of data across the wire.</p> <p>If the data is persisted as a CSV file, then <code>df.select(\"city\")</code> will transmit all the data across the wire.</p>"},{"location":"apache-spark/optimizing-data-lakes/#lazy-vs-eager-evaluation","title":"Lazy vs eager evaluation","text":"<p>As discussed in this blog post CSV files are sometimes eagerly evaluated so Spark needs to perform a slow process to infer the schema (JSON files are always eagerly evaluated).</p> <p>The Parquet file format makes it easy to avoid eager evaluation.</p> <p>Spark can easily determine the schema of Parquet files from metadata, so it doesn't need to go through the time consuming process of reading files and inferring the schema.</p>"},{"location":"apache-spark/optimizing-data-lakes/#ease-of-compression","title":"Ease of compression","text":"<p>Column oriented file formats are more compressible:</p> <p>Intuitively, data stored in columns is more compressible than data stored in rows. Compression algorithms perform better on data with low information entropy (high data value locality). Take, for example, a database table containing information about customers (name, phone number, e-mail address, snail-mail address, etc.). Storing data in columns allows all of the names to be stored together, all of the phone numbers together, etc. Certainly phone numbers are more similar to each other than surrounding text fields like e-mail addresses or names. Further, if the data is sorted by one of the columns, that column will be super-compressible (for example, runs of the same value can be run-length encoded).</p>"},{"location":"apache-spark/optimizing-data-lakes/#splittable-compression-algorithms","title":"Splittable compression algorithms","text":"<p>Files can be compressed with gzip, lzo, bzip2 and other compression algorithms.</p> <p>gzipped files can't be split, so they're not ideal as described in this blog post.</p> <p>bzip2 files are splittable, but they are expensive from a CPU perspective. This blogger decided to go with uncompressed files after looking into the gzip and bzip2 options!</p> <p>Snappy is a different type of compression algorithm that \"aims for very high speeds and reasonable compression\". Snappy is also a splittable (there are some nuances, but you can think of Snappy as splittable).</p> <p>The Snappy compression algorithm is used by default in Spark and you should use Snappy unless you have a good reason to deviate from the Spark standard.</p>"},{"location":"apache-spark/optimizing-data-lakes/#small-file-problem","title":"Small file problem","text":"<p>S3 is an object store and listing files takes a long time. S3 does not list files quickly like a Unix-like object store.</p> <p>Listing files is even slower when you glob, e.g. <code>spark.read.parquet(\"/mnt/my-bucket/lake/*/*\")</code>.</p> <p>For large data lakes, it's ideal to use evenly sized 1 GB Parquet files. You'll want to create a process to periodically compact the small files into 1 GB Parquet files to keep the small file problem under control.</p> <p>Incremental updates tend to create lots of small files. The more frequently the data lake is incrementally updated, the more rapidly small files will accumulate.</p>"},{"location":"apache-spark/optimizing-data-lakes/#disk-partitioning","title":"Disk partitioning","text":"<p>You might want to partition your data on disk if you're frequently filtering on a certain column. Suppose you have a DataFrame (<code>df</code>) with <code>country</code>, <code>name</code>, and <code>date_of_birth</code> columns. You can create a data lake that's partitioned by <code>country</code> if you're frequently writing queries like <code>df.filter($\"country\" === \"China\")</code>.</p> <p>Read this blog post for more information on creating and querying partitioned data lakes.</p> <p>A partitioned data lake can cause queries to run 100 times+ faster - don't overlook this important feature.</p>"},{"location":"apache-spark/optimizing-data-lakes/#multiple-data-lakes","title":"Multiple data lakes","text":"<p>You might need to store the same data in different lakes that are optimized for different types of queries. Partitioned data lakes are great for queries that filter on a partitioned column, but they tend to have a lot of files and will be slower for other queries.</p> <p>We'll want to use a unpartitioned lake for queries like this: <code>unpartitionedLakeDF.filter($\"date_of_birth\" === \"2018-01-01\")</code>.</p> <p>As discussed in the previous section, a data lake that's partitioned on the <code>country</code> column is perfect for queries like this: <code>countryPartitionedDF.filter($\"country\" === \"China\")</code>.</p>"},{"location":"apache-spark/optimizing-data-lakes/#conclusion","title":"Conclusion","text":"<p>Properly designed data lakes will save your company a lot of time and money. I've seen queries that run 100 times faster and cheaper on a partitioned data lake for example.</p> <p>This blog post explains common problems found in data lakes, but doesn't explain how to implement the solutions. These questions are still open.</p> <ul> <li>How should data lakes get updated incrementally?</li> <li>How should small files get compacted into larger files?</li> <li>How can multiple data lakes get updated simultaneously?</li> </ul> <p>We'll dive into these details in future posts.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/","title":"Writing out single files with Spark (CSV or Parquet)","text":"<p>This blog explains how to write out a DataFrame to a single file with Spark. It also describes how to write out data in a file with a specific name, which is surprisingly challenging.</p> <p>Writing out a single file with Spark isn't typical. Spark is designed to write out multiple files in parallel. Writing out many files at the same time is faster for big datasets.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/#default-behavior","title":"Default behavior","text":"<p>Let's create a DataFrame, use <code>repartition(3)</code> to create three memory partitions, and then write out the file to disk.</p> <pre><code>val df = Seq(\"one\", \"two\", \"three\").toDF(\"num\")\ndf\n  .repartition(3)\n  .write.csv(sys.env(\"HOME\")+ \"/Documents/tmp/some-files\")\n</code></pre> <p>Here's the files that are generated on disk.</p> <pre><code>Documents/\n  tmp/\n    some-files/\n      _SUCCESS\n      part-00000-b69460e8-fdc3-4593-bab4-bd15fa0dad98-c000.csv\n      part-00001-b69460e8-fdc3-4593-bab4-bd15fa0dad98-c000.csv\n      part-00002-b69460e8-fdc3-4593-bab4-bd15fa0dad98-c000.csv\n</code></pre> <p>Spark writes out one file per memory partition. We used <code>repartition(3)</code> to create three memory partitions, so three files were written.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/#writing-out-one-file-with-repartition","title":"Writing out one file with repartition","text":"<p>We can use <code>repartition(1)</code> write out a single file.</p> <pre><code>df\n  .repartition(1)\n  .write.csv(sys.env(\"HOME\")+ \"/Documents/tmp/one-file-repartition\")\n</code></pre> <p>Here's the file that's written to disk.</p> <pre><code>Documents/\n  tmp/\n    one-file-repartition/\n      _SUCCESS\n      part-00000-d5a15f40-e787-4fd2-b8eb-c810d973b6fe-c000.csv\n</code></pre> <p>We can't control the name of the file that's written. We can control the name of the directory, but not the file itself.</p> <p>This solution isn't sufficient when you want to write data to a file with a specific name.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/#writing-out-a-single-file-with-coalesce","title":"Writing out a single file with coalesce","text":"<p>We can also use <code>coalesce(1)</code> to write out a single file.</p> <pre><code>df\n  .coalesce(1)\n  .write.csv(sys.env(\"HOME\")+ \"/Documents/tmp/one-file-coalesce\")\n</code></pre> <p>Here's what's outputted.</p> <pre><code>Documents/\n  tmp/\n    one-file-coalesce/\n      _SUCCESS\n      part-00000-c7521799-e6d8-498d-b857-2aba7f56533a-c000.csv\n</code></pre> <p>coalesce doesn't let us set a specific filename either (it only let's us customize the folder name). We'll need to use spark-daria to access a method that'll output a single file.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/#writing-out-a-file-with-a-specific-name","title":"Writing out a file with a specific name","text":"<p>You can use the <code>DariaWriters.writeSingleFile</code> function defined in spark-daria to write out a single file with a specific filename.</p> <p>Here's the code that writes out the contents of a DataFrame to the <code>~/Documents/better/mydata.csv</code> file.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DariaWriters\n\nDariaWriters.writeSingleFile(\n    df = df,\n    format = \"csv\",\n    sc = spark.sparkContext,\n    tmpFolder = sys.env(\"HOME\") + \"/Documents/better/tmp\",\n    filename = sys.env(\"HOME\") + \"/Documents/better/mydata.csv\"\n)\n</code></pre> <p>The <code>writeSingleFile</code> method let's you name the file without worrying about complicated implementation details.</p> <p><code>writeSingleFile</code> is uses <code>repartition(1)</code> and Hadoop filesystem methods underneath the hood. All of the Hadoop filesystem methods are available in any Spark runtime environment - you don't need to attach any separate JARs.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/#compatibility-with-other-filesystems","title":"Compatibility with other filesystems","text":"<p>It's best to use the Hadoop filesystem methods when moving, renaming, or deleting files, so your code will work on multiple platforms. <code>writeSingleFile</code> works on your local filesystem and in S3. You can use this approach when running Spark locally or in a Databricks notebook.</p> <p>There are other solutions to this problem that are not cross platform. There are solutions that only work in Databricks notebooks, or only work in S3, or only work on a Unix-like operating system.</p> <p>The Hadoop filesystem methods are clumsy to work with, but the best option cause they work on multiple platforms.</p> <p>The <code>writeSingleFile</code> method uses the <code>fs.rename()</code> Hadoop method, as described in this answer. Here's the psuedocode:</p> <pre><code>val src = new Path(\"s3a://bucket/data/src\")\nval dest = new Path(\"s3a://bucket/data/dest\")\nval conf = sc.hadoopConfiguration   // assuming sc = spark context\nval fs = src.getFileSystem(conf)\nfs.rename(src, dest)\n</code></pre>"},{"location":"apache-spark/output-one-file-csv-parquet/#copymerge","title":"copyMerge","text":"<p>Hadoop 2 has a <code>FileUtil.copyMerge()</code> method that's an elegant solution to this problem, but this method is deprecated and will be removed in Hadoop 3. There is an answer in this thread that reimplements <code>copyMerge</code> for Hadoop 3 users.</p> <p>In any case, don't write code that relies on the <code>FileUtil.copyMerge()</code> method. We know that method will be inaccessible when Spark upgrades to Hadoop 3 and you don't want to rely on a deprecated method that'll break at some unknown time in the future.</p>"},{"location":"apache-spark/output-one-file-csv-parquet/#next-steps","title":"Next steps","text":"<p>You'll typically want to write out multiple files in parallel, but in the rare occasions when you want to write out a single file, the spark-daria <code>writeSingleFile</code> method will help.</p> <p>Try your best to wrap the complex Hadoop filesystem logic in helper methods that are tested separated. Combining Hadoop filesystem operations and Spark code in the same method will make your code too complex.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/","title":"Fast Filtering with Spark PartitionFilters and PushedFilters","text":"<p>Spark can use the disk partitioning of files to greatly speed up certain filtering operations.</p> <p>This post explains the difference between memory and disk partitioning, describes how to analyze physical plans to see when filters are applied, and gives a conceptual overview of why this design pattern can provide massive performace gains.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/#normal-dataframe-filter","title":"Normal DataFrame filter","text":"<p>Let's create a CSV file (<code>/Users/powers/Documents/tmp/blog_data/people.csv</code>) with the following data:</p> <pre><code>first_name,last_name,country\nErnesto,Guevara,Argentina\nVladimir,Putin,Russia\nMaria,Sharapova,Russia\nBruce,Lee,China\nJack,Ma,China\n</code></pre> <p>Let's read in the CSV data into a DataFrame:</p> <pre><code>val df = spark\n  .read\n  .option(\"header\", \"true\")\n  .csv(\"/Users/powers/Documents/tmp/blog_data/people.csv\")\n</code></pre> <p>Let's write a query to fetch all the Russians in the CSV file with a <code>first_name</code> that starts with <code>M</code>.</p> <pre><code>df\n  .where($\"country\" === \"Russia\" &amp;&amp; $\"first_name\".startsWith(\"M\"))\n  .show()\n</code></pre> <pre><code>+----------+---------+-------+\n|first_name|last_name|country|\n+----------+---------+-------+\n|     Maria|Sharapova| Russia|\n+----------+---------+-------+\n</code></pre> <p>Let's use <code>explain()</code> to see how the query is executed.</p> <pre><code>df\n  .where($\"country\" === \"Russia\" &amp;&amp; $\"first_name\".startsWith(\"M\"))\n  .explain()\n</code></pre> <pre><code>== Physical Plan ==\nProject [first_name#12, last_name#13, country#14]\n+- Filter (((isnotnull(country#14) &amp;&amp; isnotnull(first_name#12)) &amp;&amp; (country#14 = Russia)) &amp;&amp; StartsWith(first_name#12, M))\n   +- FileScan csv [first_name#12,last_name#13,country#14]\n        Batched: false,\n        Format: CSV,\n        Location: InMemoryFileIndex[file:/Users/powers/Documents/tmp/blog_data/people.csv],\n        PartitionFilters: [],\n        PushedFilters: [IsNotNull(country), IsNotNull(first_name), EqualTo(country,Russia), StringStartsWith(first_name,M)],\n        ReadSchema: struct\n</code></pre> <p>Take note that there are no <code>PartitionFilters</code> in the physical plan.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/#partitionby","title":"<code>partitionBy()</code>","text":"<p>The <code>repartition()</code> method partitions the data in memory and the <code>partitionBy()</code> method partitions data in folders when it's written out to disk.</p> <p>Let's write out the data in partitioned CSV files.</p> <pre><code>df\n  .repartition($\"country\")\n  .write\n  .option(\"header\", \"true\")\n  .partitionBy(\"country\")\n  .csv(\"/Users/powers/Documents/tmp/blog_data/partitioned_lake\")\n</code></pre> <p>Here's what the directory structure looks like:</p> <pre><code>partitioned_lake/\n  country=Argentina/\n    part-00044-c5d2f540-e89b-40c1-869d-f9871b48c617.c000.csv\n  country=China/\n    part-00059-c5d2f540-e89b-40c1-869d-f9871b48c617.c000.csv\n  country=Russia/\n    part-00002-c5d2f540-e89b-40c1-869d-f9871b48c617.c000.csv\n</code></pre> <p>Here are the contents of the CSV file in the <code>country=Russia</code> directory.</p> <pre><code>first_name,last_name\nVladimir,Putin\nMaria,Sharapova\n</code></pre> <p>Notice that the <code>country</code> column is not included in the CSV file anymore. Spark has abstracted a column from the CSV file to the directory name.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/#partitionfilters","title":"PartitionFilters","text":"<p>Let's read from the partitioned data folder, run the same filters, and see how the physical plan changes.</p> <p>Let's run the same filter as before, but on the partitioned lake, and examine the physical plan.</p> <pre><code>val partitionedDF = spark\n  .read\n  .option(\"header\", \"true\")\n  .csv(\"/Users/powers/Documents/tmp/blog_data/partitioned_lake\")\n\npartitionedDF\n  .where($\"country\" === \"Russia\" &amp;&amp; $\"first_name\".startsWith(\"M\"))\n  .explain()\n</code></pre> <pre><code>== Physical Plan ==\nProject [first_name#74, last_name#75, country#76]\n+- Filter (isnotnull(first_name#74) &amp;&amp; StartsWith(first_name#74, M))\n   +- FileScan csv [first_name#74,last_name#75,country#76]\n        Batched: false,\n        Format: CSV,\n        Location: InMemoryFileIndex[file:/Users/powers/Documents/tmp/blog_data/partitioned_lake],\n        PartitionCount: 1,\n        PartitionFilters: [isnotnull(country#76), (country#76 = Russia)],\n        PushedFilters: [IsNotNull(first_name), StringStartsWith(first_name,M)],\n        ReadSchema: struct\n</code></pre> <p>You need to examine the physical plans carefully to identify the differences.</p> <p>When filtering on <code>df</code> we have <code>PartitionFilters: []</code> whereas when filtering on <code>partitionedDF</code> we have <code>PartitionFilters: [isnotnull(country#76), (country#76 = Russia)]</code>.</p> <p>Spark only grabs data from certain partitions and skips all of the irrelevant partitions. Data skipping allows for a big performance boost.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/#pushedfilters","title":"PushedFilters","text":"<p>When we filter off of <code>df</code>, the pushed filters are <code>[IsNotNull(country), IsNotNull(first_name), EqualTo(country,Russia), StringStartsWith(first_name,M)]</code>.</p> <p>When we filter off of <code>partitionedDf</code>, the pushed filters are <code>[IsNotNull(first_name), StringStartsWith(first_name,M)]</code>.</p> <p>Spark doesn't need to push the <code>country</code> filter when working off of <code>partitionedDF</code> because it can use a partition filter that is a lot faster.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/#partitioning-in-memory-vs-partitioning-on-disk","title":"Partitioning in memory vs. partitioning on disk","text":"<p><code>repartition()</code> and <code>coalesce()</code> change how data is partitioned in memory.</p> <p><code>partitionBy()</code> changes how data is partitioned when it's written out to disk.</p> <p>Use <code>repartition()</code> before writing out partitioned data to disk with <code>partitionBy()</code> because it'll execute a lot faster and write out fewer files.</p> <p>Partitioning in memory and paritioning on disk are related, but completely different concepts that expert Spark programmers must master.</p>"},{"location":"apache-spark/partition-filters-pushed-filters/#disk-partitioning-with-skewed-columns","title":"Disk partitioning with skewed columns","text":"<p>Suppose you have a data lake with information on all 7.6 billion people in the world. The country column is skewed because a lot of people live in countries like China and India and compatively few people live in countries like Montenegro.</p> <p>This code is problematic because it will write out the data in each partition as a single file.</p> <pre><code>df\n  .repartition($\"country\")\n  .write\n  .option(\"header\", \"true\")\n  .partitionBy(\"country\")\n  .csv(\"/Users/powers/Documents/tmp/blog_data/partitioned_lake\")\n</code></pre> <p>We don't our data lake to contain some massive files because that'll make Spark reads / writes unnecessarily slow.</p> <p>If we don't do any in memory reparitioning, Spark will write out a ton of files for each partition and our data lake will contain way too many small files.</p> <pre><code>df\n  .write\n  .option(\"header\", \"true\")\n  .partitionBy(\"country\")\n  .csv(\"/Users/powers/Documents/tmp/blog_data/partitioned_lake\")\n</code></pre> <p>This answer explains how to intelligently repartition in memory before writing out to disk with <code>partitionBy()</code>.</p> <p>Here's how we can limit each partition to a maximum of 100 files.</p> <pre><code>import org.apache.spark.sql.functions.rand\n\ndf\n  .repartition(100, $\"country\", rand)\n  .write\n  .option(\"header\", \"true\")\n  .partitionBy(\"country\")\n  .csv(\"/Users/powers/Documents/tmp/blog_data/partitioned_lake\")\n</code></pre>"},{"location":"apache-spark/partition-filters-pushed-filters/#next-steps","title":"Next steps","text":"<p>I recommend rereading this blog post and running all the code on your local machine with the Spark shell.</p> <p>Effective disk partitioning can greatly speed up filter operations.</p>"},{"location":"apache-spark/partitionby/","title":"Partitioning on Disk with partitionBy","text":"<p>Spark writers allow for data to be partitioned on disk with <code>partitionBy</code>. Some queries can run 50 to 100 times faster on a partitioned data lake, so partitioning is vital for certain queries.</p> <p>Creating and maintaining partitioned data lake is hard.</p> <p>This blog post discusses how to use <code>partitionBy</code> and explains the challenges of partitioning production-sized datasets on disk. Different memory partitioning tactics will be discussed that let <code>partitionBy</code> operate more efficiently.</p> <p>You'll need to master the concepts covered in this blog to create partitioned data lakes on large datasets, especially if you're dealing with a high cardinality or high skew partition key.</p> <p>Make sure to read\u00a0Writing Beautiful Spark Code\u00a0for a detailed overview of how to create production grade partitioned lakes.</p>"},{"location":"apache-spark/partitionby/#memory-partitioning-vs-disk-partitioning","title":"Memory partitioning vs. disk partitioning","text":"<p><code>coalesce()</code> and <code>repartition()</code> change the memory partitions for a DataFrame.</p> <p><code>partitionBy()</code> is a <code>DataFrameWriter</code> method that specifies if the data should be written to disk in folders. By default, Spark does not write data to disk in nested folders.</p> <p>Memory partitioning is often important independent of disk partitioning. In order to write data on disk properly, you'll almost always need to repartition the data in memory first.</p>"},{"location":"apache-spark/partitionby/#simple-example","title":"Simple example","text":"<p>Suppose we have the following CSV file with <code>first_name</code>, <code>last_name</code>, and <code>country</code> columns:</p> <pre><code>first_name,last_name,country\nErnesto,Guevara,Argentina\nVladimir,Putin,Russia\nMaria,Sharapova,Russia\nBruce,Lee,China\nJack,Ma,China\n</code></pre> <p>Let's partition this data on disk with <code>country</code> as the partition key. Let's create one file per partition.</p> <pre><code>val path = new java.io.File(\"./src/main/resources/ss_europe/\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/partitioned_lake1/\").getCanonicalPath\ndf\n  .repartition(col(\"country\"))\n  .write\n  .partitionBy(\"country\")\n  .parquet(outputPath)\n</code></pre> <p>Here's what the data will look like on disk:</p> <pre><code>partitioned_lake1/\n  country=Argentina/\n    part-00044-cf737804-90ea-4c37-94f8-9aa016f6953a.c000.snappy.parquet\n  country=China/\n    part-00059-cf737804-90ea-4c37-94f8-9aa016f6953a.c000.snappy.parquet\n  country=Russia/\n    part-00002-cf737804-90ea-4c37-94f8-9aa016f6953a.c000.snappy.parquet\n</code></pre> <p>Creating one file per disk partition is not going to work for production sized datasets. Suppose the China partition contains 100GB of data - we won't be able to write out all of that data in a single file.</p>"},{"location":"apache-spark/partitionby/#partitionby-with-repartition5","title":"partitionBy with repartition(5)","text":"<p>Let's run <code>repartition(5)</code> to get each row of data in a separate memory partition before running <code>partitionBy</code> and see how that impacts how the files get written to disk.</p> <pre><code>val outputPath = new java.io.File(\"./tmp/partitioned_lake2/\").getCanonicalPath\ndf\n  .repartition(5)\n  .write\n  .partitionBy(\"country\")\n  .parquet(outputPath)\n</code></pre> <p>Here's what the files look like on disk:</p> <pre><code>partitioned_lake2/\n  country=Argentina/\n    part-00003-c2d1b76a-aa61-437f-affc-a6b322f1cf42.c000.snappy.parquet\n  country=China/\n    part-00000-c2d1b76a-aa61-437f-affc-a6b322f1cf42.c000.snappy.parquet\n    part-00004-c2d1b76a-aa61-437f-affc-a6b322f1cf42.c000.snappy.parquet\n  country=Russia/\n    part-00001-c2d1b76a-aa61-437f-affc-a6b322f1cf42.c000.snappy.parquet\n    part-00002-c2d1b76a-aa61-437f-affc-a6b322f1cf42.c000.snappy.parquet\n</code></pre> <p>The <code>partitionBy</code> writer will write out files on disk for each memory partition. The maximum number of files written out is the number of unique countries multiplied by the number of memory partitions.</p> <p>In this example, we have 3 unique countries * 5 memory partitions, so up to 15 files could get written out (if each memory partition had one Argentinian, one Chinese, and one Russian person). We only have 5 rows of data, so only 5 files are written in this example.</p>"},{"location":"apache-spark/partitionby/#partitionby-with-repartition1","title":"partitionBy with repartition(1)","text":"<p>If we repartition the data to one memory partition before partitioning on disk with <code>partitionBy</code>, then we'll write out a maximum of three files. numMemoryPartitions * numUniqueCountries = maxNumFiles. 1 * 3 = 3.</p> <p>Let's take a look at the code.</p> <pre><code>val outputPath = new java.io.File(\"./tmp/partitioned_lake2/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .partitionBy(\"country\")\n  .parquet(outputPath)\n</code></pre> <p>Here's what the files look like on disk:</p> <pre><code>partitioned_lake3/\n  country=Argentina/\n    part-00000-bc6ce757-d39f-489e-9677-0a7105b29e66.c000.snappy.parquet\n  country=China/\n    part-00000-bc6ce757-d39f-489e-9677-0a7105b29e66.c000.snappy.parquet\n  country=Russia/\n    part-00000-bc6ce757-d39f-489e-9677-0a7105b29e66.c000.snappy.parquet\n</code></pre>"},{"location":"apache-spark/partitionby/#partitioning-datasets-with-a-max-number-of-files-per-partition","title":"Partitioning datasets with a max number of files per partition","text":"<p>Let's use a dataset with 80 people from China, 15 people from France, and 5 people from Cuba. Here's a link to the data.</p> <p>Here's what the data looks like:</p> <pre><code>person_name,person_country\na,China\nb,China\nc,China\n...77 more China rows\na,France\nb,France\nc,France\n...12 more France rows\na,Cuba\nb,Cuba\nc,Cuba\n...2 more Cuba rows\n</code></pre> <p>Let's create 8 memory partitions and scatter the data randomly across the memory partitions (we'll write out the data to disk, so we can inspect the contents of a memory partition).</p> <pre><code>val outputPath = new java.io.File(\"./tmp/repartition_for_lake4/\").getCanonicalPath\ndf\n  .repartition(8, col(\"person_country\"), rand)\n  .write\n  .csv(outputPath)\n</code></pre> <p>Let's look at one of the CSV files that is outputted:</p> <pre><code>p,China\nf1,China\nn1,China\na2,China\nb2,China\nd2,China\ne2,China\nf,France\nc,Cuba\n</code></pre> <p>This technique helps us set a maximum number of files per partition when creating a partitioned lake. Let's write out the data to disk and observe the output.</p> <pre><code>val outputPath = new java.io.File(\"./tmp/partitioned_lake4/\").getCanonicalPath\ndf\n  .repartition(8, col(\"person_country\"), rand)\n  .write\n  .partitionBy(\"person_country\")\n  .csv(outputPath)\n</code></pre> <p>Here's what the files look like on disk:</p> <pre><code>partitioned_lake4/\n  person_country=China/\n    part-00000-0887fbd2-4d9f-454a-bd2a-de42cf7e7d9e.c000.csv\n    part-00001-0887fbd2-4d9f-454a-bd2a-de42cf7e7d9e.c000.csv\n    ... 6 more files\n  person_country=Cuba/\n    part-00002-0887fbd2-4d9f-454a-bd2a-de42cf7e7d9e.c000.csv\n    part-00003-0887fbd2-4d9f-454a-bd2a-de42cf7e7d9e.c000.csv\n    ... 2 more files\n  person_country=France/\n    part-00000-0887fbd2-4d9f-454a-bd2a-de42cf7e7d9e.c000.csv\n    part-00001-0887fbd2-4d9f-454a-bd2a-de42cf7e7d9e.c000.csv\n    ... 5 more files\n</code></pre> <p>Each disk partition will have up to 8 files. The data is split randomly in the 8 memory partitions. There won't be any output files for a given disk partition if the memory partition doesn't have any data for the country.</p> <p>This is better, but still not ideal. We have 4 files for Cuba and seven files for France, so too many small files are being created.</p> <p>Let's review the contents of our memory partition from earlier:</p> <pre><code>p,China\nf1,China\nn1,China\na2,China\nb2,China\nd2,China\ne2,China\nf,France\nc,Cuba\n</code></pre> <p><code>partitionBy</code> will split up this particular memory partition into three files: one China file with 7 rows of data, one France file with one row of data, and one Cuba file with one row of data.</p>"},{"location":"apache-spark/partitionby/#partitioning-dataset-with-max-rows-per-file","title":"Partitioning dataset with max rows per file","text":"<p>Let's write some code that'll create partitions with ten rows of data per file. We'd like our data to be stored in 8 files for China, one file for Cuba, and two files for France.</p> <p>We can use the <code>maxRecordsPerFile</code> option to output files with 10 rows.</p> <pre><code>val outputPath = new java.io.File(\"./tmp/partitioned_lake5/\").getCanonicalPath\ndf\n  .repartition(col(\"person_country\"))\n  .write\n  .option(\"maxRecordsPerFile\", 10)\n  .partitionBy(\"person_country\")\n  .csv(outputPath)\n</code></pre> <p>This technique is particularity important for partition keys that are highly skewed. The number of inhabitants by country is a good example of a partition key with high skew. For example Jamaica has 3 million people and China has 1.4 billion people - we'll want ~467 times more files in the China partition than the Jamaica partition.</p>"},{"location":"apache-spark/partitionby/#partitioning-dataset-with-max-rows-per-file-pre-spark-22","title":"Partitioning dataset with max rows per file pre Spark 2.2","text":"<p>The maxRecordsPerFile option was added in Spark 2.2, so you'll need to write your own custom solution if you're using an earlier version of Spark.</p> <pre><code>val countDF = df.groupBy(\"person_country\").count()\n\nval desiredRowsPerPartition = 10\n\nval joinedDF = df\n  .join(countDF, Seq(\"person_country\"))\n  .withColumn(\n    \"my_secret_partition_key\",\n    (rand(10) * col(\"count\") / desiredRowsPerPartition).cast(IntegerType)\n  )\n\nval outputPath = new java.io.File(\"./tmp/partitioned_lake6/\").getCanonicalPath\njoinedDF\n  .repartition(col(\"person_country\"), col(\"my_secret_partition_key\"))\n  .drop(\"count\", \"my_secret_partition_key\")\n  .write\n  .partitionBy(\"person_country\")\n  .csv(outputPath)\n</code></pre> <p>We calculate the total number of records per partition key and then create a <code>my_secret_partition_key</code> column rather than relying on a fixed number of partitions.</p> <p>You should choose the <code>desiredRowsPerPartition</code> based on what will give you ~1 GB files. If you have a 500 GB dataset with 750 million rows, set <code>desiredRowsPerPartition</code> to 1,500,000.</p>"},{"location":"apache-spark/partitionby/#small-file-problem","title":"Small file problem","text":"<p>Partitioned data lakes can quickly develop a small file problem when they're updated incrementally. It's hard to compact partitioned data lakes. As we've seen, it's even hard to make a partitioned data lake!</p> <p>Use the tactics outlined in this blog post to build your partitioned data lakes and start them off without the small file problem!</p>"},{"location":"apache-spark/partitionby/#conclusion","title":"Conclusion","text":"<p>Partitioned data lakes can be much faster to query (when filtering on the partition keys) because they allow for a massive data skipping.</p> <p>Creating and maintaining partitioned data lakes is challenging, but the performance gains make them a worthwhile effort.</p>"},{"location":"apache-spark/practical-introduction/","title":"Practical Introduction to Apache Spark","text":"<p>This book will teach you how to be a proficient Apache Spark programmer with minimal effort.</p> <p>Other books focus on the theoretical underpinnings of Spark. This book skips the theory and only covers the practical knowledge needed to write production grade Spark code.</p>"},{"location":"apache-spark/practical-introduction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Using the Spark shell</li> <li>Introduction to DataFrames</li> <li>Just enough Scala for Spark programmers</li> <li>Column methods</li> <li>SQL functions</li> <li>Creating Spark DataFrames</li> <li>Using null in Spark DataFrames</li> <li>User defined functions</li> <li>Chaining DataFrame transformations</li> <li>Schema independent DataFrame transformations</li> <li>Aggregations</li> <li>Dates / times (DateType, TimestampType)</li> <li>Defining DataFrame Schemas with StructField and StructType</li> <li>ArrayType columns</li> <li>MapType columns</li> <li>Managing the SparkSession, The DataFrame Entry Point</li> <li>CSV file format</li> <li>Parquet file format</li> <li>JSON file format</li> <li>Managing partitions with repartition and coalesce</li> <li>Different types of DataFrame joins</li> <li>Broadcast joins</li> <li>Introduction to SBT for Spark programmers</li> <li>Building Spark JAR files</li> <li>Testing with uTest</li> <li>Window functions</li> <li>Dependency injection</li> <li>Best practices</li> <li>Logistic Regressions</li> <li>Extending core classes</li> <li>Customizing logs</li> </ul>"},{"location":"apache-spark/practical-introduction/#scala-programming-language","title":"Scala programming language","text":"<p>Scala is a vast, multi-paradigm programming language that's notoriously difficult to learn.</p> <p>Scala can be used as a functional language, an object oriented language, or a mix of both.</p> <p>You only need to know a small fraction of the Scala programming langauge to be a productive Spark developer. You need to know how to write Scala functions, define packages, and import namespaces. Complex Scala programming topics can be ignored completely.</p> <p>I knew two programmers that began their Spark learning journey by taking the Functional Programming Principles in Scala course by Martin Odersky. Odersky created the Scala programming language and is incredibly intelligent. His course is amazing, but very hard, so my friends felt intimidated by Spark. How could they possibly learn Spark if they could barely make it through a beginners course on Scala?</p> <p>As it turns out, Spark programmers don't need to know anything about advanced Scala language features or functional programming, so courses like Functional Programming Principles in Scala are complete overkill.</p> <p>Check out the Just enough Scala for Spark programmers post to see just how little Scala is necessary for Spark.</p>"},{"location":"apache-spark/practical-introduction/#pyspark-or-scala","title":"PySpark or Scala?","text":"<p>Choosing between Python and Scala would normally be a big technology decision (e.g. if you were building a web application), but it's not as important for Spark because the APIs are so similar.</p> <p>Let's look at some Scala code that adds a column to a DataFrame:</p> <pre><code>import org.apache.spark.sql.functions._\n\ndf.withColumn(\"greeting\", lit(\"hi\"))\n</code></pre> <p>Here's the same code in Python:</p> <pre><code>from pyspark.sql.functions import lit\n\ndf.withColumn(\"greeting\", lit(\"hi\"))\n</code></pre> <p>PySpark code looks a lot like Scala code.</p> <p>The community is shifting towards PySpark so that's a good place to get started, but it's not a mission critical decision. It's all compiled to Spark at the end of the day!</p>"},{"location":"apache-spark/practical-introduction/#running-spark-code","title":"Running Spark code","text":"<p>Spinning up your own Spark clusters is complicated. You need to install Spark, create the driver node, create the worker nodes, and make sure messages are properly being sent across machines in the cluster.</p> <p>Databricks let's you easily spin up a cluster with Spark installed, so you don't need to worry about provisioning packages or cluster management. Databricks also has cool features like autoscaling clusters.</p> <p>If you're just getting started with Spark, it's probably best to pay a bit more and use Databricks.</p>"},{"location":"apache-spark/practical-introduction/#theoretical-stuff-to-ignore","title":"Theoretical stuff to ignore","text":"<p>Spark is a big data engine that's built on theoretical cluster computing principles.</p> <p>You don't need to know how Spark works to solve problems with Spark!</p> <p>Most books cover a lot of theoretical Spark before teaching the practical basics.</p> <p>This book aims to provide the easiest possible introduction to Apache Spark by starting with the practical basics. Enjoy!</p>"},{"location":"apache-spark/publishing-spark-projects-with-jitpack/","title":"Publishing Spark Projects with JitPack","text":"<p>JitPack is a package repository that provides easy access to your Spark projects that are checked into GitHub. JitPack is easier to use than Maven for open source projects and less hassle than maintaining a private artifact repository for closed source projects.</p> <p>This episode will show how to access existing Spark projects in JitPack and how to publish your own Spark projects in JitPack.</p>"},{"location":"apache-spark/publishing-spark-projects-with-jitpack/#access-existing-projects","title":"Access existing projects","text":"<p>Let's say you'd like to access the <code>v2.3.0_0.21.0</code> release of the spark-daria open source project.</p> <p>This library can be accessed by adding these two lines of code to your <code>build.sbt</code> file.</p> <pre><code>resolvers += \"jitpack\" at \"https://jitpack.io\"\nlibraryDependencies += \"com.github.mrpowers\" % \"spark-daria\" % \"v2.3.0_0.21.0\"\n</code></pre>"},{"location":"apache-spark/publishing-spark-projects-with-jitpack/#publishing-open-source-projects","title":"Publishing open source projects","text":"<p>JitPack can build JAR files based on GitHub releases, branches, or commits. It's best to work off JAR files that correspond to GitHub releases. Here is an example of a GitHub release that's picked up by JitPack.</p> <p>So all you need to do is make a GitHub release and JitPack will make the JAR file available for all consumers.</p> <p>You can easily write a script that makes multiple GitHub releases for your project for each Spark version that you support.</p>"},{"location":"apache-spark/publishing-spark-projects-with-jitpack/#publishing-closed-source-projects","title":"Publishing closed source projects","text":"<p>JitPack is also easy to use for closed source projects. You need to sign up for a plan and approve JitPack as an OAuth app for your organization's GitHub account.</p> <p>The JitPack auth credentials need to be added to your local machine, as described on this page.</p> <p>The credentials should be added to the <code>~/.sbt/.credentials</code> file as follows.</p> <pre><code>realm=JitPack\nhost=jitpack.io\nuser=AUTHENTICATION_TOKEN\npassword=.\n</code></pre> <p>The <code>AUTHENTICATION_TOKEN</code> is the JitPack personal authentication token that is supplied when you sign up for an account.</p> <p>The password is literally a period - that doesn't need to change.</p> <p>Closed source JitPack projects will only be accessible to developers with accounts in your organization's GitHub account.</p>"},{"location":"apache-spark/publishing-spark-projects-with-jitpack/#jitpack-alternatives","title":"JitPack alternatives","text":"<p>Spark developers are often confounded by the Scala ecosystem that heavily leverages Apache Maven and Apache Ivy. XML files are used extensively in the Java ecosystem.</p> <p>Creating a private artifact repository to host binary files is possible, but most data engineers don't want to provision a machine with Java and maintain an ec2 instance.</p> <p>JitPack alternatives are approachable to engineers with a lot of Java experience, but should be avoided if you're new to the Java ecosystem.</p>"},{"location":"apache-spark/publishing-spark-projects-with-jitpack/#next-steps","title":"Next steps","text":"<p>Spark developers can use JitPack and bypass Maven / private artifactory headaches.</p> <p>You'll still need to understand the principles outlined in the building Spark JAR files with SBT post so JitPack knows how to properly construct your JAR files, but you don't need to learn how to structure POM / XML files when working with Scala anymore.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/","title":"Scala Spark vs Python PySpark: Which is better?","text":"<p>Apache Spark code can be written with the Scala, Java, Python, or R APIs.</p> <p>Scala and Python are the most popular APIs. This blog post performs a detailed comparison of writing Spark with Scala and Python and helps users choose the language API that's best for their team.</p> <p>Both language APIs are great options for most workflows. Spark lets you write elegant code to run jobs on massive datasets - it's an amazing technology.</p> <p>Choosing the right language API is an important decision. It's hard to switch once you develop core libraries with one language.</p> <p>Making the right choice is difficult because of common misconceptions like \"Scala is 10x faster than Python\", which are completely misleading when comparing Scala Spark and PySpark.</p> <p>TL;DR:</p> <ul> <li>PySpark used to be buggy and poorly supported, but that's not true anymore. Python is a first class citizen in Spark. PySpark is a great option for most workflows.</li> <li>More people are familiar with Python, so PySpark is naturally their first choice when using Spark.</li> <li>Many programmers are terrified of Scala because of its reputation as a super-complex language. They don't know that Spark code can be written with basic Scala language features that you can learn in a day. You don't need to \"learn Scala\" or \"learn functional programming\" to write Spark code with Scala. You can stick to basic language features like <code>if</code>, <code>class</code>, and <code>object</code>, write code that looks exactly like Python, and enjoy the benefits of the Scala ecosystem.</li> <li>Scala is a compile-time, type-safe language, so it offers certain features that cannot be offered in PySpark, like Datasets. Compile time checks give an awesome developer experience when working with an IDE like IntelliJ.</li> <li>A lot of the Scala advantages don't matter in the Databricks notebook environment. Notebooks don't support features offered by IDEs or production grade code packagers, so if you're going to strictly work with notebooks, don't expect to benefit from Scala's advantages.</li> <li>Python has great libraries, but most are not performant / unusable when run on a Spark cluster, so Python's \"great library ecosystem\" argument doesn't apply to PySpark (unless you're talking about libraries that you know are performant when run on clusters).</li> </ul> <p>Let's dig into the details and look at code to make the comparison more concrete.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#chaining-dataframe-transformations","title":"Chaining DataFrame transformations","text":"<p>This section demonstrates how the <code>transform</code> method can elegantly invoke Scala functions (because functions can take two parameter lists) and isn't quite as easy with Python.</p> <p>Custom transformations are a great way to package Spark code. They're easily reusable and can be composed for different analyses. They're also easily testable as standalone units.</p> <p>Suppose you have the following DataFrame.</p> <pre><code>+--------+\n| country|\n+--------+\n|  brasil|\n|colombia|\n|    null|\n+--------+\n</code></pre> <p>Here's a Scala function that'll append some text to the <code>country</code> column:</p> <pre><code>def funify(colName: String, str: String)(df: DataFrame): DataFrame = {\n  df.withColumn(\"funified\", concat(col(colName), lit(\" \"), lit(str)))\n}\n</code></pre> <p>Here's how to invoke the Scala function with the <code>Dataset#transform</code> method:</p> <pre><code>df\n  .transform(funify(\"country\", \"is awesome\"))\n  .show()\n</code></pre> <pre><code>+--------+-------------------+\n| country|           funified|\n+--------+-------------------+\n|  brasil|  brasil is awesome|\n|colombia|colombia is awesome|\n|    null|               null|\n+--------+-------------------+\n</code></pre> <p>Notice how the <code>funify</code> function is defined with two parameter lists and invoked with one set of arguments. The Scala programming language allows for this elegant syntax.</p> <p>Here's an equivalent PySpark function that'll append to the <code>country</code> column:</p> <pre><code>from pyspark.sql.functions import col, lit, concat\n\ndef funify(col_name, str):\n    def _(df):\n        return df.withColumn(\"funified\", concat(col(col_name), lit(\" \"), lit(str)))\n    return _\n</code></pre> <p>Here's how to invoke the Python function with <code>DataFrame#transform</code>:</p> <pre><code>df.transform(funify(\"country\", \"is super fun!\")).show(truncate=False)\n</code></pre> <pre><code>+--------+----------------------+\n|country |funified              |\n+--------+----------------------+\n|colombia|colombia is super fun!|\n|brasil  |brasil is super fun!  |\n|null    |null                  |\n+--------+----------------------+\n</code></pre> <p>There are a lot of different ways to define custom PySpark transformations, but nested functions seem to be the most popular.</p> <p>Nested functions aren't the best. They create an extra level of indentation and require two return statements, which are easy to forget. The comparative difficulty of chaining PySpark custom transformations is a downside.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#datasets-vs-dataframes","title":"Datasets vs DataFrames","text":"<p>Datasets can only be implemented in languages that are compile-time type-safe. Java and Scala are compile-time type-safe, so they support Datasets, but Python and R are not compile-time type-safe, so they only support DataFrames.</p> <p>Datasets so useful in practice.</p> <ul> <li>It's easy to perform operations that'll convert a Dataset to a DataFrame (e.g. adding a column).</li> <li>Datasets are less performant although the performance gap is supposedly narrowing</li> <li>Expressively typed Datasets sound promising, but aren't practical yet</li> </ul> <p>Datasets shouldn't be considered to be a huge advantage because most Scala programmers use DataFrames anyways.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#feature-parity","title":"Feature parity","text":"<p>PySpark generally supports all the features in Scala Spark, with a few exceptions.</p> <p>The <code>CalendarIntervalType</code> has been in the Scala API since Spark 1.5, but still isn't in the PySpark API as of Spark 3.0.1. This is a \"serious loss of function\" and will hopefully get added.</p> <p>In general, both the Python and Scala APIs support the same functionality. Spark knows that a lot of users avoid Scala/Java like the plague and they need to provide excellent Python support.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#performance-comparison","title":"Performance comparison","text":"<p>Scala and PySpark should perform relatively equally for DataFrame operations.</p> <p>This thread has a dated performance comparison.</p> <p>\"Regular\" Scala code can run 10-20x faster than \"regular\" Python code, but that PySpark isn't executed liked like regular Python code, so this performance comparison isn't relevant.</p> <p>PySpark is converted to Spark SQL and then executed on a JVM cluster. It's not a traditional Python execution environment. Benchmarks for other Python execution environments are irrelevant for PySpark.</p> <p>Watch out! Some other blog posts imply that PySpark is 10x slower which just isn't true.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#code-editing","title":"Code editing","text":"<p>The IntelliJ community edition provides a powerful Scala integrated development environment with out of the box. If provides you with code navigation, type hints, function completion, and compile-time runtime error reporting.</p> <p>There's also a Metals project that allows for IDE-like text editor features in Vim or VSCode. Use IntelliJ if you'd like a full-serviced solution that works out of the box. Metals is good for those who enjoy text editor tinkering and custom setups.</p> <p>Scala provides excellent text editors for working with Spark.</p> <p>Some folks develop Scala code without the help of either Metals or IntelliJ, which puts you at a disadvantage. Scala IDEs give you a lot of help for free. Scala devs that reject free help from their text editor will suffer unnecessarily.</p> <p>PyCharm doesn't work out of the box with PySpark, you need to configure it. pyspark-stubs provide some nice error messages and autocompletion, but nothing compared to what's offered by Scala/IntelliJ.</p> <p>Scala has the edge for the code editor battle.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#compile-time-exceptions","title":"Compile time exceptions","text":"<p>Here's what IntelliJ will show when you try to invoke a Spark method without enough arguments.</p> <p></p> <p>The PyCharm error only shows up when pyspark-stubs is included and is more subtle.</p> <p></p> <p>At least you can hover over the method and get a descriptive hint.</p> <p></p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#code-navigation","title":"Code navigation","text":"<p>IntelliJ/Scala let you easily navigate from your code directly to the relevant parts of the underlying Spark code. Suppose your cursor is on the <code>regexp_extract</code> function. You can pretty Command + b to go directly to <code>org.apache.spark.sql.functions.regexp_extract</code> and then continue pressing Command + b to see exactly how the function is working under the hood.</p> <p>PySpark code navigation is severely lacking in comparison. You can navigate to functions within your codebase, but you'll be directed to the stub file if you try to jump to the underlying PySpark implementations of core functions.</p> <p>PySpark code navigation can't be as good due to Python language limitations.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#packaging-jars-vs-wheels","title":"Packaging JARs vs Wheels","text":"<p>Scala projects can be packaged as JAR files and uploaded to Spark execution environments like Databricks or EMR where the functions are invoked in production. JAR files can be assembled without dependencies (thin JAR files) or with dependencies (fat JAR files). Scala makes it easy to customize your fat JAR files to exclude the test dependencies, exclude Spark (because that's already included by your runtime), and contain other project dependencies. See this blog for more on building JAR files.</p> <p>You can even overwrite the packages for the dependencies in fat JAR files to avoid namespace conflicts by leveraging a process called shading.</p> <p>Suppose <code>com.your.org.projectXYZ</code> depends on <code>com.your.org.projectABC</code> and you'd like to attach <code>projectXYZ</code> to a cluster as a fat JAR file. You'd like <code>projectXYZ</code> to use version 1 of <code>projectABC</code>, but would also like to attach version 2 of <code>projectABC</code> separately. When <code>projectXYZ</code> calls <code>com.your.org.projectABC.someFunction</code>, it should use version 1. All other invocations of <code>com.your.org.projectABC.someFunction</code> should use version 2.</p> <p>You can shade <code>projectABC</code> in the <code>projectXYZ</code> fat JAR file, so the path is something like <code>projectAbcShaded.projectABC</code>, to prevent namespace conflicts for when <code>projectABC</code> version 2 is attached to the cluster. See here for more details on shading.</p> <p>Shading is a great technique to avoid dependency conflicts and dependency hell.</p> <p>Python doesn't support building fat wheel files or shading dependencies. This blog post explains some of the new ways to manage dependencies with Python and this repo shows how PySpark developers have managed dependencies historically. The PySpark solutions aren't as clean as fat JAR files, but are robust and improving nonetheless.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#scala-dependency-hell","title":"Scala dependency hell","text":"<p>Scala minor versions aren't binary compatible, so maintaining Scala projects is a lot of work. Scala 2.11 projects need to depend on projects that were also compiled with Scala 2.11.</p> <p>It was even a lot of work for the Spark creators, Scala programming experts, to upgrade the Spark codebase from Scala 2.11 to 2.12.</p> <p>Scala codebase maintainers need to track the continuously evolving Scala requirements of Spark:</p> <ul> <li>Spark 2.3 apps needed to be compiled with Scala 2.11.</li> <li>Spark 2.4 apps could be cross compiled with both Scala 2.11 and Scala 2.12.</li> <li>Spark 3 apps only support Scala 2.12.</li> </ul> <p>Suppose you add a dependency to your project in Spark 2.3, like spark-google-spreadsheets. The maintainer of this project stopped maintaining it and there are no Scala 2.12 JAR files in Maven. The spark-google-spreadsheets dependency would prevent you from cross compiling with Spark 2.4 and prevent you from upgrading to Spark 3 entirely.</p> <p>You'd either need to upgrade spark-google-spreadsheets to Scala 2.12 and publish a package yourself or drop the dependency from your project to upgrade.</p> <p>PySpark developers don't have the same dependency hell issues. A wheel file that's compiled with Python 3.6 will work on a Python 3.7 cluster. A wheel file that's compiled with Spark 2 will likely work on a Spark 3 cluster.</p> <p>Scala should thoroughly vet dependencies and the associated transitive dependencies whenever evaluating a new library for their projects. Minimizing dependencies is the best way to sidestep dependency hell.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#spark-native-functions","title":"Spark native functions","text":"<p>The <code>org.apache.spark.sql.functions</code> are examples of Spark native functions. They're implemented in a manner that allows them to be optimized by Spark before they're executed.</p> <p>The <code>pyspark.sql.functions</code> are mere wrappers that call the Scala functions under the hood.</p> <p>Spark native functions need to be written in Scala.</p> <p>Check out the itachi repo for an example of a repo that contains a bunch of Spark native functions.</p> <p>You need to write Scala code if you'd like to write your own Spark native functions. The Spark maintainers are hesitant to expose the regexp_extract_all functions to the Scala API, so I implemented it in the bebe project.</p> <p>You'll need to use Scala if you'd like to do this type of hacking. 75% of the Spark codebase is Scala code:</p> <p></p> <p>Most folks aren't interested in low level Spark programming. This advantage only counts for folks interested in digging in the weeds.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#libraries","title":"Libraries","text":"<p>Python has a great data science library ecosystem, some of which cannot be run on Spark clusters, others that are easy to horizontally scale.</p> <ul> <li>scikit-learn is an example of a lib that's not easily runnable on Spark</li> <li>jellyfish can be easily wrapped in UDFs and run on Spark in a performant manner, see the ceja project</li> </ul> <p>Just make sure that the Python libraries you love are actually runnable on PySpark when you're assessing the Python library ecosystem.</p> <p>There is also a well-supported Koalas project for folks that would like to write Spark code with Pandas syntax.</p> <p>If you absolutely need a particular library, you can assess the support for both the Scala and PySpark APIs to aid your decision. For example, if you need Tensorflow at scale, you can compare TensorFlowOnSpark and tensorflow_scala to aid your decision.</p> <p>A lot of the popular Spark projects that were formerly Scala-only now offer Python APIs (e.g. spark-nlp and python-deequ).</p> <p>Neither library ecosystem is clearly favored. It depends on your specific needs.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#udfs","title":"UDFs","text":"<p>UDFs should be avoided whenever possible, with either language API, because they're a black box for the compiler and can't be optimized.</p> <p>UDFs are also a frequent cause of NullPointerExceptions. Make sure you always test the null input case when writing a UDF.</p> <p>You should always try to solve your problem with the functions exposed in <code>org.apache.spark.sql.functions</code> or <code>pyspark.sql.functions</code> before falling back to UDFs.</p> <p>Both Python and Scala allow for UDFs when the Spark native functions aren't sufficient.</p> <p>Pandas UDFs (aka vectorized UDFs) are marketed as a cool feature, but they're really an anti-pattern that should be avoided, so don't consider them a PySpark plus.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#topandas","title":"toPandas","text":"<p>PySpark DataFrames can be converted to Pandas DataFrames with <code>toPandas</code>. This collects all the data on the driver node and negates all the parallelism benefits of regular PySpark DataFrames.</p> <p>Spark DataFrames are spread across a cluster and computations run in parallel - that's why Spark is so fast - it's a cluster computing framework.</p> <p>You throw all the benefits of cluster computing out the window when converting a Spark DataFrame to a Pandas DataFrame. All the data is transferred to the driver node. Subsequent operations run on the Pandas DataFrame will only use the computational power of the driver node.</p> <p>The driver node usually isn't big enough for all the data, so calling <code>toPandas</code> often results in an out of memory exception.</p> <p><code>toPandas</code> might be useful at times, but it probably causes more harm than good. Newbies try to convert their Spark DataFrames to Pandas so they can work with a familiar API and don't realize that it'll crash their job or make it run a lot slower. Use koalas if you'd like to write Spark code with Pandas syntax.</p> <p><code>toPandas</code> is the fastest way to convert a DataFrame column to a list, but that's another example of an antipattern that commonly results in an OutOfMemory exception.</p> <p><code>toPandas</code> shouldn't be considered a PySpark advantage. Write out a Parquet file and read it in to a Pandas DataFrame using a different computation box if that's your desired workflow. You don't need a heavyweight Spark JVM cluster to work with Pandas.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#whitespace-sensitivity","title":"Whitespace sensitivity","text":"<p>Python's whitespace sensitivity causes ugly PySpark code when backslash continuation is used.</p> <p>Here's an example from the python-deequ README:</p> <pre><code>checkResult = VerificationSuite(spark) \\\n    .onData(df) \\\n    .addCheck(\n        check.hasSize(lambda x: x &gt;= 3) \\\n        .hasMin(\"b\", lambda x: x == 0) \\\n        .isComplete(\"c\")  \\\n        .isUnique(\"a\")  \\\n        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n        .isNonNegative(\"b\")) \\\n    .run()\n</code></pre> <p>Backslash continuation is frowned upon in the Python community, but you'll still see it in the wild.</p> <p>The equivalent Scala code looks nicer without all the backslashes:</p> <pre><code>val verificationResult = VerificationSuite()\n  .onData(data)\n  .addCheck(\n    Check(CheckLevel.Error, \"unit testing my data\")\n      .hasSize(_ == 5)\n      .isComplete(\"id\")\n      .isUnique(\"id\")\n      .isComplete(\"productName\")\n      .isContainedIn(\"priority\", Array(\"high\", \"low\"))\n      .isNonNegative(\"numViews\")\n    .run()\n</code></pre> <p>You can avoid the Python backslashes by wrapping the code block in parens:</p> <pre><code>spark = (SparkSession\n    .builder\n    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n    .getOrCreate())\n</code></pre> <p>Spark encourages a long method change style of programming so Python whitespace sensitivity is annoying.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#type-safe-programming","title":"Type safe programming","text":"<p>Scala is a compile-time, type-safe language and offers type safety benefits that are useful in the big data space.</p> <p>Suppose your project has a small bug and contains a method that takes three parameters, but is only invoked with two arguments. Scala will throw a compile-time error and not allow you to build the JAR file to make a production deploy.</p> <p>Small bugs can be really annoying in big data apps. Your job might run for 5 hours before your small bug crops up and ruins the entire job run.</p> <p>Python doesn't have any similar compile-time type checks. Python will happily build a wheel file for you, even if there is a three parameter method that's run with two arguments.</p> <p>There are different ways to write Scala that provide more or less type safety. Spark is on the less type safe side of the type safety spectrum.</p> <ul> <li>Column objects are basically untyped</li> <li>Datasets aren't that practical to use</li> <li>Type casting is a core design practice to make Spark work</li> </ul> <p>Type safety has the potential to be a huge advantage of the Scala API, but it's not quite there at the moment. I'm working on a project called bebe that'll hopefully provide the community with a performant, type safe Scala programming interface.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#notebook-programming","title":"Notebook programming","text":"<p>Some of the costs / benefits we've discussed thus far don't carry over to the notebook environment.</p> <p>For example, Scala allows for compile time checks and IDEs will highlight invalid code. Databricks notebooks don't support this feature. This particular Scala advantage over PySpark doesn't matter if you're only writing code in Databricks notebooks.</p> <p>Databricks notebooks are good for exploratory data analyses, but shouldn't be overused for production jobs.</p> <p>The code for production jobs should live in version controlled GitHub repos, which are packaged as wheels / JARs and attached to clusters. Databricks notebooks should provide a thin wrapper around the package that invokes the relevant functions for the job.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#delta-engine","title":"Delta Engine","text":"<p>Databricks is developing a proprietary Spark runtime called Delta Engine that's written in C++.</p> <p>Delta Engine will provide Scala &amp; Python APIs. One of the main Scala advantages at the moment is that it's the language of Spark. This advantage will be negated if Delta Engine becomes the most popular Spark runtime.</p> <p>The Delta Engine source code is private. That'll make navigating to internals and seeing how things work under the hood impossible, in any language. That'll also make it impossible for other players to release Delta Engine based runtimes.</p> <p>The existence of Delta Engine makes the future of Spark unclear.</p> <p>Delta Lake, another Databricks product, started private and eventually succumbed to pressure and became free &amp; open source. It's possible Delta Engine will become open source and the future of hardcore Spark hacking will be C++.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#publishing-open-source-projects","title":"Publishing open source projects","text":"<p>Publishing open source Scala projects to Maven is a pain.</p> <ul> <li>You need to open a JIRA ticket to create your Maven namespace (not kidding)</li> <li>Wait for a couple of days for them to respond to the JIRA ticket</li> <li>You need to create a GPG key and upload the public key to a keyserver</li> <li>Actually publishing requires a separate SBT plugin (SBT plugin maintenance / version hell is a thing too!)</li> </ul> <p>Every time you run the publish command, you need to remember the password for your GPG key.</p> <p>Publishing open source Python projects to PyPi is much easier. You run the publishing command, enter your username / password, and the wheel is uploaded, pretty much instantaneously. The Poetry dependency management tool provides the <code>poetry publish</code> tool out of the box, so you don't need to deal with the Python equivalent of a SBT plugin.</p> <p>Python open source publishing is a joy compared to Scala.</p>"},{"location":"apache-spark/python-pyspark-scala-which-better/#conclusion","title":"Conclusion","text":"<p>Spark is an awesome framework and the Scala and Python APIs are both great for most workflows.</p> <p>PySpark is more popular because Python is the most popular language in the data community.</p> <p>PySpark is a well supported, first class Spark API, and is a great choice for most organizations.</p> <p>Scala is a powerful programming language that offers developer friendly features that aren't available in Python. You can use basic Scala programming features with the IntelliJ IDE and get useful features like type hints and compile time checks for free.</p> <p>Scala offers a lot of advance programming features, but you don't need to use any of them when writing Spark code. Complex Spark data processing frameworks can be built with basic Scala language features like <code>object</code>, <code>if</code>, and functions.</p> <p>Scala is also great for lower level Spark programming and easy navigation directly to the underlying source code.</p> <p>Scala gets a lot of hate and many developers are terrified to even try working with the language. Their aversion of the language is partially justified. Scala allows certain developers to get out of line and write code that's really hard to read. Sidenote: Spark codebase is a great example of well written Scala that's easy to follow.</p> <p>The best language for your organization will depend on your particular team. Platforms like Databricks make it easy to write jobs in both languages, but that's not a realistic choice for most companies. Once core libraries are developed in one language, then all subsequent jobs are forced to use the chosen language to avoid rework.</p> <p>Choosing the right language API is important. Think and experiment extensively before making the final decision!</p>"},{"location":"apache-spark/read-google-sheets-dataframe/","title":"Reading data from Google Sheets to Spark DataFrames","text":"<p>This blog post explains how to read a Google Sheet into a Spark DataFrame with the spark-google-spreadsheets library.</p> <p>Google Sheets is not a good place to store a lot of data, but is fine for small datasets. Google Sheets are easy for non-technical users to understand and modify.</p>"},{"location":"apache-spark/read-google-sheets-dataframe/#create-a-google-spreadsheet","title":"Create a Google Spreadsheet","text":"<p>Create a Google Spreadsheet with a few columns and rows of data.</p> <p></p> <p>You'll of course need a Google account to access Sheets.</p>"},{"location":"apache-spark/read-google-sheets-dataframe/#setup-credentials","title":"Setup credentials","text":"<p>Setting up the credentials is the hardest part.</p> <p>Luckily this guide provides a detailed description of the process.</p> <p>You'll need to create a service account key and then share your sheet with the email address in the service account ID field.</p> <p>As part of the process, you'll create a credentials file that you can store on your machine, perhaps in the <code>~/gcloud_credentials/</code> directory. Make sure to never check in credentials to source control.</p>"},{"location":"apache-spark/read-google-sheets-dataframe/#import-the-library-and-read-the-sheet","title":"Import the library and read the sheet","text":"<p>Add this line to your <code>build.sbt</code> file to import the library:</p> <pre><code>libraryDependencies += \"com.github.potix2\" %% \"spark-google-spreadsheets\" % \"0.6.3\"\n</code></pre> <p>Now you'll need to inspect the URL of your Google Sheet to decipher its unique identifier. Suppose the URL is <code>https://docs.google.com/spreadsheets/d/1d6aasdfqwergfds0P1bvmhTRasMbobegRE6Zap-Tkl3k/edit#gid=0</code>.</p> <p>The unique sheet identifier is <code>1d6aasdfqwergfds0P1bvmhTRasMbobegRE6Zap-Tkl3k</code> for this sheet.</p> <p>Further suppose that the tab name is <code>people_data</code>.</p> <p></p> <p>Here's how to read the sheet into a DataFrame:</p> <pre><code>val df = spark.sqlContext.read\n  .format(\"com.github.potix2.spark.google.spreadsheets\")\n  .load(\"1d6aasdfqwergfds0P1bvmhTRasMbobegRE6Zap-Tkl3k/people_data\")\n</code></pre> <p>Run <code>df.show()</code> to view the contents of the DataFrame:</p> <pre><code>+----------+---+\n|first_name|age|\n+----------+---+\n|       bob| 45|\n|      lucy| 84|\n+----------+---+\n</code></pre> <p>Run <code>df.printSchema()</code> to print the schema of the DataFrame:</p> <pre><code>root\n |-- first_name: string (nullable = true)\n |-- age: string (nullable = true)\n</code></pre> <p>The <code>age</code> column was not inferred to be an <code>IntegerType</code> column. Doesn't appear that this library supports schema inference.</p> <p><code>df.rdd.partitions.length</code> returns one, which means that the entire sheet was read into a single memory partition. Spark normally reads big datasets in parallel into multiple partitions. The Google Sheet data is only read into a single partition because this library isn't meant to read big datasets. It's meant to read tiny datasets.</p>"},{"location":"apache-spark/read-google-sheets-dataframe/#scala-212-spark-3","title":"Scala 2.12 / Spark 3","text":"<p>There's an open issue for cross compiling this library with Scala 2.12.</p> <p>If the issue doesn't get worked on and you'd like a JAR file that'll work with Scala 2.12 and Spark 3, email me and I'll consider forking the repo and making a release.</p>"},{"location":"apache-spark/read-google-sheets-dataframe/#next-steps","title":"Next steps","text":"<p>spark-google-spreadsheets makes it easy to read data from Google Sheets into Spark DataFrames.</p> <p>Google Sheets can be great for small single source of truth data files that are frequently updated by non-technical users.</p> <p>Users can make arbitrary changes to Google Sheets, so make sure to validate input before processing it in your Spark application. The spark-daria validators will help.</p> <p>You can also write data from Spark to Google Sheets, but that's probably a less common workflow for most organizations.</p>"},{"location":"apache-spark/registerfunction-injectfunction/","title":"Registering Native Spark Functions","text":"<p>This post explains how Spark registers native functions internally and the public facing APIs for you to register your own functions.</p> <p>Registering native functions is important if you want to access functions via the SQL API. You don't need to register functions if you're using the PySpark or Scala DSLs.</p> <p>This post is organized as follows:</p> <ul> <li>registering functions to an existing SparkSession</li> <li>registering functions via SparkSessionExt in Databricks</li> <li>command line workflow</li> <li>how Spark core registers functions</li> </ul> <p>This is a low level post for advanced Spark users and talks about code written by Spark core maintainers. You'll need to grok it hard. Studying this code is highly recommended if you really want to understand how Spark works under the hood.</p>"},{"location":"apache-spark/registerfunction-injectfunction/#registering-functions-to-existing-sparksession","title":"Registering functions to existing SparkSession","text":"<p>The itachi project provides access to Postgres / Presto SQL syntax in Spark. Let's look at how to use itachi and then dig into the implementation details.</p> <p>Suppose you have the following DataFrame:</p> <pre><code>+------+------+\n|  arr1|  arr2|\n+------+------+\n|[1, 2]|    []|\n|[1, 2]|[1, 3]|\n+------+------+\n</code></pre> <p>Concatenate the two arrays with the <code>array_cat</code> function that's defined in itachi:</p> <pre><code>yaooqinn.itachi.registerPostgresFunctions\n\nspark\n  .sql(\"select array_cat(arr1, arr2) as both_arrays from some_data\")\n  .show()\n</code></pre> <pre><code>+------------+\n| both_arrays|\n+------------+\n|      [1, 2]|\n|[1, 2, 1, 3]|\n+------------+\n</code></pre> <p><code>array_cat</code> is a Postgres function and itachi provides syntax that Postgres developers are familiar with. This makes for an easy transition to Spark.</p> <p>itachi also defines an <code>age</code> function similar to Postgres.</p> <p>Here's the <code>Age</code> Catalyst code:</p> <pre><code>case class Age(end: Expression, start: Expression)\n  extends BinaryExpression with ImplicitCastInputTypes {\n  override def left: Expression = end\n\n  override def right: Expression = start\n\n  override def inputTypes: Seq[AbstractDataType] = Seq(TimestampType, TimestampType)\n\n  override def nullSafeEval(e: Any, s: Any): Any = {\n    DateTimeUtils.age(e.asInstanceOf[Long], s.asInstanceOf[Long])\n  }\n\n  override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {\n    val dtu = DateTimeUtils.getClass.getCanonicalName.stripSuffix(\"$\")\n    defineCodeGen(ctx, ev, (ed, st) =&gt; s\"$dtu.age($ed, $st)\")\n  }\n\n  override def dataType: DataType = CalendarIntervalType\n}\n</code></pre> <p>We need to take this Catalyst code and package it properly for <code>registerFunction</code> in Spark.</p> <p><code>registerFunction</code> takes three arguments:</p> <ul> <li>name: FunctionIdentifier</li> <li>info: ExpressionInfo</li> <li>builder: FunctionBuilder</li> </ul> <p>itachi defines a package object that helps organize the required arguments:</p> <pre><code>package object extra {\n\n  type FunctionDescription = (FunctionIdentifier, ExpressionInfo, FunctionBuilder)\n\n  type Extensions = SparkSessionExtensions =&gt; Unit\n\n}\n</code></pre> <p>Let's define the <code>FunctionDescription</code> for <code>Age</code>:</p> <pre><code>object Age {\n  val fd: FunctionDescription = (\n    new FunctionIdentifier(\"age\"),\n    ExpressionUtils.getExpressionInfo(classOf[Age], \"age\"),\n    (children: Seq[Expression]) =&gt; if ( children.size == 1) {\n      Age(CurrentDate(), children.head)\n    } else {\n      Age(children.head, children.last)\n    })\n}\n</code></pre> <p>Now register the function with the current SparkSession:</p> <pre><code>spark.sessionState.functionRegistry.registerFunction(Age.fd._1, Age.fd._2, Age.fd._3)\n</code></pre> <p>Library users don't need to concern themselves with any low level function registration of course. They can just attach itachi to their cluster and run the following line to access the Postgres-style SQL functions:</p> <pre><code>yaooqinn.itachi.registerPostgresFunctions\n</code></pre> <p>Big thanks to Alex Ott for showing me this solution!</p>"},{"location":"apache-spark/registerfunction-injectfunction/#registering-functions-via-sparksessionext-in-databricks","title":"registering functions via SparkSessionExt in Databricks","text":"<p>There is a SparkSessionExt#injectFunction method that also allows you to register functions. This approach isn't as user friendly because it can't register functions with an existing SparkSession. It can only register functions to a new SparkSession. Let's look at the workflow and the usability issues will be apparent.</p> <p>Create an init script in DBFS:</p> <pre><code>dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n\ndbutils.fs.put(\"/databricks/scripts/itachi-install.sh\",\"\"\"\n#!/bin/bash\nwget --quiet -O /mnt/driver-daemon/jars/itachi_2.12-0.1.0.jar https://repo1.maven.org/maven2/com/github/yaooqinn/itachi_2.12/0.1.0/itachi_2.12-0.1.0.jar\"\"\", true)\n</code></pre> <p>Before starting the cluster, set the Spark config:</p> <pre><code>spark.sql.extensions org.apache.spark.sql.extra.PostgreSQLExtensions\n</code></pre> <p>Also set the DBFS file path to the init script before starting the cluster:</p> <pre><code>dbfs:/databricks/scripts/itachi-install.sh\n</code></pre> <p>You can now attach a notebook to the cluster using Postgres SQL syntax.</p> <p>This workflow is nice in some circumstances because all notebooks attached to the cluster can access the Postgres syntax without doing any imports. In general, it's much easier to register functions to an existing SparkSession, like we did with the first approach.</p> <p>All you need to write to support the spark.sql.extensions approach is a class that extends <code>Extensions</code>:</p> <pre><code>class PostgreSQLExtensions extends Extensions {\n  override def apply(ext: SparkSessionExtensions): Unit = {\n    ext.injectFunction(Age.fd)\n  }\n}\n</code></pre> <p><code>Age.fd</code> returns the tuple that <code>injectFunction</code> needs.</p>"},{"location":"apache-spark/registerfunction-injectfunction/#command-line-workflow","title":"command line workflow","text":"<p>It's work noting that the <code>SparkSessionExt</code> approach isn't as bulky on the command line (compared to Databricks);</p> <pre><code>bin/spark-sql --packages com.github.yaooqinn:itachi_2.12:0.1.0 --conf spark.sql.extensions=org.apache.spark.sql.extra.PostgreSQLExtensions\n</code></pre> <p><code>SparkSessionExt</code> may be easier for runtimes other than Databricks that don't require init scripts.</p>"},{"location":"apache-spark/registerfunction-injectfunction/#how-spark-registers-native-functions","title":"How Spark registers native functions","text":"<p>Here's a snippet of the Spark <code>FunctionRegistry</code> code:</p> <pre><code>object FunctionRegistry {\n\n  type FunctionBuilder = Seq[Expression] =&gt; Expression\n\n  val expressions: Map[String, (ExpressionInfo, FunctionBuilder)] = Map(\n    expression[Abs](\"abs\"),\n    expression[Coalesce](\"coalesce\"),\n    expression[Explode](\"explode\"),\n    // all the other Spark SQL functions\n  )\n\n  private def expression[T &lt;: Expression : ClassTag](name: String, setAlias: Boolean = false)\n      : (String, (ExpressionInfo, FunctionBuilder)) = {\n    val (expressionInfo, builder) = FunctionRegistryBase.build[T](name)\n    val newBuilder = (expressions: Seq[Expression]) =&gt; {\n      val expr = builder(expressions)\n      if (setAlias) expr.setTagValue(FUNC_ALIAS, name)\n      expr\n    }\n    (name, (expressionInfo, newBuilder))\n  }\n\n  // bunch of other stuff\n\n}\n</code></pre> <p>The <code>expression</code> method is passed an expression class name and a function name and returns a <code>String</code>, <code>ExpressionInfo</code>, and <code>FunctionBuilder</code>.</p> <p>The code eventually loops over the <code>expressions</code> map and registers each function.</p> <pre><code>expressions.foreach {\n  case (name, (info, builder)) =&gt; fr.registerFunction(FunctionIdentifier(name), info, builder)\n}\n</code></pre> <p>We've made a full loop back to <code>registerFunction</code> in the original approach ;)</p>"},{"location":"apache-spark/registerfunction-injectfunction/#thanks","title":"Thanks","text":"<p>Big shout out to cloud-fan for helping with my basic questions. Thanks to yaooqinn for making itachi and Sim for also talking about registering native functions.</p> <p>The Spark open source community is a pleasure to work with.</p>"},{"location":"apache-spark/registerfunction-injectfunction/#conclusion","title":"Conclusion","text":"<p>Spark doesn't make it easy to register native functions, but this isn't a common task, so the existing interfaces are fine.</p> <p>Registering Spark native functions is only for advanced Spark programmers that are comfortable writing Catalyst expressions and want to expose functionality via the SQL API (rather than the Python / Scala DSLs).</p> <p>The <code>SparkSessionExt#injectFunction</code> approach is problematic because it's difficult to use and init scripts are fragile.</p> <p><code>spark.sessionState.functionRegistry.registerFunction</code> is a better approach because it gives end users a smoother interface.</p> <p>itachi shows how this design pattern can provide powerful functionality to end users.</p> <p>A company with Spark experts and SQL power users could also benefit from this design pattern. Spark experts can register native SQL functions with for custom business use cases and hypercharge the productivity of the SQL analysts.</p>"},{"location":"apache-spark/scala-packages-imports/","title":"Designing Scala Packages and Imports for Readable Spark Code","text":"<p>This blog post explains how to import core Spark and Scala libraries like spark-daria into your projects.</p> <p>It's important for library developers to organize package namespaces so it's easy for users to import their code.</p> <p>Library users should import code so it's easy for teammates to identify the source of functions when they're invoked.</p> <p>I wrote a book called Beautiful Spark that teaches you the easiest way to build Spark applications and manage complex logic. The book will teach you the most important aspects of Spark development and will supercharge your career.</p> <p>Let's start with a simple example that illustrates why wilcard imports can generate code that's hard to follow.</p>"},{"location":"apache-spark/scala-packages-imports/#simple-example","title":"Simple example","text":"<p>Let's look at a little code snippet that uses the Spark <code>col()</code> function and the spark-daria <code>removeAllWhitespace()</code> function.</p> <pre><code>import org.apache.spark.sql.functions._\nimport com.github.mrpowers.spark.daria.sql._\n\ndf.withColumn(\"clean_text\", removeAllWhitespace(col(\"text\")))\n</code></pre> <p>Wildcard imports (imports with underscores) create code that's difficult to follow. It's hard to tell where the <code>removeAllWhitespace()</code> function is defined.</p>"},{"location":"apache-spark/scala-packages-imports/#curly-brace-import","title":"Curly brace import","text":"<p>We can use the curly brace import style to make it easy for other programmers to search the codebase and find where <code>removeAllWhitespace()</code> is defined.</p> <pre><code>import org.apache.spark.sql.functions._\nimport com.github.mrpowers.spark.daria.sql.{removeAllWhitespace}\n</code></pre> <p>Per the Databricks Scala style guide:</p> <p>Avoid using wildcard imports, unless you are importing more than 6 entities, or implicit methods. Wildcard imports make the code less robust to external changes.</p> <p>In other words, use curly brace imports unless you're going to use more than 6 methods from the object that's being imported.</p> <p>That's not the best advice because <code>import com.github.mrpowers.spark.daria.sql._</code> will still leave users confused about where <code>removeAllWhitespace()</code> is defined, regardless of how extensively the spark-daria functions are used.</p>"},{"location":"apache-spark/scala-packages-imports/#named-import","title":"Named import","text":"<p>We can name imports so all function invocations make it clear where the functions are defined.</p> <pre><code>import org.apache.spark.sql.functions._\nimport com.github.mrpowers.spark.daria.sql.{functions =&gt; dariaFunctions}\n\ndf.withColumn(\"clean_text\", dariaFunctions.removeAllWhitespace(col(\"text\")))\n</code></pre> <p>This allows users to search for <code>dariaFunctions</code> and figure out that <code>removeAllWhitespace</code> is defined in spark-daria.</p> <p>This approach is potentially confusing if the same import isn't consistently named throughout the codebase. If some developers import the daria functions as <code>darF</code> and other developers import them as <code>dariaFunctions</code>, it could get confusing.</p> <p>Named imports help here because the package name is so ridiculously long. If the package name was shorter, we could do <code>import mrpowers.daria</code> and invoke the function with <code>daria.functions.removeAllWhitespace()</code>. This gives us the best of both worlds - easy imports and consistent function invocation.</p>"},{"location":"apache-spark/scala-packages-imports/#complicated-package-names-are-the-root-issue","title":"Complicated package names are the root issue","text":"<p>Scala package names typically follow the verbose Java conventions. Instead of <code>spark.sql.functions</code> we have <code>org.apache.spark.sql.functions</code>.</p> <p>Most Spark libraries follow the same trend. We have <code>com.github.mrpowers.spark.daria.sql.functions</code> instead of <code>mrpowers.daria.functions</code>.</p> <p>Some of the great libraries created by Li Haoyi allow for short imports:</p> <ul> <li><code>import fastparse._</code></li> <li><code>import utest._</code></li> </ul> <p>It's arguable that Li Haoyi's import statments are too short because they could cause some namespace collisions (with another library named <code>fastparse</code> for example).</p> <p>These imports would strike a good balance of being short and having a low probability of name collisions.</p> <ul> <li><code>import lihaoyi.fastparse._</code></li> <li><code>import lihaoyi.utest._</code></li> </ul> <p>The internet has mixed feelings on if the import statements should be short or if they should follow the verbose Java style.</p>"},{"location":"apache-spark/scala-packages-imports/#wildcard-imports-are-ok-for-spark-core-classes","title":"Wildcard Imports are OK for Spark core classes","text":"<p>Wildcard imports should be avoided in general, but they're OK for core Spark classes.</p> <p>The following code is completely fine, even though you're importing a ton of functions to the global namespace.</p> <pre><code>import org.apache.spark.sql.functions._\n</code></pre> <p>Spark programmers are familiar with the Spark core functions and will know that functions like <code>col()</code> and <code>lit()</code> are defined in Spark.</p>"},{"location":"apache-spark/scala-packages-imports/#implicit-imports","title":"Implicit imports","text":"<p>You have to use the wildcard syntax to import objects that wrap implicit classes. Here's a snippet of code that extends the Spark <code>Column</code> class:</p> <pre><code>package com.github.mrpowers.spark.daria.sql\n\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions._\n\nobject ColumnExt {\n\n  implicit class ColumnMethods(col: Column) {\n    def chain(colMethod: (Column =&gt; Column)): Column = {\n      colMethod(col)\n    }\n  }\n\n}\n</code></pre> <p>You can import the column extensions as follows: <code>import com.github.mrpowers.spark.daria.sql.ColumnExt._</code>.</p> <p>Staying away from implicits is generally best, so feel free to avoid this language feature.</p>"},{"location":"apache-spark/scala-packages-imports/#spark-implicits","title":"Spark implicits","text":"<p>Spark implicits come in handy, expecially in the notebook enviroment. They can be imported with <code>import spark.implicits._</code>.</p> <p>The Spark shell and Databricks notebooks both import implicits automatically after creating the SparkSession. See here for best practices on managing the SparkSession in Scala projects.</p> <p>If you'd like to access Spark implicits other environments (e.g. code in an IntelliJ text editor), you'll need to import them youself.</p> <p>Importing Spark implicits is a little tricky because the SparkSession needs to be instantiated first.</p>"},{"location":"apache-spark/scala-packages-imports/#conclusion","title":"Conclusion","text":"<p>There are a variety of ways to import Spark code.</p> <p>It's hard to maintain code that has lots of wildcard imports. The namespace gets cluttered with functions from a variety of objects and it can be hard to tell which methods belong to which library.</p> <p>I recommend ignoring the Java conventions of having deeply nested packages, so you don't force users to write really long import statements. Also give your projects one word names so import statements are shorter too ;)</p> <p>Library developers are responsible for providing users with a great user experience. The package structure of your project is a key part of your public interface. Choose wisely!</p> <p>Check out Beautiful Spark to learn more Spark best practices.</p>"},{"location":"apache-spark/shading-dependencies-with-sbt/","title":"Shading Dependencies in Spark Projects with SBT","text":"<p><code>sbt-assembly</code> makes it easy to shade dependencies in your Spark projects when you create fat JAR files. This blog post will explain why it's useful to shade dependencies and will teach you how to shade dependencies in your own projects.</p>"},{"location":"apache-spark/shading-dependencies-with-sbt/#when-shading-is-useful","title":"When shading is useful","text":"<p>Let's look at a snippet from the spark-pika <code>build.sbt</code> file and examine the JAR file that's constructed by <code>sbt assembly</code>.</p> <p>You can read this blog post on building JAR files with SBT if you need background information on JAR file basics before diving into shading, which is an advanced feature.</p> <pre><code>libraryDependencies += \"mrpowers\" % \"spark-daria\" % \"2.3.1_0.24.0\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.1\" % \"provided\"\nlibraryDependencies += \"MrPowers\" % \"spark-fast-tests\" % \"2.3.1_0.15.0\" % \"test\"\nlibraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.0.1\" % \"test\"\n\nassemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)\nassemblyJarName in assembly := s\"${name.value}_2.11-${sparkVersion.value}_${version.value}.jar\"\n</code></pre> <p>The <code>sbt assembly</code> command will create a JAR file that includes <code>spark-daria</code> and all of the <code>spark-pika</code> code. The JAR file won't include the <code>libraryDependencies</code> that are flagged with \"provided\" or \"test\" (i.e. <code>spark-sql</code>, <code>spark-fast-tests</code>, and <code>scalatest</code> won't be included in the JAR file). Let's verify the contents of the JAR file with the <code>jar tvf target/scala-2.11/spark-pika_2.11-2.3.1_0.0.1.jar</code> command.</p> <pre><code>0 Sun Sep 23 23:04:00 COT 2018 com/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/daria/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/daria/ml/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/daria/sql/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/daria/sql/types/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/daria/utils/\n0 Sun Sep 23 23:04:00 COT 2018 com/github/mrpowers/spark/pika/\n</code></pre> <p>If the <code>spark-pika</code> fat JAR file is attached to a cluster, users will be able to access the <code>com.github.mrpowers.spark.daria</code> and <code>com.github.mrpowers.spark.pika</code> namespaces.</p> <p>We don't want to provide access to the <code>com.github.mrpowers.spark.daria</code> namespace when <code>spark-pika</code> is attached to a cluster for two reasons:</p> <ol> <li>It just feels wrong. When users attach the <code>spark-pika</code> JAR file to their Spark cluster, they should only be able to access the <code>spark-pika</code> namespace. Adding additional namespaces to the classpath is unexpected.</li> <li>It prevents users from accessing a different <code>spark-daria</code> version than what's specified in the <code>spark-pika</code> <code>build.sbt</code> file. In this example, users are forced to use <code>spark-daria</code> version <code>2.3.1_0.24.0</code>.</li> </ol>"},{"location":"apache-spark/shading-dependencies-with-sbt/#how-to-shade-the-spark-daria-dependency","title":"How to shade the <code>spark-daria</code> dependency","text":"<p>We can use SBT to change the <code>spark-daria</code> namespace for all the code that's used by <code>spark-pika</code>. <code>spark-daria</code> will still be in the fat JAR file, but the namespace will be different, so users can still attach their own version of <code>spark-daria</code> to the cluster.</p> <p>Here is the code to shade the <code>spark-daria</code> dependency in <code>spark-pika</code>.</p> <pre><code>assemblyShadeRules in assembly := Seq(\n  ShadeRule.rename(\"com.github.mrpowers.spark.daria.**\" -&gt; \"shadedSparkDariaForSparkPika.@1\").inAll\n)\n</code></pre> <p>Let's run <code>sbt clean</code> and then rebuild the <code>spark-pika</code> JAR file with <code>sbt assembly</code>. Let's examine the contents of the new JAR file with <code>jar tvf target/scala-2.11/spark-pika_2.11-2.3.1_0.0.1.jar</code>.</p> <pre><code>0 Sun Sep 23 23:29:32 COT 2018 com/\n0 Sun Sep 23 23:29:32 COT 2018 com/github/\n0 Sun Sep 23 23:29:32 COT 2018 com/github/mrpowers/\n0 Sun Sep 23 23:29:32 COT 2018 com/github/mrpowers/spark/\n0 Sun Sep 23 23:29:32 COT 2018 com/github/mrpowers/spark/pika/\n0 Sun Sep 23 23:29:32 COT 2018 shadedSparkDariaForSparkPika/\n0 Sun Sep 23 23:29:32 COT 2018 shadedSparkDariaForSparkPika/ml/\n0 Sun Sep 23 23:29:32 COT 2018 shadedSparkDariaForSparkPika/sql/\n0 Sun Sep 23 23:29:32 COT 2018 shadedSparkDariaForSparkPika/sql/types/\n0 Sun Sep 23 23:29:32 COT 2018 shadedSparkDariaForSparkPika/utils/\n</code></pre> <p>The JAR file used to contain the <code>com.github.mrpowers.spark.daria</code> namespace and that's now been replaced with a <code>shadedSparkDariaForSparkPika</code> namespace.</p> <p>All the <code>spark-pika</code> references to <code>spark-daria</code> will use the <code>shadedSparkDariaForSparkPika</code> namespace.</p> <p>Users can attach both <code>spark-daria</code> and <code>spark-pika</code> to the same Spark cluster now and there won't be a <code>com.github.mrpowers.spark.daria</code> namespace collision anymore.</p>"},{"location":"apache-spark/shading-dependencies-with-sbt/#conclusion","title":"Conclusion","text":"<p>When creating Spark libraries, make sure to shade dependencies that are included in the fat JAR file, so your library users can specify different versions for dependencies at will. Try your best to design your libraries to only add a single namespace to the classpath when the JAR files is attached to a cluster.</p>"},{"location":"apache-spark/spark-sql-functions/","title":"Introduction to Spark SQL functions","text":"<p>Spark SQL functions make it easy to perform DataFrame analyses.</p> <p>This post will show you how to use the built-in Spark SQL functions and how to build your own SQL functions.</p> <p>Make sure to read\u00a0Writing Beautiful Spark Code\u00a0for a detailed overview of how to use SQL functions in production applications.</p>"},{"location":"apache-spark/spark-sql-functions/#review-of-common-functions","title":"Review of common functions","text":"<p>The Spark SQL functions are stored in the <code>org.apache.spark.sql.functions</code> object.</p> <p>The documentation page lists all of the built-in SQL functions.</p> <p>Let's create a DataFrame with a <code>number</code> column and use the <code>factorial</code> function to append a <code>number_factorial</code> column.</p> <pre><code>import org.apache.spark.sql.functions._\n\nval df = Seq(2, 3, 4).toDF(\"number\")\n\ndf\n  .withColumn(\"number_factorial\", factorial(col(\"number\")))\n  .show()\n</code></pre> <pre><code>+------+----------------+\n|number|number_factorial|\n+------+----------------+\n|     2|               2|\n|     3|               6|\n|     4|              24|\n+------+----------------+\n</code></pre> <p>The <code>factorial()</code> function takes a single <code>Column</code> argument. The <code>col()</code> function, also defined in the <code>org.apache.spark.sql.functions</code> object, returns a <code>Column</code> object based on the column name.</p> <p>If Spark implicits are imported (i.e. <code>import spark.implicits._</code>), then you can also create a <code>Column</code> object with the <code>$</code> operator. This code also works.</p> <pre><code>import org.apache.spark.sql.functions._\nimport spark.implicits._\n\nval df = Seq(2, 3, 4).toDF(\"number\")\n\ndf\n  .withColumn(\"number_factorial\", factorial($\"number\"))\n  .show()\n</code></pre>"},{"location":"apache-spark/spark-sql-functions/#lit-function","title":"<code>lit()</code> function","text":"<p>The <code>lit()</code> function creates a <code>Column</code> object out of a literal value. Let's create a DataFrame and use the <code>lit()</code> function to append a <code>spanish_hi</code> column to the DataFrame.</p> <pre><code>val df = Seq(\"sophia\", \"sol\", \"perro\").toDF(\"word\")\n\ndf\n  .withColumn(\"spanish_hi\", lit(\"hola\"))\n  .show()\n</code></pre> <pre><code>+------+----------+\n|  word|spanish_hi|\n+------+----------+\n|sophia|      hola|\n|   sol|      hola|\n| perro|      hola|\n+------+----------+\n</code></pre> <p>The <code>lit()</code> function is especially useful when making boolean comparisons.</p>"},{"location":"apache-spark/spark-sql-functions/#when-and-otherwise-functions","title":"<code>when()</code> and <code>otherwise()</code> functions","text":"<p>The <code>when()</code> and <code>otherwise()</code> functions are used for control flow in Spark SQL, similar to <code>if</code> and <code>else</code> in other programming languages.</p> <p>Let's create a DataFrame of countries and use some <code>when()</code> statements to append a <code>country</code> column.</p> <pre><code>val df = Seq(\"china\", \"canada\", \"italy\", \"tralfamadore\").toDF(\"word\")\n\ndf\n  .withColumn(\n    \"continent\",\n    when(col(\"word\") === lit(\"china\"), lit(\"asia\"))\n      .when(col(\"word\") === lit(\"canada\"), lit(\"north america\"))\n      .when(col(\"word\") === lit(\"italy\"), lit(\"europe\"))\n      .otherwise(\"not sure\")\n  )\n  .show()\n</code></pre> <pre><code>+------------+-------------+\n|        word|    continent|\n+------------+-------------+\n|       china|         asia|\n|      canada|north america|\n|       italy|       europe|\n|tralfamadore|     not sure|\n+------------+-------------+\n</code></pre> <p>Spark lets you cut the <code>lit()</code> method calls sometimes and to express code compactly.</p> <pre><code>df\n  .withColumn(\n    \"continent\",\n    when(col(\"word\") === \"china\", \"asia\")\n      .when(col(\"word\") === \"canada\", \"north america\")\n      .when(col(\"word\") === \"italy\", \"europe\")\n      .otherwise(\"not sure\")\n  )\n  .show()\n</code></pre> <p>Here's another example of using <code>when()</code> to manage control flow.</p> <pre><code>val df = Seq(10, 15, 25).toDF(\"age\")\n\ndf\n  .withColumn(\n    \"life_stage\",\n    when(col(\"age\") &lt; 13, \"child\")\n      .when(col(\"age\") &gt;= 13 &amp;&amp; col(\"age\") &lt;= 18, \"teenager\")\n      .when(col(\"age\") &gt; 18, \"adult\")\n  )\n  .show()\n</code></pre> <pre><code>+---+----------+\n|age|life_stage|\n+---+----------+\n| 10|     child|\n| 15|  teenager|\n| 25|     adult|\n+---+----------+\n</code></pre>"},{"location":"apache-spark/spark-sql-functions/#writing-your-own-sql-function","title":"Writing your own SQL function","text":"<p>You can use the built-in Spark SQL functions to build your own SQL functions. Let's create a <code>lifeStage()</code> function that takes an age as an argument and returns child, teenager or adult.</p> <pre><code>import org.apache.spark.sql.Column\n\ndef lifeStage(col: Column): Column = {\n  when(col &lt; 13, \"child\")\n    .when(col &gt;= 13 &amp;&amp; col &lt;= 18, \"teenager\")\n    .when(col &gt; 18, \"adult\")\n}\n</code></pre> <p>Let's use the <code>lifeStage()</code> function in a code snippet.</p> <pre><code>val df = Seq(10, 15, 25).toDF(\"age\")\n\ndf\n  .withColumn(\n    \"life_stage\",\n    lifeStage(col(\"age\"))\n  )\n  .show()\n</code></pre> <pre><code>+---+----------+\n|age|life_stage|\n+---+----------+\n| 10|     child|\n| 15|  teenager|\n| 25|     adult|\n+---+----------+\n</code></pre> <p>Let's create a <code>trimUpper()</code> function that trims all whitespace and capitalizes all of the characters in a string.</p> <pre><code>import org.apache.spark.sql.Column\n\ndef trimUpper(col: Column): Column = {\n  trim(upper(col))\n}\n</code></pre> <p>Let's run <code>trimUpper()</code> on a sample data set.</p> <pre><code>val df = Seq(\n  \"   some stuff\",\n  \"like CHEESE     \"\n).toDF(\"weird\")\n\ndf\n  .withColumn(\n    \"cleaned\",\n    trimUpper(col(\"weird\"))\n  )\n  .show()\n</code></pre> <pre><code>+----------------+-----------+\n|           weird|    cleaned|\n+----------------+-----------+\n|      some stuff| SOME STUFF|\n|like CHEESE     |LIKE CHEESE|\n+----------------+-----------+\n</code></pre> <p>Custom SQL functions can typically be used instead of UDFs. Avoiding UDFs is a great way to write better Spark code as described in this post.</p>"},{"location":"apache-spark/spark-sql-functions/#testing-sql-functions","title":"Testing SQL functions","text":"<p>You can inspect the SQL that's generated by a SQL function with the <code>toString</code> method.</p> <pre><code>lifeStage(lit(\"10\")).toString\n</code></pre> <pre><code>CASE\n  WHEN (10 &lt; 13) THEN child\n  WHEN ((10 &gt;= 13) AND (10 &lt;= 18)) THEN teenager\n  WHEN (10 &gt; 18) THEN adult\nEND\n</code></pre> <p>In our test suite, we can make sure that the SQL string that's generated equals what's expected.</p> <pre><code>val expected = \"CASE WHEN (10 &lt; 13) THEN child WHEN ((10 &gt;= 13) AND (10 &lt;= 18)) THEN teenager WHEN (10 &gt; 18) THEN adult END\"\nlifeStage(lit(\"10\")).toString == expected\n</code></pre> <p>We can also create a DataFrame, append a column with the <code>lifeStage()</code> function, and use the spark-fast-tests library to compare DataFrame equality.</p>"},{"location":"apache-spark/spark-sql-functions/#next-steps","title":"Next steps","text":"<p>Spark SQL functions are preferable to UDFs because they handle the <code>null</code> case gracefully (without a lot of code) and because they are not a black box.</p> <p>Most Spark analyses can be run by leveraging the standard library and reverting to custom SQL functions when necessary. Avoid UDFs at all costs!</p>"},{"location":"apache-spark/sparksession/","title":"Managing the SparkSession, The DataFrame Entry Point","text":"<p>The SparkSession is used to create and read DataFrames. It's used whenever you create a DataFrame in your test suite or whenever you read a Parquet / CSV data lake into a DataFrame.</p> <p>This post explains how to create a SparkSession, share it throughout your program, and use it to create DataFrames.</p>"},{"location":"apache-spark/sparksession/#accessing-the-sparksession","title":"Accessing the SparkSession","text":"<p>A SparkSession is automatically created and stored in the <code>spark</code> variable whenever you start the Spark console or open a Databricks notebook.</p> <p>Your program should reuse the same SparkSession and you should avoid any code that creates and uses a different SparkSession.</p>"},{"location":"apache-spark/sparksession/#creating-a-rdd","title":"Creating a RDD","text":"<p>Let's open the Spark console and use the <code>spark</code> variable to create a RDD from a sequence.</p> <p>Notice that the message <code>Spark session available as 'spark'</code> is printed when you start the Spark shell.</p> <pre><code>val data = Seq(2, 4, 6)\nval myRDD = spark.sparkContext.parallelize(data)\n</code></pre> <p>The <code>SparkSession</code> is used to access the <code>SparkContext</code>, which has a <code>parallelize</code> method that converts a sequence into a RDD.</p> <p>RDDs aren't used much now that the DataFrame API has been released, but they're still useful when creating DataFrames.</p>"},{"location":"apache-spark/sparksession/#creating-a-dataframe","title":"Creating a DataFrame","text":"<p>The SparkSession is used twice when manually creating a DataFrame:</p> <ol> <li>Converts a sequence into a RDD</li> <li>Converts a RDD into a DataFrame</li> </ol> <pre><code>import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nval rdd = spark.sparkContext.parallelize(\n  Seq(\n    Row(\"bob\", 55)\n  )\n)\n\nval schema = StructType(\n  Seq(\n    StructField(\"name\", StringType, true),\n    StructField(\"age\", IntegerType, true)\n  )\n)\n\nval df = spark.createDataFrame(rdd, schema)\n</code></pre> <pre><code>df.show()\n\n+----+---+\n|name|age|\n+----+---+\n| bob| 55|\n+----+---+\n</code></pre> <p>You will frequently use the SparkSession to create DataFrames when testing your code.</p>"},{"location":"apache-spark/sparksession/#reading-a-dataframe","title":"Reading a DataFrame","text":"<p>The SparkSession is also used to read CSV, JSON, and Parquet files.</p> <p>Here are some examples.</p> <pre><code>val df1 = spark.read.csv(\"/mnt/my-bucket/csv-data\")\nval df2 = spark.read.json(\"/mnt/my-bucket/json-data\")\nval df3 = spark.read.parquet(\"/mnt/my-bucket/parquet-data\")\n</code></pre> <p>There are separate posts on CSV, JSON, and Parquet files that do deep dives into the intracacies of each file format.</p>"},{"location":"apache-spark/sparksession/#creating-a-sparksession","title":"Creating a SparkSession","text":"<p>You can create a SparkSession in your applications with the <code>getOrCreate</code> method:</p> <pre><code>val spark = SparkSession.builder().master(\"local\").appName(\"my cool app\").getOrCreate()\n</code></pre> <p>You don't need to manually create a SparkSession in programming environments that already define the variable (e.g. the Spark shell or a Databricks notebook). Creating your own SparkSession becomes vital when you start writing Spark code in a text editor.</p> <p>A lot of Spark programmers have trouble writing high quality code because they cannot make the jump to writing Spark code in a text editor with tests. It's hard to write good code without following best practices.</p> <p>Wrapping the <code>spark</code> variable in a <code>trait</code> is the best way to share it across different classes and objects in your codebase.</p> <pre><code>import org.apache.spark.sql.SparkSession\n\ntrait SparkSessionWrapper extends Serializable {\n\n  lazy val spark: SparkSession = {\n    SparkSession.builder().master(\"local\").appName(\"my cool app\").getOrCreate()\n  }\n\n}\n</code></pre> <p>The <code>getOrCreate()</code> method will create a new SparkSession if one does not exist, but reuse an exiting SparkSession if it exists.</p> <p>When your test suite is run, this code will create a <code>SparkSession</code> when the first <code>spark</code> variable is found. After the initial SparkSession is created, it will be reused for every subsequent reference to <code>spark</code>.</p> <p>Your production environment will probably already define the <code>spark</code> variable, so <code>getOrCreate()</code> won't ever both creating a SparkSession and will simply use the SparkSession already created by the environment.</p> <p>Here is how the <code>SparkSessionWrapper</code> can be used in some example objects.</p> <pre><code>object transformations extends SparkSessionWrapper {\n\n  def withSomeDatamart(\n    coolDF: DataFrame = spark.read.parquet(\"/mnt/my-bucket/cool-data\")\n  )(df: DataFrame): DataFrame = {\n    df.join(\n      broadcast(coolDF),\n      df(\"some_id\") &lt;=&gt; coolDF(\"some_id\")\n    )\n  }\n\n}\n</code></pre> <p>The <code>transformations.withSomeDatamart()</code> method is injecting <code>coolDF</code>, so the code can easily be tested and intelligently grab the right file by default when run in production.</p> <p>Notice how the <code>spark</code> variable is used to set our smart default.</p> <p>We will use the <code>SparkSessionWrapper</code> trait and <code>spark</code> variable again when testing the <code>withSomeDatamart</code> method.</p> <pre><code>import utest._\n\nobject TransformsTest extends TestSuite with SparkSessionWrapper with ColumnComparer {\n\n  val tests = Tests {\n\n    'withSomeDatamart - {\n\n      val coolDF = spark.createDF(\n        List(\n\n        ), List(\n\n        )\n      )\n\n      val df = spark.createDF(\n        List(\n\n        ), List(\n\n        )\n      ).transform(transformations.withSomeDatamart())\n\n    }\n\n  }\n\n}\n</code></pre> <p>The test leverages the <code>createDF</code> method, which is a SparkSession extension defined in <code>spark-daria</code>.</p> <p><code>createDF</code> is similar to <code>createDataFrame</code>, but more cocise. See this blog post on manually creating Spark DataFrames for more details.</p>"},{"location":"apache-spark/sparksession/#reusing-the-sparksession-in-the-test-suite","title":"Reusing the SparkSession in the test suite","text":"<p>Starting and stopping the SparkSession is slow, so you want to reuse the same SparkSession throughout your test suite. Don't restart the SparkSession for every test file that is run - Spark tests run slowly enough as is and shouldn't be made any slower.</p> <p>The <code>SparkSessionWrapper</code> can be reused in your application code and the test suite.</p>"},{"location":"apache-spark/sparksession/#sparkcontext","title":"SparkContext","text":"<p>The <code>SparkSession</code> encapsulates the <code>SparkConf</code>, <code>SparkContext</code>, and <code>SQLContext</code>.</p> <p>Prior to Spark 2.0, developers needed to explicly create <code>SparkConf</code>, <code>SparkContext</code>, and <code>SQLContext</code> objects. Now Spark developers, can just create a <code>SparkSession</code> and access the other objects as needed.</p> <p>The following code snippet uses the <code>SparkSession</code> to access the <code>sparkContext</code>, so the <code>parallelize</code> method can be used to create a DataFrame (we saw this same snippet earlier in the blog post).</p> <pre><code>spark.sparkContext.parallelize(\n  Seq(\n    Row(\"bob\", 55)\n  )\n)\n</code></pre> <p>You shouldn't have to access the <code>sparkContext</code> much - pretty much only when manually creating DataFrames. See the spark-daria <code>createDF()</code> method, so you don't even need to explicitly call <code>sparkContext</code> when you want to create a DataFrame.</p> <p>Read this blog post for more information.</p>"},{"location":"apache-spark/sparksession/#conclusion","title":"Conclusion","text":"<p>You'll need a SparkSession in your programs to create DataFrames.</p> <p>Reusing the SparkSession in your application is critical for good code organization. Reusing the SparkSession in your test suite is vital to make your tests execute as quickly as possible.</p>"},{"location":"apache-spark/speaking-slack-notifications/","title":"Speaking Slack Notifications from Spark","text":"<p>The spark-slack library can be used to speak notifications to Slack from your Spark programs and handle Slack Slash command responses.</p> <p>You can speak Slack notifications to alert stakeholders when an important job is done running or even speak counts from a Spark DataFrame.</p> <p>This blog post will also show how to run Spark ETL processes from the Slack command line that will allow your organization to operate more transparently and efficiently.</p>"},{"location":"apache-spark/speaking-slack-notifications/#slack-messages","title":"Slack Messages","text":"<p>Here\u2019s how to speak a \u201cYou are amazing\u201d message in the #general channel:</p> <pre><code>import com.github.mrpowers.spark.slack.Notifier\n\nval webhookUrl = \"https://hooks.slack.com/services/...\"\nval notifier = new Notifier(webhookUrl)\nnotifier.speak(\"You are amazing\", \"general\", \":wink:\", \"Frank\")\nval notifier = new Notifier()\nnotifier.speak(\"You are amazing\", \"general\", \":wink:\", \"Frank\")\n</code></pre> <p>Here\u2019s how to speak a count of all the records in a DataFrame (df) to a Slack channel.</p> <pre><code>val notifier = new Notifier()\nval formatter = java.text.NumberFormat.getIntegerInstance\nval message = s\"Total Count: ${formatter.format(df.count)}\"\nnotifier.speak(message, \"general\", \":wink:\", \"Frank\")\n</code></pre> <p>You can add this code to a job that\u2019s run periodically with cron and use this to speak updated counts for key metrics to stakeholders.</p>"},{"location":"apache-spark/speaking-slack-notifications/#handling-slack-slash-command-responses","title":"Handling Slack Slash Command Responses","text":"<p>Slack lets you create custom Slash commands that can be used to kick off Spark jobs.</p> <p>You can send the Slack Slash command POST response to AWS Lambda and write a Lamdba function that sends another POST request to the Databricks API to kick off a job.</p> <p>The spark-slack SlashParser class converts a Slack Slash JSON string into a Scala object that\u2019s much easier to work with in Spark.</p> <p>Let\u2019s say you create a Slack Slash command for /mcsc (my cool Slash command) and type in the following command:</p> <pre><code>/mcsc marketing extract quarterly_report\n</code></pre> <p>Slack will send a JSON response with this format:</p> <pre><code>token=gIkuvaNzQIHg97ATvDxqgjtO\nteam_id=T0001\nteam_domain=example\nchannel_id=C2147483705\nchannel_name=test\nuser_id=U2147483697\nuser_name=Steve\ncommand=/mcsc\ntext=marketing extract quarterly_report\nresponse_url=https://hooks.slack.com/commands/1234/5678\n</code></pre> <p>Here\u2019s an example of code that will parse the text and run different Databricks notebooks based on the Slack Slash command arguments.</p> <pre><code>import com.github.mrpowers.spark.slack.slash_commands.SlashParser\nimport com.github.mrpowers.spark.slack.Notifier\n\nval parser = new SlashParser(dbutils.widgets.get(\"slack-response\"))\nval arg0 = parser.textComponents(0)\nval arg1 = parser.textComponents(1)\nval arg2 = parser.textComponents(2)\n\narg1 match {\n  case \"extract\" =&gt; dbutils.notebook.run(\"./Extracts\", 0, Map(\"extractName\" -&gt; arg2))\n  case \"transform\" =&gt; dbutils.notebook.run(\"./Transforms\", 0, Map(\"transformationName\" -&gt; arg2))\n  case \"report\" =&gt; dbutils.notebook.run(\"./Reports\", 0, Map(\"reportName\" -&gt; arg2))\n}\n\nval notifier = new Notifier(WEBHOOK_URL)\nnotifier.speak(\n  \"Your extract ran successfully\",\n  parser.slashResponse.channel_name,\n  \":bob:\",\n  \"Bob\"\n)\n</code></pre> <p>Running Spark ETL processes from the Slack command line is a transparent way to keep all stakeholders updated.</p>"},{"location":"apache-spark/speaking-slack-notifications/#next-steps","title":"Next steps","text":"<p>Important notifications from Spark jobs should be spoken in shared Slack channels to increase the transparency of workflows.</p> <p>Slack should be used as a shared terminal with custom commands to kick off important jobs.</p> <p>Recurring analyses should be run with cron jobs and should notify stakeholders when completed.</p> <p>Making Spark jobs easy to run is critical for organizational adoption of the technology. Most employees spend most of their day in Slack, so it\u2019s the best place to make Spark jobs accessible.</p>"},{"location":"apache-spark/start-end-month-last-day-date-trunc/","title":"Calculating Month Start and End Dates with Spark","text":"<p>This post shows how to create <code>beginningOfMonthDate</code> and <code>endOfMonthDate</code> functions by leveraging the native Spark datetime functions.</p> <p>The native Spark datetime functions are not easy to use, so it's important to build abstractions on top of the standard lib. Using the standard lib functions directly results in code that's difficult to understand.</p>"},{"location":"apache-spark/start-end-month-last-day-date-trunc/#month-end","title":"Month end","text":"<p>Spark has a function that calculates the last day of the month, but it's poorly named. Let's give the Spark function a more descriptive name so our code is readable.</p> <pre><code>def endOfMonthDate(col: Column): Column = {\n  last_day(col)\n}\n</code></pre> <p>You can access this function via the spark-daria library if you don't want to define it yourself.</p> <p>Suppose you have the following data:</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2016-09-10|\n|2020-01-01|\n|2016-01-10|\n|      null|\n+----------+\n</code></pre> <p>Append an <code>end_of_month</code> column to the DataFrame:</p> <pre><code>import com.github.mrpowers.spark.daria.sql.functions._\n\ndf\n  .withColumn(\"end_of_month\", endOfMonthDate(col(\"some_date\")))\n  .show()\n</code></pre> <p>Observe the results:</p> <pre><code>+----------+------------+\n| some_date|end_of_month|\n+----------+------------+\n|2016-09-10|  2016-09-30|\n|2020-01-01|  2020-01-31|\n|2016-01-10|  2016-01-31|\n|      null|        null|\n+----------+------------+\n</code></pre>"},{"location":"apache-spark/start-end-month-last-day-date-trunc/#month-start","title":"Month start","text":"<p>You can calculate the start of the month with the <code>trunc</code> or <code>date_trunc</code> functions. Suppose you have the following DataFrame with a date column:</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2017-11-25|\n|2017-12-21|\n|2017-09-12|\n|      null|\n+----------+\n</code></pre> <p>Here are the two different ways to calculate the beginning of the month:</p> <pre><code>datesDF\n  .withColumn(\"beginning_of_month_date\", trunc(col(\"some_date\"), \"month\"))\n  .withColumn(\"beginning_of_month_time\", date_trunc(\"month\" ,col(\"some_date\")))\n  .show()\n</code></pre> <pre><code>+----------+-----------------------+-----------------------+\n| some_date|beginning_of_month_date|beginning_of_month_time|\n+----------+-----------------------+-----------------------+\n|2017-11-25|             2017-11-01|    2017-11-01 00:00:00|\n|2017-12-21|             2017-12-01|    2017-12-01 00:00:00|\n|2017-09-12|             2017-09-01|    2017-09-01 00:00:00|\n|      null|                   null|                   null|\n+----------+-----------------------+-----------------------+\n</code></pre> <p>Important observations:</p> <ul> <li><code>trunc</code> returns a date column and <code>date_trunc</code> returns a timestamp column</li> <li><code>trunc</code> takes <code>col(\"some_date\")</code> as the first argument and <code>date_trunc</code> takes <code>col(\"some_date\")</code> as the second argument. They're inconsistent.</li> <li><code>date_trunc</code> sounds like it should be returning a date column. It's not named well.</li> </ul> <p>Let's define <code>beginningOfMonthDate</code> and <code>beginningOfMonthTime</code> functions that are more intuitive.</p> <pre><code>def beginningOfMonthDate(col: Column): Column = {\n  trunc(col, \"month\")\n}\n\ndef beginningOfMonthTime(col: Column): Column = {\n  date_trunc(\"month\", col)\n}\n</code></pre> <p>These functions let us write code that's easier to read:</p> <pre><code>datesDF\n  .withColumn(\"beginning_of_month_date\", beginningOfMonthDate(col(\"some_date\")))\n  .withColumn(\"beginning_of_month_time\", beginningOfMonthTime(col(\"some_date\")))\n  .show()\n</code></pre> <p>These functions are defined in spark-daria.</p>"},{"location":"apache-spark/start-end-month-last-day-date-trunc/#next-steps","title":"Next steps","text":"<p>Spark's standard datetime functions aren't the best, but they're still better than building UDFs with the <code>java.time</code> library.</p> <p>Using the spark-daria datetime abstractions is the best way to create readable code.</p> <p>The spark-daria datetime function names are based on Rails, which is a well designed datetime library.</p> <p>See this post for a detailed explanation on how spark-daria makes computing the week start / week end / next weekday easy. These are examples of core datetime functionality that should be abstracted in an open source library. You shouldn't need to reinvent the wheel and write core datetime logic in your applications.</p>"},{"location":"apache-spark/structured-streaming-trigger-once/","title":"Reverse Engineering Spark Structured Streaming and Trigger.Once","text":"<p>Spark Structured Streaming and Trigger.Once make it easy to run incremental updates. Spark uses a checkpoint directory to identify the data that's already been processed and only analyzes the new data.</p> <p>This blog post demonstrates how to use Structured Streaming and Trigger.Once and provides a detailed look at the checkpoint directory that easily allows Spark to identify the newly added files.</p> <p>Let's start by running a simple example and examining the contents of the checkpoint directory.</p>"},{"location":"apache-spark/structured-streaming-trigger-once/#simple-example","title":"Simple example","text":"<p>Let's create a <code>dog_data_csv</code> directory with the following <code>dogs1</code> file to start.</p> <pre><code>first_name,breed\nfido,lab\nspot,bulldog\n</code></pre> <p>Let's use Spark Structured Streaming and Trigger.Once to write our all the CSV data in <code>dog_data_csv</code> to a <code>dog_data_parquet</code> data lake.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\nimport org.apache.spark.sql.types._\n\nval csvPath = new java.io.File(\"./tmp/dog_data_csv/\").getCanonicalPath\n\nval schema = StructType(\n  List(\n    StructField(\"first_name\", StringType, true),\n    StructField(\"breed\", StringType, true)\n  )\n)\n\nval df = spark.readStream\n  .schema(schema)\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(csvPath)\n\nval checkpointPath = new java.io.File(\"./tmp/dog_data_checkpoint/\").getCanonicalPath\nval parquetPath = new java.io.File(\"./tmp/dog_data_parquet/\").getCanonicalPath\n\ndf\n  .writeStream\n  .trigger(Trigger.Once)\n  .format(\"parquet\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .start(parquetPath)\n</code></pre> <p>The parquet data is written out in the <code>dog_data_parquet</code> directory. Let's print out the Parquet data to verify it only contains the two rows of data from our CSV file.</p> <pre><code>val parquetPath = new java.io.File(\"./tmp/dog_data_parquet/\").getCanonicalPath\nspark.read.parquet(parquetPath).show()\n\n+----------+-------+\n|first_name|  breed|\n+----------+-------+\n|      fido|    lab|\n|      spot|bulldog|\n+----------+-------+\n</code></pre> <p>The <code>dog_data_checkpoint</code> directory contains the following files.</p> <pre><code>dog_data_checkpoint/\n  commits/\n    0\n  offsets/\n    0\n  sources/\n    0/\n      0\n  metadata\n</code></pre> <p>Spark creates lots of JSON files in the checkpoint directory (the files don't have extensions for some reason). Let's take a peek at the metadata included in these files.</p> <p>Here's the <code>dog_data_checkpoint/commits/0</code> file:</p> <pre><code>v1\n{\"nextBatchWatermarkMs\":0}\n</code></pre> <p>Here's the <code>dog_data_checkpoint/offsets/0</code> file:</p> <pre><code>v1\n{\n  \"batchWatermarkMs\":0,\n  \"batchTimestampMs\":1565368744601,\n  \"conf\":{\n    \"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\n    \"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\n    \"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\n    \"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\n    \"spark.sql.shuffle.partitions\":\"200\"\n  }\n}\n{\"logOffset\":0}\n</code></pre> <p>Here's the <code>dog_data_checkpoint/sources/0/0</code> file:</p> <pre><code>v1\n{\n  \"path\":\"file:///Users/powers/Documents/code/my_apps/yello-taxi/tmp/dog_data_csv/dogs1.csv\",\n  \"timestamp\":1565354770000,\n  \"batchId\":0\n}\n</code></pre>"},{"location":"apache-spark/structured-streaming-trigger-once/#incremental-update","title":"Incremental update","text":"<p>Let's add the <code>dogs2.csv</code> file to <code>tmp/dog_data_csv</code>, run the incremental update code, verify that the Parquet lake is incrementally updated, and explore what files are added to the checkpoint directory when an incremental update is run.</p> <p>Here's the <code>tmp/dog_data_csv/dogs2.csv</code> file that'll be created:</p> <pre><code>first_name,breed\nfido,beagle\nlou,pug\n</code></pre> <p>The Trigger.Once code was wrapped in a method (see this file) and can be run from the SBT console with this command: <code>mrpowers.yellow.taxi.IncrementalDogUpdater.update()</code>.</p> <p>Let's verify the Parquet data lake has been updated with the new data:</p> <pre><code>val parquetPath = new java.io.File(\"./tmp/dog_data_parquet/\").getCanonicalPath\nspark.read.parquet(parquetPath).show()\n\n+----------+-------+\n|first_name|  breed|\n+----------+-------+\n|      fido|    lab|\n|      spot|bulldog|\n|      fido| beagle|\n|       lou|    pug|\n+----------+-------+\n</code></pre> <p>The <code>dog_data_checkpoint</code> directory now contains the following files.</p> <pre><code>dog_data_checkpoint/\n  commits/\n    0\n    1\n  offsets/\n    0\n    1\n  sources/\n    0/\n      0\n      1\n  metadata\n</code></pre> <p>Here are the contents of the <code>dog_data_checkpoint/commits/1</code> file:</p> <pre><code>v1\n{\"nextBatchWatermarkMs\":0}\n</code></pre> <p>Here's the <code>dog_data_checkpoint/offsets/1</code> file:</p> <pre><code>v1\n{\n  \"batchWatermarkMs\":0,\n  \"batchTimestampMs\":1565446320529,\n  \"conf\":{\n    \"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\n    \"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\n    \"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\n    \"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\n    \"spark.sql.shuffle.partitions\":\"200\"\n  }\n}\n{\"logOffset\":0}\n</code></pre> <p>Here's the <code>dog_data_checkpoint/sources/0/1</code> file:</p> <pre><code>v1\n{\n  \"path\":\"file:///Users/powers/Documents/code/my_apps/yello-taxi/tmp/dog_data_csv/dogs2.csv\",\n  \"timestamp\":1565354885000,\n  \"batchId\":1\n}\n</code></pre>"},{"location":"apache-spark/structured-streaming-trigger-once/#incrementally-updating-aggregations-manually","title":"Incrementally updating aggregations manually","text":"<p>Let's start over and create a job that creates a running count of each dog by name. We don't want to reprocess files that have already been aggregated. We'd like to combine the existing results with the new files that are added, so we don't need to recalculate the same data.</p> <p>We'll run the following code to create an initial cache of the counts by name.</p> <pre><code>val csvPath = new java.io.File(\"./tmp/dog_data_csv/dogs1.csv\").getCanonicalPath\nval df = spark.read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(csvPath)\n\nval outputPath = new java.io.File(\"./tmp/dog_data_name_counts\").getCanonicalPath\ndf\n  .groupBy(\"first_name\")\n  .count()\n  .repartition(1)\n  .write\n  .parquet(outputPath)\n</code></pre> <p>Let's inspect the aggregation after processing the first file.</p> <pre><code>val parquetPath = new java.io.File(\"./tmp/dog_data_name_counts/\").getCanonicalPath\nspark.read.parquet(parquetPath).show()\n\n+----------+-----+\n|first_name|count|\n+----------+-----+\n|      fido|    1|\n|      spot|    1|\n+----------+-----+\n</code></pre> <p>Let's run some code that will combine the aggregations from the first data file with the second data file, rerun the aggregations, and then write out the results. We will not aggregate the results in the first file again - we'll just combine our existing results with the second data file.</p> <pre><code>val csvPath = new java.io.File(\"./tmp/dog_data_csv/dogs2.csv\").getCanonicalPath\nval df = spark.read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(csvPath)\n\nval existingCountsPath = new java.io.File(\"./tmp/dog_data_name_counts\").getCanonicalPath\nval tmpPath = new java.io.File(\"./tmp/dog_data_name_counts_tmp\").getCanonicalPath\nval existingCountsDF = spark\n  .read\n  .parquet(existingCountsPath)\n\ndf\n  .union(existingCountsDF)\n  .cache()\n  .groupBy(\"first_name\")\n  .count()\n  .repartition(1)\n  .write\n  .mode(SaveMode.Overwrite)\n  .parquet(tmpPath)\n\nspark\n  .read\n  .parquet(tmpPath)\n  .write\n  .mode(SaveMode.Overwrite)\n  .parquet(existingCountsPath)\n</code></pre> <p>Let's inspect the aggregation after processing the incremental update.</p> <pre><code>val parquetPath = new java.io.File(\"./tmp/dog_data_name_counts/\").getCanonicalPath\nspark.read.parquet(parquetPath).show()\n\n+----------+-----+\n|first_name|count|\n+----------+-----+\n|      fido|    2|\n|      spot|    1|\n|       lou|    1|\n+----------+-----+\n</code></pre> <p>This implementation isn't ideal because the initial run and incremental update require different code. Let's see if we can write a single method that'll be useful for the initial run and the incremental update.</p>"},{"location":"apache-spark/structured-streaming-trigger-once/#refactored-updating-aggregations","title":"Refactored updating aggregations","text":"<p>Let's start out with a little helper method to check if a directory exists.</p> <pre><code>def dirExists(hdfsDirectory: String): Boolean = {\n  val hadoopConf = new org.apache.hadoop.conf.Configuration()\n  val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)\n  fs.exists(new org.apache.hadoop.fs.Path(hdfsDirectory))\n}\n</code></pre> <p>Now we can write a method that's flexible enough to create or incrementally update the aggregate name counts, depending on if an output file already exists.</p> <pre><code>def refactoredUpdateCountByName(csvPath: String): Unit = {\n  val df = spark.read\n    .option(\"header\", \"true\")\n    .option(\"charset\", \"UTF8\")\n    .csv(csvPath)\n\n  val existingCountsPath = new java.io.File(\"./tmp/dog_data_name_counts\").getCanonicalPath\n  val tmpPath = new java.io.File(\"./tmp/dog_data_name_counts_tmp\").getCanonicalPath\n\n  val unionedDF = if(dirExists(existingCountsPath)) {\n    spark\n      .read\n      .parquet(existingCountsPath)\n      .union(df)\n  } else {\n    df\n  }\n\n  unionedDF\n    .groupBy(\"first_name\")\n    .count()\n    .repartition(1)\n    .write\n    .mode(SaveMode.Overwrite)\n    .parquet(tmpPath)\n\n  spark\n    .read\n    .parquet(tmpPath)\n    .write\n    .mode(SaveMode.Overwrite)\n    .parquet(existingCountsPath)\n}\n</code></pre> <p>Let's run this method with both CSV files and examine the output.</p> <pre><code>val csvPath1 = new java.io.File(\"./tmp/dog_data_csv/dogs1.csv\").getCanonicalPath\nrefactoredUpdateCountByName(csvPath1)\nshowDogDataNameCounts()\n\n+----------+-----+\n|first_name|count|\n+----------+-----+\n|      fido|    1|\n|      spot|    1|\n+----------+-----+\n\nval csvPath2 = new java.io.File(\"./tmp/dog_data_csv/dogs2.csv\").getCanonicalPath\nrefactoredUpdateCountByName(csvPath2)\nshowDogDataNameCounts()\n\n+----------+-----+\n|first_name|count|\n+----------+-----+\n|      fido|    2|\n|      spot|    1|\n|       lou|    1|\n+----------+-----+\n</code></pre>"},{"location":"apache-spark/structured-streaming-trigger-once/#incrementally-updating-aggregates-with-structured-streaming-triggeronce","title":"Incrementally updating aggregates with Structured Streaming + Trigger.Once","text":"<p>It turns out that we can't build an incrementally updating aggregate file with Structured Streaming and Trigger.Once.</p> <p>Structured Streaming and Trigger.Once allows for a lot of complicated features, many of which are not needed for batch analyses.</p> <p>We need to build another open source solution that provides Spark developers with a flexible interface to run batch analysis. We can borrow the good ideas from Structured Streaming + Trigger.Once and make something that's a lot simpler, as we won't need to also consider streaming use cases.</p>"},{"location":"apache-spark/structured-streaming-trigger-once/#proposed-interface-for-the-batch-incremental-updater","title":"Proposed interface for the batch incremental updater","text":"<p>The batch incremental updater will track files that have already been processed and allow users to easily identify new files. Suppose a data lake contains 50,000 files that have already been processed and two new files are added. The batch incremental updater will make it easy for the user to identify the two newly added files for analysis.</p> <p>The batch incremental updater will use the standard Dataset API (not the streaming API), so developers can easily access all batch features. For example, users can use <code>SaveMode.Overwrite</code> that's not supported by the streaming API.</p>"},{"location":"apache-spark/structured-streaming-trigger-once/#next-steps","title":"Next steps","text":"<p>Structured Streaming + Trigger.Once is great for simple batch updates.</p> <p>We need a better solution for more complicated batch updates. This is a common use case and the community needs a great solution. Can't wait to collaborate with smart Spark engineers to get this built!</p>"},{"location":"apache-spark/testing-with-utest/","title":"Testing Spark Applications with uTest","text":"<p>The uTest Scala testing framework can be used to elegantly test your Spark code.</p> <p>The other popular Scala testing frameworks (Scalatest and Specs2) provide multiple different ways to solve the same thing, whereas uTest only provides one way to solve common testing problems.</p> <p>Spark users face a lot of choices when testing Spark code (testing framework customizations, creating Spark DataFrames, comparing DataFrame equality) and uTest does a great job making some choices for you and limiting your menu of options.</p> <p>Let's dive in an see how uTest can be used to test Scala and Spark code.</p>"},{"location":"apache-spark/testing-with-utest/#testing-scala-code-with-utest","title":"Testing Scala code with uTest","text":"<p>Let's start with the basics and show how uTest can test some pure Scala code.</p> <p>Add uTest to the <code>build.sbt</code> file to add this testing framework to your project.</p> <pre><code>libraryDependencies += \"com.lihaoyi\" %% \"utest\" % \"0.6.3\" % \"test\"\ntestFrameworks += new TestFramework(\"utest.runner.Framework\")\n</code></pre> <p>Create a <code>Calculator</code> object with an <code>add()</code> method that adds two integers.</p> <pre><code>package com.github.mrpowers.spark.daria.utils\n\nobject Calculator {\n\n  def add(x: Int, y: Int): Int = {\n    x + y\n  }\n\n}\n</code></pre> <p>Create a <code>CalculatorTest</code> file to test the <code>Calculator.add()</code> method.</p> <pre><code>package com.github.mrpowers.spark.daria.utils\n\nimport utest._\n\nobject CalculatorTest extends TestSuite {\n\n  val tests = Tests {\n\n    'add - {\n\n      \"adds two integers\" - {\n        assert(Calculator.add(2, 3) == 5)\n      }\n\n    }\n\n  }\n\n}\n</code></pre> <p>Important observations about a uTest test file:</p> <ul> <li><code>import utest._</code> provides access to a <code>TestSuite</code> trait that all uTest test files must mix in</li> <li><code>CalculatorTest</code> must be an <code>object</code> and cannot be a <code>class</code></li> <li><code>val tests = Tests</code> must be included in each test file</li> <li><code>sbt test</code> runs all the test files in the project, just like other frameworks</li> <li><code>sbt \"testOnly -- com.github.mrpowers.spark.daria.utils.CalculatorTest\"</code> only runs the tests in the <code>CalculatorTest</code> file</li> <li><code>sbt \"testOnly -- com.github.mrpowers.spark.daria.utils.CalculatorTest.add\"</code> only runs the tests in the <code>'add</code> expression</li> <li><code>'add</code> is an example of a Scala symbol. We could also use a string instead (i.e. <code>\"add\"</code> works too).</li> </ul>"},{"location":"apache-spark/testing-with-utest/#testing-spark-code-with-utest","title":"Testing Spark code with uTest","text":"<p>spark-testing-base and spark-fast-tests are the two most popular libraries with helper functions for testing Spark code.</p> <p>spark-testing-base only works with the Scalatest framework, so you'll need to use spark-fast-tests or write your own test helper methods when using the uTest framework.</p> <p>Let's see how uTest and spark-fast-tests are used to test the <code>removeAllWhitespace()</code> function in the spark-daria project. Here's the <code>removeAllWhitespace()</code> function definition.</p> <pre><code>object functions {\n\n  def removeAllWhitespace(col: Column): Column = {\n    regexp_replace(col, \"\\\\s+\", \"\")\n  }\n\n}\n</code></pre> <p>The spark-fast-tests <code>assertColumnEquality</code> method can be used to verify the equality of two columns in a DataFrame.</p> <p>You can create a DataFrame, add a column that removes all the whitespace with the <code>removeAllWhitespace</code> function, and compare the actual column that's appended with your expectations.</p> <p>The spark-daria <code>createDF</code> method is used to create the DataFrame in this test.</p> <pre><code>object FunctionsTest\n    extends TestSuite\n    with ColumnComparer\n    with SparkSessionTestWrapper {\n\n  val tests = Tests {\n\n    'removeAllWhitespace - {\n\n      \"removes all whitespace from a string\" - {\n\n        val df = spark.createDF(\n          List(\n            (\"Bruce   willis   \", \"Brucewillis\"),\n            (\"    obama\", \"obama\"),\n            (\"  nice  hair person  \", \"nicehairperson\"),\n            (null, null)\n          ), List(\n            (\"some_string\", StringType, true),\n            (\"expected\", StringType, true)\n          )\n        ).withColumn(\n            \"some_string_without_whitespace\",\n            functions.removeAllWhitespace(col(\"some_string\"))\n          )\n\n        assertColumnEquality(df, \"expected\", \"some_string_without_whitespace\")\n\n      }\n\n    }\n\n  }\n\n}\n</code></pre> <p>We can also test this function by creating two DataFrames and verifying equality with the spark-fast-tests <code>assertSmallDataFrameEquality</code> method.</p> <pre><code>object FunctionsTest\n    extends TestSuite\n    with DataFrameComparer\n    with SparkSessionTestWrapper {\n\n  val tests = Tests {\n\n    'removeAllWhitespace - {\n\n      \"removes all whitespace from a string with a column argument\" - {\n\n        val sourceDF = spark.createDF(\n          List(\n            (\"Bruce   willis   \"),\n            (\"    obama\"),\n            (\"  nice  hair person  \"),\n            (null)\n          ), List(\n            (\"some_string\", StringType, true)\n          )\n        )\n\n        val actualDF = sourceDF.withColumn(\n          \"some_string_without_whitespace\",\n          functions.removeAllWhitespace(col(\"some_string\"))\n        )\n\n        val expectedDF = spark.createDF(\n          List(\n            (\"Bruce   willis   \", \"Brucewillis\"),\n            (\"    obama\", \"obama\"),\n            (\"  nice  hair person  \", \"nicehairperson\"),\n            (null, null)\n          ), List(\n            (\"some_string\", StringType, true),\n            (\"some_string_without_whitespace\", StringType, true)\n          )\n        )\n\n        assertSmallDataFrameEquality(actualDF, expectedDF)\n\n      }\n\n    }\n\n  }\n\n}\n</code></pre> <p>The <code>assertSmallDataFrameEquality</code> requires more code than <code>assertColumnEquality</code>. <code>assertSmallDataFrameEquality</code> is also slower because you need to create two DataFrames instead of one. You should always use <code>assertColumnEquality</code> whenever possible.</p>"},{"location":"apache-spark/testing-with-utest/#what-testing-framework-should-you-use","title":"What testing framework should you use?","text":"<p>I think the uTest framework's design philosophy of only providing one way performing common tasks makes it the best framework for testing Spark code.</p> <p>When testing Spark code you'll already face multiple decisions on if you'll use a library with Spark test helper methods (spark-fast-tests or spark-testing-base) or if you'll create your own test helper methods. You don't need more decisions about the testing framework that you'll use.</p> <p>uTest also forces multiple teams to define tests and make assertions the same way. You won't have one team that uses <code>===</code>, another team that uses <code>must beEqual</code>, and a third team that uses <code>should be</code> for equality comparisons.</p> <p>If you're forced to use Scalatest (because you need to leverage some spark-testing-base features), you should use FreeSpec, so the tests look like uTest specs at least.</p>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/","title":"Migrating Scala Projects to Spark 3","text":"<p>This post explains how to migrate your Scala projects to Spark 3.</p> <p>It covers the high level steps and doesn't get into all the details.</p> <p>Migrating PySpark projects is easier. Python wheel files generated in a PySpark 2 app also work with PySpark 3.</p> <p>Scala projects that use Scala 2.11 will not work with Spark 3 projects and you need to cross compile the project to avoid strange error messages.</p> <p>There a few upgrade approaches:</p> <ul> <li>Cross compile with Spark 2.4.5 and Scala 2.11/2.12 and gradually shift jobs to Spark 3 (with the JAR files compiled with Scala 2.12)</li> <li>Upgrade your project to Spark 3 / Scala 2.12 and immediately switch everything over to Spark 3, skipping the cross compilation step</li> <li>Create a build matrix and build several jar files for different combinations of Scala and Spark (e.g. Scala 2.11/Spark 2.4.5, Scala 2.12/Spark 2.4.5, Scala 2.12/Spark 3.0.1)</li> </ul>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#specifying-dependencies","title":"Specifying dependencies","text":"<p>Different JAR files are stored in Maven for the different Scala versions.</p> <p>Scala project JAR files should be stored in Maven using this convention:</p> <pre><code>com/\n  github/\n    mrpowers/\n      spark-daria_2.11/\n        0.38.2/\n          spark-daria_2.11-0.38.2.jar\n      spark-daria_2.12/\n        0.38.2/\n          spark-daria_2.12-0.38.2.jar\n</code></pre> <p>See the Maven website for the 2.11 JARs and here for the 2.12 JARs.</p> <p>Scala dependencies should be added to your <code>build.sbt</code> file with the <code>%%</code> operator that automatically fetches the Scala version that corresponds with your project version.</p> <p>You should add dependencies to your project like this:</p> <pre><code>libraryDependencies += \"com.github.mrpowers\" %% \"spark-daria\" % \"0.38.2\"\n</code></pre> <p>The <code>%%</code> operator will grab the Scala 2.11 JAR file when the project version is set to Scala 2.11 and will use the Scala 2.12 JAR file when the project version is set to Scala 2.12.</p> <p>Make sure all of your Scala dependencies use the <code>%%</code> operator.</p> <p>Java dependencies don't need to use the <code>%%</code> operator because they don't need to be cross compiled for different versions of Scala. You can add a Java dependency like this an it'll cross compile fine with different Scala versions:</p> <pre><code>libraryDependencies += \"joda-time\" % \"joda-time\" % \"2.10.8\"\n</code></pre>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#cross-compile-with-scala-212","title":"Cross compile with Scala 2.12","text":"<p>Scala 2.12 support was added as of Spark 2.4.</p> <p>For Spark 2.0 - Spark 2.3, you could only use Scala 2.11.</p> <p>Once you upgrade your project to Spark 2.4.x, add the following line to your <code>build.sbt</code> file:</p> <pre><code>crossScalaVersions := Seq(\"2.11.12\", \"2.12.12\")\n</code></pre> <p>Run the <code>sbt +assembly</code> command to build two JAR files, one for Scala 2.11 and another for Scala 2.12. Here's where the JAR files are outputted in the file system:</p> <pre><code>spark-daria/\n  target/\n    scala-2.11/\n      spark-daria-assembly-0.38.2.jar\n    scala-2.12/\n      spark-daria-assembly-0.38.2.jar\n</code></pre> <p>This post discusses building JAR files in more detail.</p>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#errors-for-scala-211-jar-files-on-spark-3-clusters","title":"Errors for Scala 2.11 JAR files on Spark 3 clusters","text":"<p>Suppose you have the following DataFrame:</p> <pre><code>+----------+----------+\n|First Name|Person Age|\n+----------+----------+\n|     alice|         4|\n|     marco|         2|\n+----------+----------+\n</code></pre> <p>Let's run a spark-daria transformations to snake case all of the DataFrame columns:</p> <pre><code>import com.github.mrpowers.spark.daria.sql.transformations._\ndf.transform(snakeCaseColumns()).show()\n</code></pre> <p>This code errors out with this strange message: java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps.</p> <p>Your code will error out if you attach a JAR file that was compiled with Scala 2.11 to a Spark 3 cluster.</p> <p>Here's the error you'll see in Databricks:</p> <p></p> <p>The Scala 2.11 JAR file can be added to the Spark 3 cluster and imported without any errors. You won't get errors until you actually start running the code.</p>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#scala-212-projects-on-spark-2-cluster","title":"Scala 2.12 projects on Spark 2 cluster","text":"<p>All the Databricks Spark 2 clusters use Scala 2.11:</p> <p></p> <p>Scala 2.12 JAR files surprisingly work on Spark 2 clusters without any issues:</p> <p></p> <p>I'm not certain Scala 2.12 JAR files will work on Spark 2/Scala 2.11 clusters properly in all situations.</p> <p>It's always best to match the JAR file Scala version with the cluster Scala version.</p>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#how-to-make-migrating-easier","title":"How to make migrating easier","text":"<p>Upgrading Spark from Scala 2.11 to Scala 2.12 was a massive undertaking as you can see here.</p> <p></p> <p>You can make your projects easier to upgrade by limiting dependencies. Every dependency you have in a project is a potential point of failure when you're upgrading.</p> <p>When you add a dependency, make sure it's actively maintained and cross compiled with the latest versions of Scala. If a library you rely on does not publish a Scala 2.12 JAR file, then you'll need to fork it and build the JAR file yourself.</p> <p>spark-google-spreadsheets is a good example of a project that's not being built with Scala 2.12 yet. Don't add this dependency to your Spark 2 project unless you're prepared to release the Scala 2.12 JAR file yourself when you're trying to upgrade to Spark 3.</p> <p>When you're considering adding a library to your project, make sure to evaluate all the transitive dependencies. Here are some dependencies from spark-testing-base for example:</p> <pre><code>lazy val commonDependencies = Seq(\n  \"org.scalatest\" %% \"scalatest\" % \"3.0.5\",\n  \"io.github.nicolasstucki\" %% \"multisets\" % \"0.4\",\n  \"org.scalacheck\" %% \"scalacheck\" % \"1.14.0\",\n  \"junit\" % \"junit\" % \"4.12\",\n  \"org.eclipse.jetty\" % \"jetty-util\" % \"9.3.11.v20160721\",\n  \"com.novocode\" % \"junit-interface\" % \"0.11\" % \"test-&gt;default\")\n</code></pre> <p>If you depend on spark-testing-base, you'll be depending on multisets, which is now archived. multisets has JAR files for Scala 2.10, 2.11, and 2.12, but no JAR file for Scala 2.13. This'll be a problem when Spark eventually upgrades to Scala 2.13.</p> <p>Look for developers that build Scala libraries that are dependency free, have stable APIs, and are always updated to be cross compiled with the latest version of Scala.</p> <p>Li Haoyi's libraries like os-lib, utest, and fansi are great examples of stable, dependency-free libraries.</p> <p>You can go to Maven and see how the latest version of utest has JAR files for Scala 2.11, 2.12, and 2.13.</p> <p></p> <p>Scalatest is an example of a less reliable dependency, for example the Scalatest 3.1 release made backwards incompatible changes. Library users normally don't expect breaking changes for minor version bumps. Users typically expect semantic versioning rules to be followed.</p> <p>spark-testing-base depends on Scalatest 3.0.5 and upgrading the Scalatest version will be hard. Using the latest version of Scalatest with spark-testing-base isn't possible. spark-testing-base doesn't have an easy path forward. They're already building JAR files for different versions of Scala / Spark. Are they supposed to create a four dimensional build matrix for all combinations of Scala / Spark / Scalatest / project versions?</p> <p>Part of the reason I authored the dependency-free spark-fast-tests library was to avoid depending on Scalatest. Avoid libraries with dependencies whenever possible.</p> <p>Sidenote: Scalatest is an important part of the Scala community and I'm grateful for all they've contributed.</p>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#scala-ecosystem-isnt-very-stable","title":"Scala ecosystem isn't very stable","text":"<p>Scala minor version bumps can introduce breaking changes to your code. Scala 2.11 code may not work with Scala 2.12.</p> <p>Lots of Scala libraries use the SBT build tool, which also changes a lot.</p> <p>Scala runs on the JVM, which is also changing.</p> <p>Maintaining a Scala project can be quite burdensome. I highly recommend limiting dependencies and leveraging dependency-free libraries when working with Scala.</p>"},{"location":"apache-spark/upgrate-to-spark-3-scala-2-12/#best-upgrade-path","title":"Best upgrade path","text":"<p>Upgrade your Spark application to Spark 2.4.5 and cross compile it with Scala 2.11 or 2.12. The Scala 2.12 JAR files will work for Spark 3 and the Scala 2.11 JAR files will work with Spark 2.</p> <p>Transition some of your production workflows to Spark 3 and make sure everything is working properly.</p> <p>After some jobs are running on Spark 3 without problems, then get ready to make the full switch. Upgrade the Scala version to 2.12 and the Spark version to 3.0.1 in your project and remove the cross compile code.</p> <p>See the frameless example of cross compiling and then cutting Spark 2/Scala 2.11:</p> <p></p> <p>Spark 3 only works with Scala 2.12, so you can't cross compile once your project is using Spark 3. See the Spark Maven page with the available versions:</p> <p></p> <p>Before beginning the upgrade process, you might want to do a full audit of all your project dependencies (including the transitive dependencies). Limiting your project dependencies will make it easier to maintain your project going forward.</p>"},{"location":"apache-spark/using-the-console/","title":"How to use the Spark Shell (REPL)","text":"<p>The Spark console is a great way to run Spark code on your local machine.</p> <p>You can easily create a DataFrame and play around with code in the Spark console to avoid spinning up remote servers that cost money!</p>"},{"location":"apache-spark/using-the-console/#starting-the-console","title":"Starting the console","text":"<p>Download Spark and run the <code>spark-shell</code> executable command to start the Spark console. Consoles are also known as read-eval-print loops (REPL).</p> <p>I store my Spark versions in the <code>~/Documents/spark</code> directory, so I can start my Spark shell with this command.</p> <pre><code>bash ~/Documents/spark/spark-2.3.0-bin-hadoop2.7/bin/spark-shell\n</code></pre>"},{"location":"apache-spark/using-the-console/#important-variables-accessible-in-the-console","title":"Important variables accessible in the console","text":"<p>The Spark console creates a <code>sc</code> variable to access the <code>SparkContext</code> and a <code>spark</code> variable to access the <code>SparkSession</code>.</p> <p>You can use the <code>spark</code> variable to read a CSV file on your local machine into a DataFrame.</p> <pre><code>val df = spark.read.csv(\"/Users/powers/Documents/tmp/data/silly_file.csv\")\n</code></pre> <p>You can use the <code>sc</code> variable to convert a sequence of <code>Row</code> objects into a RDD:</p> <pre><code>import org.apache.spark.sql.Row\n\nsc.parallelize(Seq(Row(1, 2, 3)))\n</code></pre> <p>The Spark console automatically runs <code>import spark.implicits._</code> when it starts, so you have access to handy methods like <code>toDF()</code> and the shorthand <code>$</code> syntax to create column objects. We can easily create a column object like this: <code>$\"some_column_name\"</code>.</p>"},{"location":"apache-spark/using-the-console/#console-commands","title":"Console commands","text":"<p>The <code>:quit</code> command stops the console.</p> <p>The <code>:paste</code> lets the user add multiple lines of code at once. Here's an example:</p> <pre><code>scala&gt; :paste\n// Entering paste mode (ctrl-D to finish)\n\nval y = 5\nval x = 10\nx + y\n\n// Exiting paste mode, now interpreting.\n\ny: Int = 5\nx: Int = 10\nres8: Int = 15\n</code></pre> <p>The <code>:help</code> command lists all the available console commands. Here's a full list of all the console commands.</p> <pre><code>scala&gt; :help\nAll commands can be abbreviated, e.g., :he instead of :help.\n:edit &lt;id&gt;|&lt;line&gt;        edit history\n:help [command]          print this summary or command-specific help\n:history [num]           show the history (optional num is commands to show)\n:h? &lt;string&gt;             search the history\n:imports [name name ...] show import history, identifying sources of names\n:implicits [-v]          show the implicits in scope\n:javap &lt;path|class&gt;      disassemble a file or class name\n:line &lt;id&gt;|&lt;line&gt;        place line(s) at the end of history\n:load &lt;path&gt;             interpret lines in a file\n:paste [-raw] [path]     enter paste mode or paste a file\n:power                   enable power user mode\n:quit                    exit the interpreter\n:replay [options]        reset the repl and replay all previous commands\n:require &lt;path&gt;          add a jar to the classpath\n:reset [options]         reset the repl to its initial state, forgetting all session entries\n:save &lt;path&gt;             save replayable session to a file\n:sh &lt;command line&gt;       run a shell command (result is implicitly =&gt; List[String])\n:settings &lt;options&gt;      update compiler options, if possible; see reset\n:silent                  disable/enable automatic printing of results\n:type [-v] &lt;expr&gt;        display the type of an expression without evaluating it\n:kind [-v] &lt;expr&gt;        display the kind of expression's type\n:warnings                show the suppressed warnings from the most recent line which had any\n</code></pre> <p>This Stackoverflow answer contains a good description of the available console commands.</p>"},{"location":"apache-spark/using-the-console/#starting-the-console-with-a-jar-file","title":"Starting the console with a JAR file","text":"<p>The Spark console can be initiated with a JAR files as follows:</p> <pre><code>bash ~/Documents/spark/spark-2.3.0-bin-hadoop2.7/bin/spark-shell --jars ~/Downloads/spark-daria-2.3.0_0.24.0.jar\n</code></pre> <p>You can download the spark-daria JAR file on this release page if you'd like to try for yourself.</p> <p>Let's access the <code>EtlDefinition</code> class in the console to make sure that the spark-daria namespace was successfully added to the console.</p> <pre><code>scala&gt; com.github.mrpowers.spark.daria.sql.EtlDefinition\nres0: com.github.mrpowers.spark.daria.sql.EtlDefinition.type = EtlDefinition\n</code></pre> <p>You can add a JAR file to an existing console session with the <code>:require</code> command.</p> <pre><code>:require /Users/powers/Downloads/spark-daria-2.3.0_0.24.0.jar\n</code></pre>"},{"location":"apache-spark/using-the-console/#next-steps","title":"Next steps","text":"<p>The Spark console is a great way to play around with Spark code on your local machine.</p> <p>Try reading the Introdution to Spark DataFrames post and pasting in all the examples to a Spark console as you go. It'll be a great way to learn about the Spark console and DataFrames!</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/","title":"Calculating Week Start and Week End Dates with Spark","text":"<p>You can use native Spark functions to compute the beginning and end dates for a week, but the code isn't intuitive.</p> <p>This blog post demonstrates how to wrap the complex code in simple functions, so your code is readable.</p> <p>The Spark datetime functions aren't the best, but they're better than using UDFs with ugly Java code.</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#tldr","title":"TL;DR","text":"<p>Use the <code>beginningOfWeek</code> and <code>endOfWeek</code> functions defined in spark-daria to easily calculate these values.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.functions._\n\ndf\n  .withColumn(\"beginning_of_week\", beginningOfWeek(col(\"some_date\")))\n  .withColumn(\"end_of_week\", endOfWeek(col(\"some_date\")))\n</code></pre> <p>Here's example output:</p> <pre><code>+----------+-----------------+-----------+\n| some_date|beginning_of_week|end_of_week|\n+----------+-----------------+-----------+\n|2021-01-09|       2021-01-03| 2021-01-09|\n|2021-01-10|       2021-01-10| 2021-01-16|\n|2021-01-11|       2021-01-10| 2021-01-16|\n|2021-01-12|       2021-01-10| 2021-01-16|\n|2021-01-13|       2021-01-10| 2021-01-16|\n|2021-01-14|       2021-01-10| 2021-01-16|\n|2021-01-15|       2021-01-10| 2021-01-16|\n|2021-01-16|       2021-01-10| 2021-01-16|\n|2021-01-17|       2021-01-17| 2021-01-23|\n|2021-01-18|       2021-01-17| 2021-01-23|\n|      null|             null|       null|\n+----------+-----------------+-----------+\n</code></pre> <p>Spark considers Sunday to be the first day of the week and Saturday to be the last day of the week. You'll need to pass in an optional <code>lastDayOfWeek</code> argument if you'd like to use a custom week definition.</p> <p>Keep reading for more details.</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#dayofweek","title":"dayofweek","text":"<p>Let's start by analyzing Spark's internal representation of days, so you can understand the required inputs for the date functions.</p> <p>Spark has a <code>dayofweek</code> function that returns an integer from 1 to 7. 1 is for Sunday, 2 is for Monday, \u2026, and 7 is for Saturday.</p> <p>Suppose you have the following DataFrame with all the dates from a week in January 2021.</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2021-01-10|\n|2021-01-11|\n|2021-01-12|\n|2021-01-13|\n|2021-01-14|\n|2021-01-15|\n|2021-01-16|\n|      null|\n+----------+\n</code></pre> <p>Run <code>df.withColumn(\"dayofweek\", dayofweek(col(\"some_date\"))).show()</code> and observe the output.</p> <pre><code>+----------+---------+\n| some_date|dayofweek|\n+----------+---------+\n|2021-01-10|        1| // a Sunday\n|2021-01-11|        2|\n|2021-01-12|        3|\n|2021-01-13|        4|\n|2021-01-14|        5|\n|2021-01-15|        6|\n|2021-01-16|        7| // a Saturday\n|      null|     null|\n+----------+---------+\n</code></pre> <p>Notice that Spark considers Sunday to be the first day of the week and Saturday to be the last day of the week.</p> <p>The <code>dayofweek</code> function will come in handy when calculating the end of the week.</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#next_day","title":"<code>next_day</code>","text":"<p>Let's look at another DataFrame with a couple of dates.</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2021-01-11|\n|2021-01-23|\n+----------+\n</code></pre> <p>Use the <code>next_day</code> function to calculate the next Friday:</p> <pre><code>sourceDF\n  .withColumn(\"next_friday\", next_day(col(\"some_date\"), \"Friday\"))\n  .show()\n</code></pre> <p>Here are the results:</p> <pre><code>+----------+-----------+\n| some_date|next_friday|\n+----------+-----------+\n|2021-01-11| 2021-01-15|\n|2021-01-23| 2021-01-29|\n+----------+-----------+\n</code></pre> <p>Take a look at the calendar to see how the <code>next_day</code> function is working:</p> <p></p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#endofweek","title":"endOfWeek","text":"<p>Let's create an <code>endOfWeek</code> function that returns the last day of the week. Saturday should be the default week end date, but the function should take an optional parameter to allow for user customization.</p> <pre><code>def dayOfWeekStr(col: Column): Column = {\n  when(col.isNull, null)\n    .when(col === lit(1), lit(\"Sun\"))\n    .when(col === lit(2), lit(\"Mon\"))\n    .when(col === lit(3), lit(\"Tue\"))\n    .when(col === lit(4), lit(\"Wed\"))\n    .when(col === lit(5), lit(\"Thu\"))\n    .when(col === lit(6), lit(\"Fri\"))\n    .when(col === lit(7), lit(\"Sat\"))\n}\n\ndef endOfWeek(col: Column, lastDayOfWeek: String = \"Sat\"): Column = {\n  when(dayOfWeekStr(dayofweek(col)) === lit(lastDayOfWeek), col)\n    .otherwise(next_day(col, lastDayOfWeek))\n}\n</code></pre> <p>Suppose you have the following dates:</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2020-12-27|\n|2020-12-28|\n|2021-01-03|\n|2020-12-12|\n|      null|\n+----------+\n</code></pre> <p>Run <code>df.withColumn(\"week_end\", endOfWeek(col(\"some_date\"))).show()</code> to calculate the last day of the week:</p> <pre><code>+----------+----------+\n| some_date|  week_end|\n+----------+----------+\n|2020-12-27|2021-01-02|\n|2020-12-28|2021-01-02|\n|2021-01-03|2021-01-09|\n|2020-12-12|2020-12-12|\n|      null|      null|\n+----------+----------+\n</code></pre> <p>The last day of the week is Saturday by default.</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#beginningofweek","title":"beginningOfWeek","text":"<p>Now that we have an <code>endOfWeek</code> function, it's easy to calculate the beginning of the week:</p> <pre><code>def beginningOfWeek(col: Column, lastDayOfWeek: String = \"Sat\"): Column = {\n  val endOfWeek = endOfWeek(col, lastDayOfWeek)\n  date_sub(endOfWeek, 6)\n}\n</code></pre> <p>You take the end of the week and subtract six days to calculate the beginning of the week.</p> <p>Let's use the same dataset as above and calculate the beginning of the week, assuming the week end on Wednesday (so the weeks start on Thursday).</p> <pre><code>df\n  .withColumn(\"beginning_of_week\", beginningOfWeek(col(\"some_date\"), \"Wed\"))\n  .show()\n</code></pre> <p>Here's the output:</p> <pre><code>+----------+-----------------+\n| some_date|beginning_of_week|\n+----------+-----------------+\n|2020-12-27|       2020-12-24|\n|2020-12-28|       2020-12-24|\n|2021-01-03|       2020-12-31|\n|2020-12-12|       2020-12-10|\n|      null|             null|\n+----------+-----------------+\n</code></pre>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#weekofyear","title":"weekofyear","text":"<p>Spark's <code>weekofyear</code> function returns the week number as an integer that ranges from 1 to 53.</p> <p>Let's use this DataFrame as input data for the function:</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2021-01-01|\n|2021-01-02|\n|2021-01-09|\n|2021-01-10|\n|2021-01-11|\n|2021-01-12|\n|2021-01-13|\n|2021-01-14|\n|2021-01-15|\n|2021-01-16|\n|2021-01-17|\n|2021-01-18|\n|      null|\n+----------+\n</code></pre> <p>Run the <code>weekofyear</code> function with <code>weekofyear(col(\"some_date\"))</code> and observe the output:</p> <pre><code>+----------+----------+\n| some_date|weekofyear|\n+----------+----------+\n|2021-01-01|        53|\n|2021-01-02|        53|\n|2021-01-09|         1|\n|2021-01-10|         1|\n|2021-01-11|         2|\n|2021-01-12|         2|\n|2021-01-13|         2|\n|2021-01-14|         2|\n|2021-01-15|         2|\n|2021-01-16|         2|\n|2021-01-17|         2|\n|2021-01-18|         3|\n|      null|      null|\n+----------+----------+\n</code></pre> <p>Here's the unexpected function output:</p> <ul> <li>The first week of 2021 is grouped with the last week of 2020 (desirable for some calculations)</li> <li>Week starts with Monday and ends with Sunday (other Spark functions assume week starts with Sunday and ends with Saturday)</li> </ul> <p>The <code>weekofyear</code> function is good for high level weekly aggregations, but shouldn't be used if you need fine grained week control.</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#next-week-day","title":"Next week day","text":"<p>The weekdays are Monday, Tuesday, Wednesday, Thursday, and Friday.</p> <p>Spark has a <code>date_add</code> function that can be used to calculate the next day, but we'll need a little more logic for calculations the next weekday. It the day is Friday, the next weekday is the following Monday, not the next day (which would be a Saturday).</p> <p>Here's the code for calculating the next weekday:</p> <pre><code>def nextWeekDay(col: Column): Column = {\n  val d = dayofweek(col)\n  val friday = lit(6)\n  val saturday = lit(7)\n  when(col.isNull, null)\n    .when(d === friday || d === saturday, next_day(col,\"Mon\"))\n    .otherwise(date_add(col, 1))\n}\n</code></pre> <p>Notice how <code>lit(6)</code> and <code>lit(7)</code> are assigned to variables that make the code readable.</p> <p>Also notice how the if/else logic is modeled with when/otherwise in Spark.</p> <p>Let's use this dataset to observe the behavior of the <code>nextWeekday</code> function:</p> <pre><code>+----------+\n| some_date|\n+----------+\n|2021-01-10|\n|2021-01-11|\n|2021-01-12|\n|2021-01-13|\n|2021-01-14|\n|2021-01-15|\n|2021-01-16|\n|      null|\n+----------+\n</code></pre> <p>Run the function with <code>nextWeekday(col(\"some_date\"))</code> and observe the output:</p> <pre><code>+----------+------------+\n| some_date|next_weekday|\n+----------+------------+\n|2021-01-10|  2021-01-11| // for Sunday, next weekday is Monday\n|2021-01-11|  2021-01-12|\n|2021-01-12|  2021-01-13|\n|2021-01-13|  2021-01-14| // for Wednesday, next weekday is Thursday\n|2021-01-14|  2021-01-15|\n|2021-01-15|  2021-01-18| // for Friday, next weekday is Monday\n|2021-01-16|  2021-01-18|\n|      null|        null|\n+----------+------------+\n</code></pre> <p>As you can see in this answer, the code is more complex if the <code>java.time</code> libraries are used. Avoid the Java Time libraries and UDFs whenever possible.</p>"},{"location":"apache-spark/week-end-start-dayofweek-next-day/#conclusion","title":"Conclusion","text":"<p>The Spark native functions make it relatively easy to compute the end and beginning of the week.</p> <p>You should wrap generic datetime functions in helper methods so your code is more readable.</p> <p>Datetime related manipulation is particularily hard to read because of all the underlying assumptions (e.g. the integer 1 represents Sunday, the default week ends on Saturday).</p> <p>Hide all this messiness by simply using the spark-daria helper functions.</p>"},{"location":"aws/athena-spark-best-friends/","title":"AWS Athena and Apache Spark are Best Friends","text":"<p>Apache Spark makes it easy to build data lakes that are optimized for AWS Athena queries.</p> <p>This blog post will demonstrate that it's easy to follow the AWS Athena tuning tips with a tiny bit of Spark code - let's dive in!</p>"},{"location":"aws/athena-spark-best-friends/#creating-parquet-data-lake","title":"Creating Parquet Data Lake","text":"<p>We can convert a CSV data lake to a Parquet data lake with AWS Glue or we can write a couple lines of Spark code.</p> <pre><code>val df = spark.read.csv(\"/mnt/my-bucket/csv-lake/\")\nspark.write.parquet(\"/mnt/my-bucket/parquet-lake/\")\n</code></pre> <p>The pure Spark solution is less complicated than the AWS Glue solution if your company already has an environment setup to run Spark code (like Databricks). AWS Glue uses Spark under the hood, so they're both Spark solutions at the end of the day.</p>"},{"location":"aws/athena-spark-best-friends/#incrementally-updating-parquet-lake","title":"Incrementally updating Parquet lake","text":"<p>Suppose your CSV data lake is incrementally updated and you'd also like to incrementally update your Parquet data lake for Athena queries.</p> <p>Spark allows for incremental updates with Structured Streaming and Trigger.Once.</p> <p>Here's example code for an incremental update job.</p> <pre><code>import org.apache.spark.sql.streaming.Trigger\n\nval sDF = spark.readStream.format(\"csv\").load(\"/mnt/my-bucket/csv-lake/\")\n\nsDF\n  .writeStream\n  .trigger(Trigger.Once)\n  .format(\"parquet\")\n  .option(\"checkpointLocation\", \"/mnt/my-bucket/incremental-parquet-lake-checkpoint/\")\n  .start(\"/mnt/my-bucket/incremental-parquet-lake/\")\n</code></pre> <p>The checkpoint directory tracks the files that have already been loaded into the incremental Parquet data lake. Spark grabs the new CSV files and loads them into the Parquet data lake every time the job is run.</p>"},{"location":"aws/athena-spark-best-friends/#creating-a-hive-partitioned-lake","title":"Creating a hive partitioned lake","text":"<p>The #1 AWS Athena tuning tip is to partition your data.</p> <p>A partitioned data set limits the amount of data that Athena needs to scan for certain queries.</p> <p>The Spark <code>partitionBy</code> method makes it easy to partition data in disc with directory naming conventions that work with Athena (the standard Hive partition naming conventions).</p> <p>Read this blog post for more background on partitioning with Spark.</p> <p>Let's write some code that converts a standard Parquet data lake into a Parquet data lake that's partitioned in disc on the <code>country</code> column.</p> <pre><code>val df = spark.read.parquet(\"/mnt/my-bucket/parquet-lake/\")\n\ndf\n  .write\n  .partitionBy(\"country\")\n  .parquet(\"/mnt/my-bucket/partitioned_lake\")\n</code></pre> <p>Here's how to create a partitioned table in Athena.</p> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS this_is_awesome(\n  first_name STRING,\n  last_name STRING\n)\nPARTITIONED BY (country STRING)\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/partitioned_lake'\n</code></pre> <p>The partitioned table will make queries like this run faster:</p> <pre><code>select count(*) from this_is_awesome where country = 'Malaysia'\n</code></pre> <p>This blog post discusses how Athena works with partitioned data sources in more detail.</p>"},{"location":"aws/athena-spark-best-friends/#programmatically-creating-athena-tables","title":"Programmatically creating Athena tables","text":"<p>It can be really annoying to create AWS Athena tables for Spark data lakes, especially if there are a lot of columns. Athena should really be able to infer the schema from the Parquet metadata, but that\u2019s another rant.</p> <p>The spark-daria <code>printAthenaCreateTable()</code> method makes this easier by programmatically generating the Athena <code>CREATE TABLE</code> code from a Spark DataFrame.</p> <p>Suppose we have this DataFrame (<code>df</code>):</p> <pre><code>+--------+--------+---------+\n|    team|   sport|goals_for|\n+--------+--------+---------+\n|    jets|football|       45|\n|nacional|  soccer|       10|\n+--------+--------+---------+\n</code></pre> <p>Run this spark-daria code to generate the Athena <code>CREATE TABLE</code> query.</p> <pre><code>import com.github.mrpowers.spark.daria.sql.DataFrameHelpers\n\nDataFrameHelpers.printAthenaCreateTable(\n  df,\n  \"my_cool_athena_table\",\n  \"s3://my-bucket/extracts/people\"\n)\n</code></pre> <pre><code>CREATE TABLE IF NOT EXISTS my_cool_athena_table(\n  team STRING,\n  sport STRING,\n  goals_for INT\n)\nSTORED AS PARQUET\nLOCATION 's3://my-bucket/extracts/people'\n</code></pre> <p>You'll thank me for this helper method if you ever have to create a table with 100+ columns :wink:</p>"},{"location":"aws/athena-spark-best-friends/#conclusion","title":"Conclusion","text":"<p>It's easy to build data lakes that are optimized for AWS Athena queries with Spark.</p> <p>Spinning up a Spark cluster to run simple queries can be overkill. Athena is great for quick queries to explore a Parquet data lake.</p> <p>Athena and Spark are best friends - have fun using them both!</p>"},{"location":"books/self-publishing-programming-books/","title":"Self Publishing High Quality Programming Books","text":"<p>This post describes a workflow for self publishing programming books that readers will love.</p> <p>Writing a book seems like a daunting task, but it's less intimidating if each chapter is \"pre-published\" as a blog post. Validating user engagement based on blog post traffic metrics gives you confidence that your writing is valuable and you have what it takes to author a book.</p> <p>This post explains how you can break up the book writing process into manageable chunks (blog posts), collect them in a well organized package, and delight readers with a finished product they'll actually read end-to-end.</p> <p>Lots of developers don't read programming books cause they're structured like traditional school textbooks with big blocks of text. Self publishing is awesome because you can write a lightweight, easily readable book.</p> <p>Let's take a look at the process.</p>"},{"location":"books/self-publishing-programming-books/#validating-content-via-blog-posts","title":"Validating content via blog posts","text":"<p>A book is a collection of chapters, most of which can be separately published as blog posts.</p> <p>Publishing chapters as blog posts gives you valuable feedback on the quality of your content. Your blog posts should get ranked by search engines, accumulate backlinks, and have average time on page of at least 5 minutes.</p> <p>This post on broadcast joins with Spark is a good example of a blog post that's also a great book chapter. This blog is the #1 search result for \"spark broadcast join\", has tens of thousands of page views, and the average time spent on the page is 5.5 minutes. We know users value this content from objective metrics.</p> <p>The broadcast joins with Spark blog post met the prerequisites to be included as a chapter in the Beautiful Spark book.</p> <p>Don't bother writing a book till you're able to write blog posts that get good user engagement.</p> <p>A portfolio of popular blog posts is a great starting point for a book.</p>"},{"location":"books/self-publishing-programming-books/#book-organization","title":"Book organization","text":"<p>Books should have flow, so you can't simply organize the chapters by their popularity on your blog.</p> <p>The blog post popularity should be the primary prioritization factor, but the overall book layout should also make sense. You may need to add some chapters that aren't blog posts to fill gaps and avoid conceptual leaps.</p> <p>Book authors don't usually prioritize chapters based on metrics like blog post views or the top voted StackOverflow questions for a topic. Authors organize the table of contents based on their subject matter expertise, which isn't usually the best.</p> <p>It's better to prioritize content with metrics (top Stackoverflow questions or blog post page views) instead of \"personal feel\".</p>"},{"location":"books/self-publishing-programming-books/#most-important-content-first","title":"Most important content first","text":"<p>A small percentage of a language / framework API is what accounts for the majority of production code.</p> <p>Look at learning Spanish as an instructive example. The most popular 1,000 words account for 92% of spoken Spanish. There are 93,000 total Spanish words, but only a small fraction of them are used in conversations.</p> <p>If you want to learn Spanish, it's best to focus on the top 1,000 words.</p> <p>An alphabetical dictionary is of little use for language learners. What they need is a frequency dictionary that orders words based on how often they're used.</p> <p>Similarly, programming language learners should only study the most commonly used parts of the language API when starting. They should focus on getting \"conversational\" with a programming language before becoming \"fluent\".</p> <p>Book writers should help readers by only covering the API methods they need to become fluent.</p> <p>Authors often cover API methods that aren't widely used. They fall into the trap of writing chapters like \"Chapter 5: Arrays\", so they end up writing about the less important array methods, before other content that's more relevant for the reader.</p> <p>It's no wonder that books like these are often put down, never to be opened again. They're just unbearably boring.</p> <p>Books should be unified with a thesis that'll motivate the readers to power on, even through the difficult technical content.</p>"},{"location":"books/self-publishing-programming-books/#backing-into-a-thesis","title":"Backing into a thesis","text":"<p>Once you have an idea of how your book chapters will be organized, you're ready to tie them together with a unifying thesis.</p> <p>Programming books often do not have a thesis, so they're really boring. Don't fall into the trap of writing a book that's an \"API documentation narrative\".</p> <p>Here's the thesis for the Spark book I wrote: You should follow the design patterns outlined in this book to build production grade Spark applications that are maintainable, testable, and easily understandable, even by Scala newbies.</p> <p>The thesis makes the book more engaging, even for readers that don't agree. A book that argues a point is better than a book that simply describes a language without any opinion at all.</p>"},{"location":"books/self-publishing-programming-books/#special-sauce","title":"Special sauce","text":"<p>Your book should provide some deeper insights than what the audience can absorp from reading the blog post \"chapters\" individually.</p> <p>Luckily this special sauce is a natural byproduct of organizing the most important content on a subject and neatly tying it together with a unifying thesis.</p> <p>Just follow the process and the higher level insights will flow naturally.</p>"},{"location":"books/self-publishing-programming-books/#writing-style","title":"Writing style","text":"<p>Modern programming books should follow the same writing practices as blog posts.</p> <p>Blog posts should be written with basic English, short sentences, and one or two sentence paragraphs.</p> <p>Books generally use advanced language, longer sentences, and big paragraphs. Programming books that follow this writing style feel like university textbooks.</p> <p>Formal writing makes a book \"feel\" more authoritative, but readers don't care about that anymore. If your a topic matter expert, you can write casually, skip making an appeal to authority, and keep the respect of your audience.</p> <p>Attention spans are shrinking in the era of constant interruptions, so textbook style writing is even less desirable now.</p> <p>Just stick with \"blog like\" writing and avoid creating something that feels like it's been assigned from a university professor.</p>"},{"location":"books/self-publishing-programming-books/#limited-scope","title":"Limited scope","text":"<p>Programming books are usually too long, so readers feel like finishing them is an insurmountable task.</p> <p>Make your book short, like a novel, so readers have hope they can finish the book some day.</p> <p>Novels are usually 300-400 pages of light reading whereas programming books are often 500 pages+ of dense text.</p> <p>It's better to give readers a 200-250 page programming book that's relatively easy to read. Write two books if you have 500 pages worth of content.</p> <p>I separated Testing Spark Applications to a separate book to avoid making Beautiful Spark too long. Separate books allow for distinct thesis statements, which makes the book more cohesive.</p> <p>Try to make a book that's not intimidating and readers want to consume from cover-to-cover. Don't write a reference text with big sections that the user will skip.</p>"},{"location":"books/self-publishing-programming-books/#teaching-to-fish","title":"Teaching to fish","text":"<p>Give your readers enough fundamental insights so they're well prepared to learn additional details after reading your book. Your book doesn't need to teach everything, just enough so your readers are able to effectively learn more independently.</p> <p>Here's an example of how I selected the material to be covered in Beautiful Spark. Spark has four main APIs: RDDs, DataFrames, streaming, and machine learning. I focused exclusively on the DataFrame API because that gives the foundational knowledge readers need to learn about the streaming and machine learning APIs. The RDD API isn't commonly used. The narrow scope allows for a small book that's still enlightening.</p> <p>I'd write separate books for Spark streaming and machine learning if I was going to write more Spark content. Those books are comparatively less important because once readers have a good mastery of the Spark DataFrame API, they can figure out machine learning and streaming themselves.</p> <p>Teach readers core topics and help them get good enough to Google wisely and become self sufficient.</p>"},{"location":"books/self-publishing-programming-books/#marketing","title":"Marketing","text":"<p>As previously discussed, you should write chapters as blog posts that get traffic before writing a book.</p> <p>After your book is published, you can add links to your book in the blog posts to make sales. This makes it really easy for readers that like your writing style to get more content.</p>"},{"location":"books/self-publishing-programming-books/#underlying-motivations","title":"Underlying motivations","text":"<p>High quality programming books are a great way to give back to the programming community, make others more productive, and spread knowledge.</p> <p>Programming books are not a great way to get recognition or make money.</p> <p>Make sure your underlying motivations are aligned with realistic goals when writing the book.</p> <p>I am interested in making data professionals working on important problems more efficient (e.g. climate change, oceanography, and health care). Writing a book on a data processing technology is aligned with one of my life goals. I don't recommend writing a book unless it's aligned with your core values.</p>"},{"location":"books/self-publishing-programming-books/#conclusion","title":"Conclusion","text":"<p>You can self publish books that add a lot of value to other programmers.</p> <p>I wrote a book on a tiny sliver of the Spark API in an easily digestible manner. Many readers have told me that my book helped them \"get over the hump\" and finally learn Spark without feeling so intimidated by the technology.</p> <p>I encourage more developers to write books that fill niches so technology is easier to learn. I'd rather read a 200 page book to get started on a new technology than scour API doc pages and try to decipher the important topics from StackOverflow questions.</p> <p>More developers will start reading books if they're written with the lightweight style outlined in this post. Most programmers don't want to read \"reference manual\" style books.</p>"},{"location":"career/7-steps-reject-meeting-zoom/","title":"7 Steps for rejecting meeting invites","text":"<p>Meetings are the main way to kill your productivity as a creative professional. Two strategically timed meetings can eliminate your makers hours for an entire day.</p> <p>Rejecting meeting invites to protect your maker time and stay productive is surprisingly hard. Declining a meeting is perceived as rude.</p> <p>This article explains how to diplomatically reject meeting invites and prevent unnecessary meetings from showing up in your inbox.</p> <p>Meetings can be a huge problem, depending on the company. Look at this poor developer that spends 76% of their time in meetings!</p> <p>Let's start by examining the human feelings around meetings and why they're so hard to reject.</p>"},{"location":"career/7-steps-reject-meeting-zoom/#feelings","title":"Feelings","text":"<p>There is a professional social norm that you can send a colleague a calendar invitation whenever their calendar is open. Your colleague is expected to accept the invite and only decline if they have a good explanation.</p> <p>There are two types of meeting senders:</p> <ul> <li>Some send invites without express approval.</li> <li>Others feel like you need to ask a question like \"is it OK if I schedule a meeting with you next week\" before sending the invite.</li> </ul> <p>The norms around sending work meetings are much different than other human encounters like hanging out with friends for example. You wouldn't tell a friend \"I know you are off of work on Saturday, so we will go to a movie at 7PM\". Most \"meetings\" outside work require pre-approval. The working meeting norm is a curious exception to the typical dynamics of scheduling someone else's time.</p> <p>Traditional \"managers\" are especially likely to book meetings with direct reports without pre-approval. They think \"of course I can schedule meetings with my team\" without a second thought. Meeting invite recipients that prefer \"meeting pre-approval\" don't like getting random calendar invites.</p> <p>Creators are often in a position where they feel compelled to accept meetings they don't want to attend, even when they know the meeting will impact their productivity for the day.</p> <p>Controlling your calendar is important! Let's look at some tactics you can use to start saying no to meetings.</p>"},{"location":"career/7-steps-reject-meeting-zoom/#step-1-explain-makers-hours","title":"Step 1: Explain maker's hours","text":"<p>Paul Graham has a famous article on Maker's Schedule, Manager's Schedule where he talks about how meetings are more costly for makers than managers.</p> <p>Suppose you're a maker and need 3 hours of uninterrupted work to complete a task. You can't work on the task for 1.5 hours, stop, and work on it again for 1.5 hours to finish the job. You need 3 hours of uninterrupted time.</p> <p>If you have a 30 minute meeting at 10:00 and another 30 minute meeting at 3:00, then you don't have any 3 hour uninterrupted chunk to finish the task. Lots of modern professionals are bombarded with meetings and never have a 3 hour chunk, so this is an example of a task that will always gets kicked down the road.</p> <p>The type of work that requires 3 hours of focus is usually the most important. Meetings can get in your way from doing the most important work.</p> <p>Lots of programmers resort to the Paul Graham approach of \"I used to program from dinner till about 3 am every day, because at night no one could interrupt me\" as a work-around to meetings. This only applies if you're willing to be a workaholic.</p> <p>Creative professionals need to explain the concept of maker's hours to their managers. You should explicitly tell your manager what your maker's hours are, for example \"I am most productive from 9-12 and can't do any great work when that chunk is interrupted\".</p>"},{"location":"career/7-steps-reject-meeting-zoom/#step-2-meeting-prioritization","title":"Step 2: Meeting prioritization","text":"<p>Meetings have a strange way of bypassing normal work prioritization processes. It's ironic how a low priority task will get deprioritized, but a meeting on the low priority task may get scheduled smack in the middle of your core maker hours.</p> <p>You can fight back by putting meetings in your overall work queue. Suppose you have this queue of work:</p> <ul> <li>write book chapter</li> <li>write blog post</li> <li>review pull request</li> </ul> <p>When someone sends you a meeting invite, you can respond by saying, \"here is my work queue, where should we fit in the meeting relative to my other priorities.\"</p> <p>If you have a given set of priorities for a time period, you can even take this a step further. \"OK, here are my fourth quarter priorities, what should get deprioritized to accommodate for this meeting\".</p> <p>This may sound extreme, but remember the context. Many creative professionals feel like they can't get anything done during work hours because there are so many meetings.</p>"},{"location":"career/7-steps-reject-meeting-zoom/#step-3-categorize-meeting-type","title":"Step 3: Categorize meeting type","text":"<p>There are different types of meetings:</p> <ul> <li>group brainstorming</li> <li>status updates</li> <li>1:1</li> </ul> <p>All team members should transparently disclose their preferences for different types of meetings. Here are my preferences:</p> <ul> <li>I like group brainstorming, but only after a lot of async brainstorming has taken place first. I can't think through a new problem with a group if I haven't done my homework first.</li> <li>I dislike status update meetings because they encourage discussions on minutia. I like updates on big picture items, challenges, stuff that needs to be fixed. Something is wrong if I'm getting on a call every day to share small details with you.</li> <li>1:1 are good when they're scheduled on as as-needed basis. For me, a recurring 1:1 is a chore (like any other recurring meeting).</li> </ul>"},{"location":"career/7-steps-reject-meeting-zoom/#step-4-require-meeting-agenda","title":"Step 4: Require meeting agenda","text":"<p>Agenda-less meetings are notoriously unproductive. They usually involve a conversation and don't end with concrete action plan or next steps. They're the video-call equivalent of chit-chat.</p> <p>An agenda with action items and concrete next steps makes it more likely the meeting will be productive. It also forces the meeting requester to truly think about if a meeting is necessary rather than just throwing some time on your calendar.</p>"},{"location":"career/7-steps-reject-meeting-zoom/#step-5-flag-endless-topics","title":"Step 5: Flag endless topics","text":"<p>Organizations often have a few topics that just can't get solved, no matter how many meetings are held. Different teams keep creating meetings to address the issue, but they just can't finish the conversation. Endless meetings are frequently brainstorming sessions on forward looking business issues.</p> <p>You can flag the endless meeting topic by sending the team a message like \"we've met on topic XYZ multiple times and it seems like we're having a difficult time creating actionable next steps. Perhaps we could touch base again this issue in a few months or ask the leadership team to provide some top-down guidance?\".</p>"},{"location":"career/7-steps-reject-meeting-zoom/#step-6-question-mandatory-meeting-attendance","title":"Step 6: Question mandatory meeting attendance","text":"<p>Another meeting \"social norm\" is mandatory attendance by each member of the team for certain types of meetings.</p> <p>Some external stakeholders may be added as optional attendees, but all of the core team members are expected to attend.</p> <p>It's better to make meetings optional for all attendees.</p> <ul> <li>Smaller meetings are usually more productive.</li> <li>It allows creators to protect their makers hours.</li> <li>Provides a feedback mechanism for meeting creators. If lots of participants are rejecting your meeting invites, you may want to change your meeting invite practices (e.g. adding a meeting agenda or seeking pre-approval).</li> </ul> <p>It's not a big deal if a meeting invite it sent to 7 people and one team member wants to pass because they really want to conserve their maker hours and finish an important chunk of work. Organizations can let go of the artificial notion that everyone needs to attend all team meetings.</p>"},{"location":"career/7-steps-reject-meeting-zoom/#step-7-start-rejecting-invites","title":"Step 7: Start rejecting invites","text":"<p>Everyone on your team will be well aware of your aversion to meetings after you follow the previous steps.</p> <p>You're now ready to click the \"No\" button when you start getting meeting invites that you don't want to attend.</p> <p>Simply click \"No\" and add a small explanation of why I can't attend the meeting like \"I told an external stakeholder that I'd complete task XYZ in the next two weeks and need to focus, sorry!\".</p> <p>As you might imagine from reading this post, I don't like giving an explanation of why I don't want to attend a meeting. If a friend asks you \"do you want to go do the club this weekend?\", it's perfectly acceptable for you to respond \"no thank you\" without a detailed explanation. It's unfortunate that work dynamics for you to explain yourself, but like most social norms, it's just easier to follow societal definitions of \"social niceties\".</p>"},{"location":"career/7-steps-reject-meeting-zoom/#industry-changes-will-change-meeting-sending-norms","title":"Industry changes will change meeting sending norms","text":"<p>Meetings can plague an organization and severely limit the effectiveness of creative professionals.</p> <p>It wouldn't surprise me that many organizations fail because of their ineffective meeting practices. They have a great, talented team, but don't let their professionals to deep work and instead force them to constantly discuss minutia. These organizations only leverage a fraction of the creative potential of their people and that's why they can't build a truly innovative product that can succeed in the marketplace.</p>"},{"location":"career/7-steps-reject-meeting-zoom/#conclusion","title":"Conclusion","text":"<p>Follow the advice in this post and start rejecting meeting invites at your own peril. This isn't a sure-fire success plan. It's possible these tactics will back fire and cause you to get labelled as a problematic employee.</p> <p>I am at a place in my career where I am willing to take the risk to make a job I enjoy. It's impossible for me to be happy at work if I have meetings all day and can't focus. I like thinking, creating, building, and quantifying the value I add organizations with objective metrics. I'd rather get phased out of an organization that doesn't share my core values than sit in video calls all day that give me a massive \"Zoom hangover\" and leave me completely unmotivated.</p>"},{"location":"creator/how-content-creators-make-lots-money/","title":"Content creators making more than $50,000 a month","text":"<p>This post demonstrates how much money you can make as a content creator and contrasts the content creation and restaurant business models.</p> <p>Content creators can make a lot of money and enjoy a nice lifestyle:</p> <ul> <li>make money, even when you take days off</li> <li>location independence</li> <li>minimal logistics</li> <li>no boss or meetings</li> </ul> <p>Owning a restaurant is the opposite business model in a lot of regards:</p> <ul> <li>high employee turnover</li> <li>complicated logistics (perishable inventory)</li> <li>lots of competition</li> </ul> <p>This post will show you that it's less work and more profitable to start a content creation business instead of a logistics intensive business. Content creation isn't for everyone because most people aren't able to create engaging content. For those of you that are capable of creating content, it's probably a better career than brick-and-mortar.</p> <p>Let's start by looking at the core skills that talented content creators possess.</p>"},{"location":"creator/how-content-creators-make-lots-money/#creator-skill-set","title":"Creator skill set","text":"<p>The best content creators typically have the following combination of skills:</p> <ul> <li>likeable personality</li> <li>technical expertise (backed up by real world accomplishments)</li> <li>intuitive ability to make content that's engaging</li> </ul> <p>Most people don't have this skill set and will not succeed as content creators.</p> <p>The small amount of folks that can make good content, don't find it a struggle to create blog posts or videos. They're already subject experts, so talking about something they've already mastered is relatively easy.</p> <p>Let's take a look at some different content creators, starting with the ones that reveal how much they're making.</p>"},{"location":"creator/how-content-creators-make-lots-money/#real-estate-investor-scam-reporter","title":"Real estate investor / scam reporter","text":"<p>Spencer Corneila made $59,000 from his YouTube channel in November 2021. His YouTube channel has 335,000 subscribers and here's the video where he talks about its profitability.</p> <p>https://www.youtube.com/embed/i-5UiOoCJII</p> <p>Here are the main reasons Spencer is successful:</p> <ul> <li>He makes factual videos on controversial topics. He doesn't troll, but isn't scared to call out unethical behavior. Controversial topics get views!</li> <li>His videos are well organized and present the material in a linear, easy to follow manner.</li> <li>He makes videos that get a lot of traffic in the short term (e.g. reporting on current events), but also makes content that gets less views but is more aligned with the long term vision of his channel (exposing scammers).</li> <li>He exudes friendly, positive, confident energy.</li> </ul> <p>Spencer is a great example of how high quality content will get ranked by the YouTube algorithm and can make you a lot of money. His videos are monetized with YouTube ads.</p>"},{"location":"creator/how-content-creators-make-lots-money/#ex-amazon-content-creator","title":"Ex-Amazon content creator","text":"<p>Daniel Vassallo is a content creator with a strong Twitter following. Daniel publicly talked about why he quit his $500,000 a year Amazon job to work in content creation.</p> <p>Daniel shows his income from info products by month:</p> <p>https://twitter.com/dvassallo/status/1461786838524387330</p> <p>He makes up to $50,000 from info products some months.</p> <p>He made a graph that compares the income from SaaS products, info products, and freelancing.</p> <p>https://twitter.com/dvassallo/status/1262251147135340544?ref_src=twsrc%5Etfw</p> <p>Freelancing is stable, info products are risky, and SaaS products have long term potential.</p> <p>He encourages multiple income streams, which is a wonderful idea.</p> <p>https://twitter.com/dvassallo/status/1465783475512897545?ref_src=twsrc%5Etfw</p> <p>Daniel is popular on Twitter. His personality works better on Twitter compared to a platform like YouTube.</p> <p>You don't need to be great at multiple social to build an audience that's big enough to make good money as a content creator. YouTube works for Spencer. Twitter works for Daniel. Getting good at one marketing platform is enough.</p>"},{"location":"creator/how-content-creators-make-lots-money/#miss-excel","title":"Miss Excel","text":"<p>Kat Norton markets her Excel courses on Instagram and TikTok and is making up to $100,000 a day.</p> <p>https://twitter.com/rex_woodbury/status/1469727152333488132?ref_src=twsrc%5Etfw</p> <p>If she's not actively making a new course, then she only works 15 hours per week.</p> <p>Her personality is exceedingly bubbly and her story is inspiring:</p> <p>https://www.youtube.com/embed/adIN_hZD-6k</p> <p>Her content is clearly optimized for the TikTok format:</p> <p>https://www.youtube.com/embed/k3ees1F-mVc</p> <p>Miss Excel is a content creation genius, a true natural.</p>"},{"location":"creator/how-content-creators-make-lots-money/#professional-gamer","title":"Professional gamer","text":"<p>Hungrybox is a professional Super Smash Bros gamer with a solid YouTube following, here's an example video:</p> <p>https://www.youtube.com/embed/aTTDOabCSTg</p> <p>Hungrybox didn't spend a lot of time creating this YouTube video. He did a Twitch livestream and his editor made a highlight reel that's optimized for YouTube.</p> <p>Hungrybox is a top Smash pro, has won a lot of tournaments, and has been part of the competitive Smash scene since the beginning. He has a lot of authority in the Smash community and a hilarious personality, so it's not surprising he was able to build a Twitch/YouTube following.</p> <p>It's easy to make high quality YouTube content for video games, which are designed to be visually stimulating.</p> <p>Video game streaming is highly competitive and it's comparatively hard for streamers to make money and build a following. It's probably one of the worst areas for content creation. It was great back in the day when hardly anyone had video capture cards, so few people had the tech to make videos.</p> <p>Top pros will easily be able to build a following. Regular gamers should focus their content creation efforts in less competitive domains.</p>"},{"location":"creator/how-content-creators-make-lots-money/#louis-rossman","title":"Louis Rossman","text":"<p>Louis Rossman has built a massive YouTube following talking about his struggles running a brick-and-mortar business in New York City, buying a house, renting an office, and fighting for the right to repair technology products.</p> <p>He has great success as a content creator. It's ironic that most of his content discusses his struggles operating in the physical world. This video has great background on his life story:</p> <p>https://www.youtube.com/embed/Ch8vws8tjVI</p> <p>Some key bullet points from the talk:</p> <ul> <li>He couldn't afford to lease a decent store when starting his business because New York commercial rent prices are high</li> <li>Computer / cell phone repair is a low margin business</li> <li>He worked a day job from 10AM-8PM then worked on ebay side jobs till 2AM-4AM in the morning so he could pay a $10,000 store deposit. It's been a real struggle for him to build a business and survive.</li> </ul> <p>He regretfully mentions that he's suffering from imposter syndrome and \"the reason I had to do what I did is because I don't belong here\".</p> <p>He mentions that getting 400 five star reviews and having 12 employees that get paid 20-40% more than what other repair shops offer didn't make him feel like he belongs in NY either.</p> <p>It goes to show how even a smart, ambitious New Yorker can barely scrape by if they're in a highly competitive, low-margin business.</p> <p>Rossman is a talented content creator and probably would have been better off moving to a low cost area, focusing on content creation, and avoiding spending time on running a brick-and-mortar business in NYC.</p> <p>Let's take a look at the ramen business in Japan which provides another example of how difficult it is to operate a brick-and-mortar business with complicated logistics.</p>"},{"location":"creator/how-content-creators-make-lots-money/#ramen-business-in-japan","title":"Ramen business in Japan","text":"<p>Here's a great mini-documentary on owning a ramen shop in Japan:</p> <p>https://www.youtube.com/embed/EsC4J6MVbSc</p> <p>Here are the key points from the video:</p> <ul> <li>Owners work long hours</li> <li>It's easy to open a shop, but hard to keep it in business</li> <li>You can set up a shop from scratch for $270,000 or taking over an existing shop for ~$20,000</li> <li>Only 30% of shops survive longer than 3 years</li> <li>Owners work 6 days a week, 12 hours a day, and make around $15 an hour</li> </ul> <p>Some ramen broth can be cooked in big batches and stored, but other broths need to be made fresh, so the logistics of the business are always complicated.</p> <p>The biggest challenge facing ramen shop owners is finding employees. Owners are worried about giving employees too much responsibilities and teaching them the critical components of cooking ramen. They don't want their employees to steal their recipes and start a competing business.</p> <p>The ramen industry in Japan is fragmenting into owner operated shops (the opposite of consolidating and scaling). The industry used to be 60% national chains / 40% owner operated shops in 2000. Now, the ramen industry is 60% owner operated shops. From an economies of scale perspective, the industry is getting less efficient.</p> <p>There is a long history of ramen in Japan and many shop owners are passionate about ramen. That's why some are willing to sleep in their restaurant overnight and monitor the broth. Many owners aren't in the ramen business for the money.</p> <p>Ramen-like businesses are great for some, but horrible for people that don't want to pour their life into a business.</p>"},{"location":"creator/how-content-creators-make-lots-money/#conclusion","title":"Conclusion","text":"<p>Most people prefer the lifestyle afforded by content creation instead of the brick-and-mortar slog.</p> <p>Many individuals paradoxically choose to start a business that's a logistical mess / constant headache, even if that's not aligned with their ideal lifestyle.</p> <p>Content creation isn't for everyone, but it's a great option if you're a capable creator.</p> <p>If you're passionate about ramen and want to work 12-hour days, go ahead by all means. If you're looking to make money and have a great lifestyle, try other options.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/","title":"Writing Dask DataFrame to a Single CSV File","text":"<p>Dask DataFrames are composed of multiple partitions and are outputted as multiple files, one per partition, by default.</p> <p>This post explains the different approaches to write a Dask DataFrame to a single file and the strategy that works best for different situations.</p> <p>It also explains why it's generally best to avoid single files whenever possible.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/#multi-file-default-output","title":"Multi file default output","text":"<p>Create a Dask DataFrame with two partitions and output the DataFrame to disk to see multiple files are written by default.</p> <p>Start by creating the Dask DataFrame:</p> <pre><code>import pandas as pd\nfrom dask import dataframe as dd\n\npdf = pd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n)\ndf = dd.from_pandas(pdf, npartitions=2)\ndf.to_csv(\"./tmp/some_files\", index=False)\n</code></pre> <p>Here are the files that get outputted:</p> <pre><code>some_files/\n  0.part\n  1.part\n</code></pre> <p>Here are the contents of <code>some_files/0.part</code>:</p> <pre><code>num1,num2\n1,7\n2,8\n</code></pre> <p>Here are the contents of <code>some_files/1.part</code>:</p> <pre><code>num1,num2\n3,9\n4,10\n</code></pre> <p>Each partition in a Dask DataFrame is outputted to a separate file. Outputting multiple files is an intentional design decision. This lets Dask write to multiple files in parallel, which is faster than writing to a single file.</p> <p>Now let's look at how to write single files with Dask.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/#compute","title":"compute","text":"<p><code>compute</code> collects all the data in a Dask DataFrame to a single Pandas partition.</p> <p>Once all the data is collected to a single Pandas partition, you can write it out as a single file, just as you would with a normal Pandas DataFrame.</p> <p>Here's how to write the Dask DataFrame to a single file with <code>compute</code>:</p> <pre><code>df.compute().to_csv(\"./tmp/my_one_file.csv\", index=False)\n</code></pre> <p>Here are the contents of <code>tmp/my_one_file.csv</code>:</p> <pre><code>num1,num2\n1,7\n2,8\n3,9\n4,10\n</code></pre> <p>This approach only works if your data is small enough to fit in a single Pandas DataFrame. \"small enough\" depends the size of the computer's RAM.</p> <p>1 GB of data works fine on a machine with 16 GB of RAM. 10 GB of data might not work on a computer with 16 GB of RAM.</p> <p>The Pandas rule of thumb is \"have 5 to 10 times as much RAM as the size of your dataset\". This rule of thumb is from 2017 - let me know if it has been updated.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/#single_file","title":"<code>single_file</code>","text":"<p>You can also write out a single CSV file with the <code>single_file</code> flag:</p> <pre><code>df.to_csv(\"./tmp/my_single_file.csv\", index=False, single_file=True)\n</code></pre> <p>Here are the contents of <code>my_single_file.csv</code>:</p> <pre><code>num1,num2\n1,7\n2,8\n3,9\n4,10\n</code></pre> <p>The <code>single_file</code> approach has two huge caveats:</p> <ol> <li>it only works on local filesystem</li> <li>only works with certain file formats</li> </ol> <p><code>single_file</code> works locally, but you can't use it on a cloud object storage system like AWS S3. S3 objects are immutable and don't support the append operations that <code>single_file</code> uses to write data to a single file.</p> <p><code>single_file</code> wouldn't work for Parquet files either because Parquet files are immutable. You can't open a Parquet file and perform an append operation.</p> <p><code>single_file</code> is viable for localhost, CSV workflows, and doesn't work in all situations.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/#remote-cluster-approach","title":"Remote cluster approach","text":"<p>You can use a big computer in the cloud if you want to write a really big CSV file and your local machine isn't powerful enough. You can rent these computers and only pay for the time you're using the machine, down to the nearest second, so this can be a surprisingly affordable option.</p> <p>There are ec2 machines that have 768 GB of RAM to give you an idea of what kind of computing power you can access in the cloud.</p> <p>Computers with 128GB of RAM cost $1 per hour and computers with 768GB of RAM cost $6 per hour with on demand pricing (even cheaper on the spot market), so a big computation that only takes a few hours to run isn't that costly.</p> <p>The main challenge of this approach is the devops - getting the machine provisioned with Dask/Pandas and setting the right permissions.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/#limitations-of-single-file","title":"Limitations of single file","text":"<p>Single files are fine for small datasets, but not desirable for large datasets. It's faster to read, compute, and write big datasets that are spread across multiple files.</p> <p>Developers that are new to cluster computing sometimes gravitate to single files unnecessarily. Sometimes these devs just need to learn a little more Dask to understand that they actually don't need their data in a single file.</p> <p>When an experienced big data developer encounters a large file, the first think they usually do it split it up into smaller files. Big files are a common performance bottleneck in big data processing pipelines, especially if they're using a compression algorithm that's not splittable. Snappy compression splittable and that's why it's typically used in data analyses instead of gzip, which isn't splittable.</p> <p>You'll sometimes need to output a single file, but you should generally try to perform computations on multiple files, in parallel.</p>"},{"location":"dask/ouput-dataframe-single-csv-file/#conclusion","title":"Conclusion","text":"<p>Dask makes it easy to write out a single file. There are some limitations to writing out single files, but you'll see those same limitations with any big data processing framework.</p> <p>Cluster computing newbies are usually more comfortable when working with a single file. Once they learn how to leverage the power of the cluster, they're happy splitting up their workflows to multiple files and enjoying blazing fast computation runtimes.</p>"},{"location":"dask/read-csv-to-parquet/","title":"Reading CSVs and Writing Parquet files with Dask","text":"<p>Dask is a great technology for converting CSV files to the Parquet format. Pandas is good for converting a single CSV file to Parquet, but Dask is better when dealing with multiple files.</p> <p>Convering to Parquet is important and CSV files should generally be avoided in data products. Column file formats like Parquet allow for column pruning, so queries run a lot faster. If you have CSV files, it's best to start your analysis by first converting the files to the Parquet file format.</p>"},{"location":"dask/read-csv-to-parquet/#simple-example","title":"Simple example","text":"<p>Let's look at some code that converts two CSV files to two Parquet files.</p> <p>Suppose the <code>data/people/people1.csv</code> file contains the following data:</p> <pre><code>first_name,last_name\njose,cardona\njon,smith\n</code></pre> <p>The <code>data/people/people2.csv</code> file contains the following data</p> <pre><code>first_name,last_name\nluisa,montoya\nfederica,lugo\n</code></pre> <p>Here's the code that'll write out two Parquet files:</p> <pre><code>import dask.dataframe as dd\n\ndf = dd.read_csv('./data/people/*.csv')\ndf.to_parquet('./tmp/people_parquet2', write_index=False)\n</code></pre> <p>Here are the files that are output:</p> <pre><code>tmp/\n  people_parquet2/\n    part.0.parquet\n    part.1.parquet\n</code></pre> <p>Let's inspect the contents of the <code>tmp/people_parquet2/part.0.parquet</code> file:</p> <pre><code>import pandas as pd\npd.read_parquet('./tmp/people_parquet2/part.0.parquet')\n</code></pre> <pre><code>  first_name last_name\n0       jose   cardona\n1        jon     smith\n</code></pre> <p>The <code>part.0.parquet</code> file has the same data that was in the <code>people1.csv</code> file. Here's how the code was executed:</p> <ul> <li>The <code>people1.csv</code> and <code>people2.csv</code> files were read into a Dask DataFrame. A Dask DataFrame contains multiple Pandas DataFrames. Each Pandas DataFrame is referred to as a partition of the Dask DataFrame. In this example, the Dask DataFrame consisted of two Pandas DataFrames, one for each CSV file.</li> <li>Each partition in the Dask DataFrame was written out to disk in the Parquet file format. Dask writes out files in parallel, so both Parquet files are written simultaneously. This is one example of how parallel computing makes operations quick!</li> </ul>"},{"location":"dask/read-csv-to-parquet/#customizing-number-of-output-files","title":"Customizing number of output files","text":"<p>Here's code that'll read in the same two CSV files and write out four Parquet files:</p> <pre><code>df = dd.read_csv('./data/people/*.csv')\ndf = df.repartition(npartitions=4)\ndf.to_parquet('./tmp/people_parquet4', write_index=False)\n</code></pre> <p>Here are the files that are written out to disk:</p> <pre><code>tmp/\n  people_parquet4/\n    part.0.parquet\n    part.1.parquet\n    part.2.parquet\n    part.3.parquet\n</code></pre> <p>The repartition method shuffles the Dask DataFrame partitions and creates new partitions.</p> <p>In this example, the Dask DataFrame starts with two partitions and then is updated to contain four partitions (i.e. it starts with two Pandas DataFrames and the data is the then spread out across four Pandas DataFrames).</p> <p>Let's take a look at the contents of the <code>part.0.parquet</code> file:</p> <pre><code>import pandas as pd\npd.read_parquet('./tmp/people_parquet4/part.0.parquet')\n</code></pre> <pre><code>  first_name last_name\n0       jose   cardona\n</code></pre> <p>Each row of CSV data has been separated to a different partition. Partitions should generally be 100 MB and you can repartition large datasets with <code>repartition(partition_size=\"100MB\")</code>. Repartitioning datasets can be slow, so knowing when and how to repartition is a vital skill when working on distributed computing clusters.</p>"},{"location":"dask/read-csv-to-parquet/#other-technologies-to-read-write-files","title":"Other technologies to read / write files","text":"<p>CSV files can also be converted to Parquet files with PySpark and Koalas, as described in this post. Spark is a powerful tool for writing out lots of Parquet data, but it requires a JVM runtime and is harder to use than Dask.</p>"},{"location":"dask/read-csv-to-parquet/#next-steps","title":"Next steps","text":"<p>Dask makes it easy to convert CSV files to Parquet.</p> <p>Compared to other cluster computing frameworks, Dask also makes it easy to understand how computations are executed under the hood. Cluster computing often feels like a black box - it's hard to tell what computations your cluster is running.</p> <p>Dask is an awesome framework that's fun to play with. Many more Dask blog posts are coming soon!</p>"},{"location":"dask/read-delta-lake/","title":"Reading Delta Lakes into Dask DataFrames","text":"<p>This post explains how to read Delta Lakes into Dask DataFrames.\u00a0 It shows how you can leverage powerful data lake management features like time travel, versioned data, and schema evolution with Dask.</p> <p>Delta Lakes are normally written by Spark, but there are new projects like delta-rs that provide Rust, Ruby, and Python bindings for Delta lakes.\u00a0 delta-rs does not depend on Spark, so it doesn\u2019t require Java or other heavy dependencies.</p> <p>Let\u2019s start by writing out a Delta Lake with PySpark and then reading it into a Dask DataFrame with delta-rs.</p>"},{"location":"dask/read-delta-lake/#write-delta-lake-read-into-dask","title":"Write Delta Lake &amp; Read into Dask","text":"<p>Use PySpark to write a Delta Lake that has three rows of data.</p> <pre><code>from pyspark.sql import SparkSession\n\nfrom delta import *\n\nbuilder = (\n    SparkSession.builder.appName(\"dask-interop\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n)\n\ndata = [(\"jose\", 10), (\"li\", 12), (\"luisa\", 14)]\ndf = spark.createDataFrame(data, [\"name\", \"num\"])\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"resources/delta/1\")\n</code></pre> <p>The resources/delta/1 directory contains Parquet files and a transaction log with metadata.</p> <p>Read in the Delta table to a Dask DataFrame and print it to make sure it\u2019s working properly.</p> <pre><code>import dask.dataframe as dd\nfrom deltalake import DeltaTable\n\ndt = DeltaTable(\"resources/delta/1\")\nfilenames = [\"resources/delta/1/\" + f for f in dt.files()]\n\nddf = dd.read_parquet(filenames, engine=\"pyarrow\")\n\nprint(ddf.compute())\n\n    name  num\n0   jose   10\n0     li   12\n0  luisa   14\n</code></pre> <p>delta-rs makes it really easy to read Delta Lakes into Dask DataFrames.</p>"},{"location":"dask/read-delta-lake/#understanding-delta-transaction-log","title":"Understanding Delta Transaction Log","text":"<p>The best way to learn how Delta Lake works is by inspecting the filesystem and transaction log entries after performing transactions. Here are the files that are outputted when df.write.mode(\"overwrite\").format(\"delta\").save(\"resources/delta/1\") is run.</p> <pre><code>resources/delta/1/\n  _delta_log/\n    00000000000000000000.json\n  part-00000-193bf99f-66bf-4bbb-ab4c-868851bd5a24-c000.snappy.parquet\n  part-00002-b9fda751-0b6f-4f60-ae2c-94c48b5bcb6b-c000.snappy.parquet\n  part-00005-a9d642dd-0342-44c9-9a0d-f1cba095e34b-c000.snappy.parquet\n  part-00007-73f043c4-1f01-4c08-b3aa-2230a48b60d4-c000.snappy.parquet\n</code></pre> <p>Delta Lake consists of a transaction log (_delta_log) and Parquet files in the filesystem. Let\u2019s look at the contents of the first entry in the transaction log (00000000000000000000.json).</p> <pre><code>{\n   \"commitInfo\":{\n      \"timestamp\":1632491414394,\n      \"operation\":\"WRITE\",\n      \"operationParameters\":{\n         \"mode\":\"Overwrite\",\n         \"partitionBy\":\"[]\"\n      },\n      \"isBlindAppend\":false,\n      \"operationMetrics\":{\n         \"numFiles\":\"4\",\n         \"numOutputBytes\":\"2390\",\n         \"numOutputRows\":\"3\"\n      }\n   }\n}{\n   \"protocol\":{\n      \"minReaderVersion\":1,\n      \"minWriterVersion\":2\n   }\n}{\n   \"metaData\":{\n      \"id\":\"db102a08-5265-4f86-a281-dfc8cccacf0e\",\n      \"format\":{\n         \"provider\":\"parquet\",\n         \"options\":{\n\n         }\n      },\n      \"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"num\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n      \"partitionColumns\":[\n\n      ],\n      \"configuration\":{\n\n      },\n      \"createdTime\":1632491409910\n   }\n}{\n   \"add\":{\n      \"path\":\"part-00000-193bf99f-66bf-4bbb-ab4c-868851bd5a24-c000.snappy.parquet\",\n      \"partitionValues\":{\n\n      },\n      \"size\":377,\n      \"modificationTime\":1632491410000,\n      \"dataChange\":true\n   }\n}{\n   \"add\":{\n      \"path\":\"part-00002-b9fda751-0b6f-4f60-ae2c-94c48b5bcb6b-c000.snappy.parquet\",\n      \"partitionValues\":{\n\n      },\n      \"size\":674,\n      \"modificationTime\":1632491410000,\n      \"dataChange\":true\n   }\n}{\n   \"add\":{\n      \"path\":\"part-00005-a9d642dd-0342-44c9-9a0d-f1cba095e34b-c000.snappy.parquet\",\n      \"partitionValues\":{\n\n      },\n      \"size\":654,\n      \"modificationTime\":1632491410000,\n      \"dataChange\":true\n   }\n}{\n   \"add\":{\n      \"path\":\"part-00007-73f043c4-1f01-4c08-b3aa-2230a48b60d4-c000.snappy.parquet\",\n      \"partitionValues\":{\n\n      },\n      \"size\":685,\n      \"modificationTime\":1632491410000,\n      \"dataChange\":true\n   }\n}\n</code></pre> <p>The transaction log contains important information about the filesystem operations performed on the Delta Lake. This log entry tells us the following:</p> <ul> <li>Four Parquet files were added to the Delta Lake</li> <li>The sizes of all these Parquet files (one is empty)</li> <li>The schema of the Parquet file (so we can query the transaction log instead of opening Parquet files to figure out the schema)</li> <li>When these files were added to the lake, which gives us to opportunity to view the contents of the lake at different points in time</li> </ul> <p>Let\u2019s see how we can leverage delta-rs to time travel to different versions of the underlying Delta Lake with another example.</p>"},{"location":"dask/read-delta-lake/#time-travel","title":"Time travel","text":"<p>Let\u2019s create another Delta Lake with two write transactions so we can demonstrate time travel.</p> <p>Here\u2019s the PySpark code with two different write transactions:</p> <pre><code>data = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\ndf.write.format(\"delta\").save(\"resources/delta/2\")\n\ndata = [(\"d\", 4), (\"e\", 5), (\"f\", 6)]\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\ndf.write.mode(\"append\").format(\"delta\").save(\"resources/delta/2\")\n</code></pre> <p>The first transaction wrote this data (version 0 of the dataset):</p> letternumbera1b2c3 <p>The second transaction wrote this data (version 1 of the dataset):</p> letternumberd4e5f6 <p>Read the entire dataset into a Dask DataFrame and print the contents.</p> <pre><code>dt = DeltaTable(\"resources/delta/2\")\nfilenames = [\"resources/delta/2/\" + f for f in dt.files()]\n\nddf = dd.read_parquet(filenames, engine=\"pyarrow\")\nprint(ddf.compute())\n\n  letter  number\n0      d       4\n0      a       1\n0      e       5\n0      b       2\n0      c       3\n0      f       6\n</code></pre> <p>Delta will grab the latest version of the dataset by default.</p> <p>Now let\u2019s time travel back to version 0 of the dataset and view the contents of the data before the second transaction was executed.</p> <pre><code>dt = DeltaTable(\"resources/delta/2\")\ndt.load_version(0)\nfilenames = [\"resources/delta/2/\" + f for f in dt.files()]\n\nddf = dd.read_parquet(filenames, engine=\"pyarrow\")\nprint(ddf.compute())\n\n  letter  number\n0      a       1\n0      b       2\n0      c       3\n</code></pre> <p>Delta Lake\u2019s transaction log allows for out of the box time travel support.</p> <p>Data scientists love the ability to time travel.\u00a0 When models start giving different results, data scientists often struggle to understand why.\u00a0 Are the model results different because the data lake changed or because the model code changed?\u00a0 Or, what day was data added to the lake that caused the model to give different results?</p> <p>With time travel, data scientists can train their model with different versions of the data and pinpoint exactly when the results changed.</p> <p>Let\u2019s look at another Delta Lake feature that helps when columns are added to the data.</p>"},{"location":"dask/read-delta-lake/#schema-evolution","title":"Schema evolution","text":"<p>Vanilla Parquet data lakes require that all files have the same schema.\u00a0 If you add a Parquet file to a lake with a schema that doesn\u2019t match all the existing files, the entire lake becomes corrupted and unreadable.</p> <p>Suppose you have a lake with this data.</p> letternumbera1b2c3 <p>You\u2019d like to append this data to the lake:</p> letternumbercolord4rede5bluef6green <p>Here\u2019s how you can perform the initial write with PySpark.</p> <pre><code>data = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\ndf.write.format(\"delta\").save(\"resources/delta/3\")\n</code></pre> <p>Here\u2019s how to append additional data with a different schema.</p> <pre><code>data = [(\"d\", 4, \"red\"), (\"e\", 5, \"blue\"), (\"f\", 6, \"green\")]\ndf = spark.createDataFrame(data, [\"letter\", \"number\", \"color\"])\ndf.write.mode(\"append\").format(\"delta\").save(\"resources/delta/3\")\n</code></pre> <p>Read in the Delta Lake to a PySpark DataFrame and make sure it can gracefully handle the schema mismatch.</p> <pre><code>df = spark.read.format(\"delta\").option(\"mergeSchema\", \"true\").load(\"resources/delta/3\")\ndf.show()\n\n+------+------+-----+                                                           \n|letter|number|color|\n+------+------+-----+\n|     f|     6|green|\n|     e|     5| blue|\n|     d|     4|  red|\n|     b|     2| null|\n|     c|     3| null|\n|     a|     1| null|\n+------+------+-----+\n</code></pre> <p>Allowing schema mismatches saves you from having to rewrite your entire data lake every time you\u2019d like to add a new column.</p> <p>Delta Lake has lots of other schema options of course.\u00a0 By default it\u2019ll prevent data with mismatched schema from getting added to your lake.\u00a0 You need to set the spark.databricks.delta.schema.autoMerge.enabled configuration option to True to allow for this schema merging behavior.</p> <p>I\u2019ll be reaching out to the core delta-rs team to figure out how to leverage schema evolution in Dask and Pandas.</p>"},{"location":"dask/read-delta-lake/#conclusion","title":"Conclusion","text":"<p>delta-rs makes it easy to read Delta Lakes into Dask DataFrames and leverage some Delta features like time travel.</p> <p>Features are regularly being added to delta-rs which will in turn be accessible by Dask.</p>"},{"location":"dask/software-environments-conda-miniconda/","title":"Managing Dask Software Environments with Conda","text":"<p>This post shows you how to set up conda on your machine and explains why it\u2019s the best way to manage software environments for Dask projects.</p> <p>This post will cover the following topics:</p> <ol> <li>Install Miniconda</li> <li>Install dependencies in base environment</li> <li>Create separate software environments</li> <li>Useful conda commands</li> <li>Difference between conda and conda-forge</li> </ol> <p>Most data scientists have a hard time managing software environments and debugging issues.  They absolutely hate the trial and error process that\u2019s required to get a local environment properly set up.</p> <p>Pay attention to this post closely so you can better understand the process and train yourself how to effectively debug environment issues.</p>"},{"location":"dask/software-environments-conda-miniconda/#install-miniconda","title":"Install Miniconda","text":"<p>Go to the conda page to download the installer that suits your machine.  There are a plethora of options on the page.  It\u2019s easiest to pick the Latest Miniconda Installer Link for your operating system.</p> <p>I am using a Mac, so I use the Miniconda3 MaxOSX 64-bit pkg link.</p> <p>Open the downloaded package file and it\u2019ll walk you through the installation process.</p> <p></p> <p>Close out your Terminal window, reopen it, and you should be ready to run conda commands.  Make sure the <code>conda version</code> runs in your Terminal to verify the installation completed successfully.</p>"},{"location":"dask/software-environments-conda-miniconda/#install-dependencies-in-base","title":"Install dependencies in base","text":"<p>The default conda environment is called \u201cbase\u201d.</p> <p>You can run <code>conda list</code> to see the libraries that are installed in base.</p> <pre><code># packages in environment at /Users/powers/opt/miniconda3:\n#\n# Name                    Version                   Build  Channel\nbrotlipy                  0.7.0           py39h9ed2024_1003  \nca-certificates           2021.7.5             hecd8cb5_1  \ncertifi                   2021.5.30        py39hecd8cb5_0  \ncffi                      1.14.6           py39h2125817_0  \nchardet                   4.0.0           py39hecd8cb5_1003  \nconda                     4.10.3           py39hecd8cb5_0  \nconda-package-handling    1.7.3            py39h9ed2024_1  \ncryptography              3.4.7            py39h2fd3fbb_0  \nidna                      2.10               pyhd3eb1b0_0  \nlibcxx                    10.0.0                        1  \nlibffi                    3.3                  hb1e8313_2  \nncurses                   6.2                  h0a44026_1  \nopenssl                   1.1.1k               h9ed2024_0  \npip                       21.1.3           py39hecd8cb5_0  \npycosat                   0.6.3            py39h9ed2024_0  \npycparser                 2.20                       py_2  \npyopenssl                 20.0.1             pyhd3eb1b0_1  \npysocks                   1.7.1            py39hecd8cb5_0  \npython                    3.9.5                h88f2d9e_3  \npython.app                3                py39h9ed2024_0  \nreadline                  8.1                  h9ed2024_0  \nrequests                  2.25.1             pyhd3eb1b0_0  \nruamel_yaml               0.15.100         py39h9ed2024_0  \nsetuptools                52.0.0           py39hecd8cb5_0  \nsix                       1.16.0             pyhd3eb1b0_0  \nsqlite                    3.36.0               hce871da_0  \ntk                        8.6.10               hb0a8c7a_0  \ntqdm                      4.61.2             pyhd3eb1b0_1  \ntzdata                    2021a                h52ac0ba_0  \nurllib3                   1.26.6             pyhd3eb1b0_1  \nwheel                     0.36.2             pyhd3eb1b0_0  \nxz                        5.2.5                h1de35cc_0  \nyaml                      0.2.5                haf1e3a3_0  \nzlib                      1.2.11               h1de35cc_3\n</code></pre> <p>Run <code>conda install -c conda-forge dask</code> to install Dask in the base environment.</p> <p>This will install Dask and all of its transitive dependencies.</p> <p>Run <code>conda list</code> again and you\u2019ll see a ton of new dependencies in the environment, including pandas and Dask.</p> <p>Dask depends on other libraries, so when you install Dask conda will install both the Dask source code and the source code of all the libraries that Dask depends on (aka transitive dependencies).</p>"},{"location":"dask/software-environments-conda-miniconda/#difference-between-conda-and-conda-forge","title":"Difference between conda and conda-forge","text":"<p>Conda hosts 720+ official packages.  Community contributed packages are stored in conda-forge.</p> <p>conda-forge is referred to as a \u201cchannel\u201d.</p> <p>Let\u2019s inspect the Dask installation command we ran earlier: <code>conda install -c conda-forge dask</code></p> <p>The <code>-c conda-forge</code> part of the command is instructing conda to fetch the Dask dependency from the conda-forge channel.</p>"},{"location":"dask/software-environments-conda-miniconda/#create-separate-software-environments","title":"Create separate software environments","text":"<p>You can specify a list of dependencies in a YAML file and run a command to create a software environment with all of those dependencies (and their transitive dependencies).</p> <p>This workflow is more complicated, but easier to maintain in the long run and more reliable.  It also allows your teammates to easily recreate your environment, which is key for collaboration.</p> <p>You\u2019re likely to have different projects with different sets of dependencies on your computer.  Multiple environments allow you to switch the dependencies for the different projects you\u2019re working on.</p> <p>Take a look at the following YAML file with conda dependencies:</p> <pre><code>name: standard-coiled\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.9\n  - pandas\n  - dask[complete]\n  - pyarrow\n  - jupyter\n  - ipykernel\n  - s3fs\n  - coiled\n  - python-blosc\n  - lz4\n  - nb_conda\n  - jupyterlab\n  - dask-labextension\n</code></pre> <p>We can run a single command to create the standard-coiled environment specified in the YAML file.\u00a0 Clone the coiled-resource repository and change into the project root directory to run this command on your machine.</p> <p><code>conda env create -f envs/standard-coiled.yml</code></p> <p>You can activate this environment and use all the software you just installed by running <code>conda activate standard-coiled</code>.</p> <p>This is a completely different environment than the <code>base</code> environment.  You can switch back and forth between the two environments to easily switch between the different sets of dependencies.</p>"},{"location":"dask/software-environments-conda-miniconda/#useful-conda-commands","title":"Useful conda commands","text":"<p>You can run conda env list to see all the environments on your machine.</p> <pre><code># conda environments:\n#\nbase                     /Users/powers/opt/miniconda3\nstandard-coiled       *  /Users/powers/opt/miniconda3/envs/standard-coiled\n</code></pre> <p>The star next to <code>standard-coiled</code> means it\u2019s the active environment.</p> <p>Change back to the base environment with <code>conda activate base</code>.</p> <p>Delete the standard-coiled environment with <code>conda env remove --name standard-coiled</code>.</p> <p>If an environment gets in a weird state, you can easily delete it and recreate it from the YAML file. Easily recreating environments is a big advantage of creating environments from YAML files. YAML files can also be referred to in the future as a reminder of how the environment was originally created.</p>"},{"location":"dask/software-environments-conda-miniconda/#dependency-hell","title":"Dependency hell","text":"<p>It takes a while for conda to create an environment because it needs to perform dependency resolution and download the source code for the right libraries on your machine.</p> <p>Dependency resolution is when conda figures out the set of dependency versions that\u2019ll satisfy the version requirement of each dependency / transitive dependency for the environment.</p> <p>Dependency hell is an uncomfortable situation when the dependencies cannot be resolved. Luckily conda is a mature technology and is good at resolving the dependencies whenever possible, thus saving you from dependency hell.</p> <p>There are times when conda won\u2019t be able to resolve a build. That\u2019s when you should try relaxing version constraints or installing all packages at the same time. Conda has a harder time correctly working out the dependencies when they\u2019re installed one-by-one on the command line. It\u2019s best to put all the dependencies in a YAML file and install them all at once, so conda can perform the full dependency resolution process.</p>"},{"location":"dask/software-environments-conda-miniconda/#apple-m1-chip-gotcha","title":"Apple M1 Chip gotcha","text":"<p>If you are using an Apple computer with a M1 chip, you may want to try mambaforge instead of Miniconda. See this blog post on using Conda with Mac M1 machines for more details.</p> <p>Type in <code>conda info</code> and look at the results.</p> <pre><code>active environment : base\n    active env location : /Users/powers/opt/miniconda3\n            shell level : 1\n       user config file : /Users/powers/.condarc\n populated config files : \n          conda version : 4.10.3\n    conda-build version : not installed\n         python version : 3.9.5.final.0\n       virtual packages : __osx=10.16=0\n                          __unix=0=0\n                          __archspec=1=x86_64\n       base environment : /Users/powers/opt/miniconda3  (writable)\n      conda av data dir : /Users/powers/opt/miniconda3/etc/conda\n  conda av metadata url : None\n           channel URLs : https://repo.anaconda.com/pkgs/main/osx-64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/r/osx-64\n                          https://repo.anaconda.com/pkgs/r/noarch\n          package cache : /Users/powers/opt/miniconda3/pkgs\n                          /Users/powers/.conda/pkgs\n       envs directories : /Users/powers/opt/miniconda3/envs\n                          /Users/powers/.conda/envs\n               platform : osx-64\n             user-agent : conda/4.10.3 requests/2.25.1 CPython/3.9.5 Darwin/20.3.0 OSX/10.16\n                UID:GID : 501:20\n             netrc file : None\n           offline mode : False\n</code></pre> <p>The platform is <code>osx-64</code>, which is not optimized for Mac M1 chips. The libraries downloaded with this setup are run through an emulator, which can cause a performance drag.</p>"},{"location":"dask/software-environments-conda-miniconda/#conclusion","title":"Conclusion","text":"<p>Conda is a great package manager for Python data science because it can handle the dependencies that are difficult to install like xgboost and cuda.</p> <p>Data science libraries like xgboost contain a lot of C++ code and that\u2019s why they\u2019re hard for package managers to handle properly.</p> <p>Python has other package managers, like Poetry, that work fine for simple Python builds that rely on pure Python libraries. Data scientists don\u2019t have the luxury of working with pure Python libraries, so Poetry isn\u2019t a good option for data scientists. Conda is the best option for Python data workflows.</p> <p>This post taught you how to install conda, run basic commands, and manage multiple software environments. Keep practicing till you\u2019ve memorized all the commands in this post and conda comes naturally for you. Installing software is a common data science pain point and it\u2019s worth investing time studying the basics, so you\u2019re able to debug complex installation issues.</p> <p>Now is a good time to read the post on the Dask JupyterLab workflow that\u2019ll teach you a great conda-powered Dask development workflow.</p>"},{"location":"dask/write-csv-compression-filename/","title":"Different ways to write CSV files with Dask","text":"<p>This post explains how to write a Dask DataFrame to CSV files.</p> <p>You'll see how to write CSV files, customize the filename, change the compression, and append files to an existing lake.</p> <p>We'll also discuss best practices and gotchas that you need to watch out for when productionalizing your code.</p>"},{"location":"dask/write-csv-compression-filename/#simple-example","title":"Simple example","text":"<p>Let's create a dataset, write it out to disk, and observe the files that are created.</p> <p>Create a Dask DataFrame with two partitions and then write it out to disk with <code>to_csv</code>:</p> <pre><code>pdf = pd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n)\ndf = dd.from_pandas(pdf, npartitions=2)\ndf.to_csv(\"./tmp/csv_simple\")\n</code></pre> <p>Here are the files that are outputted:</p> <pre><code>csv_simple/\n  0.part\n  1.part\n</code></pre> <p>The Dask DataFrame is written as two files because it contains two partitions. Each partition in a Dask DataFrame is written in parallel so the operation is quick.</p> <p>Here are the contents of <code>0.part</code>:</p> <pre><code>,num1,num2\n0,1,7\n1,2,8\n</code></pre> <p>Dask writes the index by default. You can set a flag, so the index isn't written:</p> <pre><code>df.to_csv(\"./tmp/csv_simple\", index=False)\n</code></pre> <p>Now let's see how to customize the filename.</p>"},{"location":"dask/write-csv-compression-filename/#customize-filename","title":"Customize filename","text":"<p>Here's how to write the data out with a proper file extension:</p> <pre><code>df.to_csv(\"./tmp/csv_simple2/number-data-*.csv\", index=False)\n</code></pre> <p>Here are the files that are outputted.</p> <pre><code>csv_simple2/\n  number-data-0.csv\n  number-data-1.csv\n</code></pre> <p>You can even write the files out to a directory structure:</p> <pre><code>df.to_csv(\"./tmp/csv_simple3/batch-1/numbers-*.csv\", index=False)\n</code></pre> <p>Here's how the files are written:</p> <pre><code>csv_simple3/\n  batch-1/\n    numbers-0.csv\n    numbers-1.csv\n</code></pre>"},{"location":"dask/write-csv-compression-filename/#set-compression","title":"Set compression","text":"<p>Write out the files with gzip compression:</p> <pre><code>df.to_csv(\"./tmp/csv_compressed/hi-*.csv.gz\", index=False, compression=\"gzip\")\n</code></pre> <p>Here's how the files are outputted:</p> <pre><code>csv_compressed/\n  hi-0.csv.gz\n  hi-1.csv.gz\n</code></pre>"},{"location":"dask/write-csv-compression-filename/#watch-out","title":"Watch out!","text":"<p>The <code>to_csv</code> writer clobbers existing files in the event of a name conflict. Be careful whenever you're writing to a folder with existing data, especially if you're using the default file names.</p> <p>Let's write some data to a CSV directory.</p> <pre><code>pdf = pd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n)\ndf = dd.from_pandas(pdf, npartitions=2)\ndf.to_csv(\"./tmp/csv_danger\")\n</code></pre> <p>Here are the files on disk:</p> <pre><code>csv_danger/\n  0.part\n  1.part\n</code></pre> <p>Let's write another file to disk:</p> <pre><code>pdf = pd.DataFrame(\n    {\"firstname\": [\"cat\"], \"lastname\": [\"dog\"]},\n)\ndf = dd.from_pandas(pdf, npartitions=1)\ndf.to_csv(\"./tmp/csv_danger\")\n</code></pre> <p>The <code>csv_danger</code> folder still has two files, but <code>0.part</code> was clobbered.</p> <p>Here's the contents of <code>0.part</code>:</p> <pre><code>,firstname,lastname\n0,cat,dog\n</code></pre> <p>Here's the contents of <code>1.part</code>:</p> <pre><code>,num1,num2\n2,3,9\n3,4,10\n</code></pre> <p>Luckily, it's easy to add a UUID to the outputted filename to avoid accidental clobbering.</p>"},{"location":"dask/write-csv-compression-filename/#concurrency-safe-appends","title":"Concurrency safe appends","text":"<p>You can use a UUID in the filename to make sure none of the underlying files will be clobbered when performing an append operation.</p> <p>Here's a chunk of code that'll write out two files in the first batch and then write out another file in the second batch:</p> <pre><code>import uuid\n\npdf = pd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n)\ndf = dd.from_pandas(pdf, npartitions=2)\nbatch_id = uuid.uuid4()\ndf.to_csv(f\"./tmp/csv_appender/mything-{batch_id}-*.csv\", index=False)\n\npdf = pd.DataFrame(\n    {\"num1\": [11, 11], \"num2\": [12, 12]},\n)\ndf = dd.from_pandas(pdf, npartitions=2)\nbatch_id = uuid.uuid4()\ndf.to_csv(f\"./tmp/csv_appender/mything-{batch_id}-*.csv\", index=False)\n</code></pre> <p>Here's what gets written to the filesystem:</p> <pre><code>csv_appender/\n  mything-44a0201c-c381-4ac8-adb6-91ba82b60a92-0.csv\n  mything-44a0201c-c381-4ac8-adb6-91ba82b60a92-1.csv\n  mything-3ff08ac4-50e3-4b68-bbdb-611a8afe0d09-0.csv\n</code></pre> <p>You can tell which files are from each batch from the UUID that's embedded in the filename.</p> <p>Multiple different writers can append to the same directory and there won't be any name collisions with this design pattern.</p>"},{"location":"dask/write-csv-compression-filename/#writing-to-a-single-file","title":"Writing to a single file","text":"<p>The <code>single_file</code> flag makes it easy to write a single file:</p> <pre><code>df.to_csv(\"./tmp/my_single_file.csv\", index=False, single_file=True)\n</code></pre> <p>There are other techniques for writing single files that are more appropriate in other situations, see here for the full description.</p>"},{"location":"dask/write-csv-compression-filename/#having-fun","title":"Having fun","text":"<p>Dask even lets you supply a function argument to customize the number part of the output. Let's write two files as <code>10.part</code> and <code>12.part</code> instead of <code>0.part</code> and <code>1.part</code>.</p> <pre><code>my_fun = lambda x: str(x * 2 + 10)\npdf = pd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n)\ndf = dd.from_pandas(pdf, npartitions=2)\ndf.to_csv(\"./tmp/csv_even\", name_function=my_fun)\n</code></pre> <p>Here's what is written to the filesystem:</p> <pre><code>csv_even/\n  10.part\n  12.part\n</code></pre>"},{"location":"dask/write-csv-compression-filename/#when-to-use-csv","title":"When to use CSV","text":"<p>CSV is a bad file format for data analyses:</p> <ul> <li>quoting bugs are common</li> <li>cannot distinguish between the empty string and \"nothing\"</li> <li>doesn't allow for column pruning</li> <li>less compressible than a columnar file format</li> </ul> <p>CSV is human readable and that's really the only advantage.</p> <p>Dask offers great support for file formats that are more amenable for production grade data workflows, like Parquet.</p> <p>Parquet gives better performances and saves you from suffering with a lot weird bugs. Avoid CSV whenever possible!</p>"},{"location":"dask/write-csv-compression-filename/#conclusion","title":"Conclusion","text":"<p>Dask makes it easy to write CSV files and provides a lot of customization options.</p> <p>Only write CSVs when a human needs to actually open the file and inspect the contents.</p> <p>Whenever possible, use a better file format for data analyses like Parquet, Avro, or ORC.</p>"},{"location":"delta-lake/compact-small-files/","title":"Compacting Small Files in Delta Lakes","text":"<p>This post explains how to compact small files in Delta lakes with Spark.</p> <p>Data lakes can accumulate a lot of small files, especially when they're incrementally updated. Small files cause read operations to be slow.</p> <p>Joining small files into bigger files via compaction is an important data lake maintenance technique to keep reads fast.</p>"},{"location":"delta-lake/compact-small-files/#simple-example","title":"Simple example","text":"<p>Let's create a Delta data lake with 1,000 files and then compact the folder to only contain 10 files.</p> <p>Here's the code to create the Delta lake with 1,000 files:</p> <pre><code>df\n  .repartition(1000)\n  .write\n  .format(\"delta\")\n  .save(\"/some/path/data\")\n</code></pre> <p>The <code>_delta_log/00000000000000000000.json</code> file will contain 1,000 rows like this:</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-some-stuff-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":12345,\n    \"modificationTime\":1567528151000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>Let's compact the data to only contain 10 files.</p> <pre><code>val df = spark\n  .read\n  .format(\"delta\")\n  .load(\"/some/path/data\")\n\ndf\n  .repartition(10)\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(\"/some/path/data\")\n</code></pre> <p>The <code>/some/path/data</code> now contains 1,010 files - the 1,000 original uncompacted files and the 10 compacted files.</p> <p>The <code>_delta_log/00000000000000000001.json</code> file will contain 10 rows like this:</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-compacted-data-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":123456,\n    \"modificationTime\":1567528453000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>The <code>_delta_log/00000000000000000001.json</code> file will also contain 1,000 rows like this:</p> <pre><code>{\n  \"remove\":{\n    \"path\":\"part-00154-some-stuff-c000.snappy.parquet\",\n    \"deletionTimestamp\":1567528580934,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>You can run the <code>vacuum()</code> command to delete the old data files, so you don't have to pay to store the uncompacted data.</p>"},{"location":"delta-lake/compact-small-files/#datachangefalse","title":"dataChange=false","text":"<p>From Michael Armbrust:</p> <p>The Delta transaction protocol contains the ability to mark entries in the transaction log as dataChange=false indicating that they are only rearranging data that is already part of the table. This is powerful as it allows you to perform compaction and other read-performance optimizations, without breaking the ability to use a Delta table as a streaming source. We should expose this as a DataFrame writer option for overwrites.</p> <p>Delta lake currently sets <code>dataChange=true</code> when data is compacted, which is a breaking change for downstream streaming consumers. Delta lake will be updated to give users the option to set <code>dataChange=false</code> when files are compacted, so compaction isn't a breaking operation for downstream streaming customers.</p>"},{"location":"delta-lake/compact-small-files/#compacting-databricks-delta-lakes","title":"Compacting Databricks Delta lakes","text":"<p>Databricks Delta and Delta Lake are different technologies. You need to pay for Databricks Delta whereas Delta Lake is free.</p> <p>Databricks Delta lakes have an <code>OPTIMIZE</code> command that is not available for Delta Lakes and probably won't be in the future.</p> <p>From Michael Armbrust:</p> <p>At this point, there are no plans to open-source the OPTIMIZE command, as the actual implementation is pretty deeply tied to other functionality that is only present in Databricks Runtime.</p>"},{"location":"delta-lake/compact-small-files/#compacting-partitioned-delta-lakes","title":"Compacting partitioned Delta lakes","text":"<p>Here's how to compact the data in a single partition of a partitioned Delta lake (answer is described here).</p> <p>Suppose our data is stored in the <code>/some/path/data</code> folder and is partitioned by the <code>year</code> field. Further suppose that the <code>2019</code> directory contains 5 files and we'd like to compact it to one file.</p> <p>Here's how we can compact the <code>2019</code> partition.</p> <pre><code>val table = \"/some/path/data\"\nval partition = \"year = '2019'\"\nval numFiles = 1\n\nspark.read\n  .format(\"delta\")\n  .load(table)\n  .where(partition)\n  .repartition(numFiles)\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"replaceWhere\", partition)\n  .save(table)\n</code></pre> <p>After compacting, the <code>00000000000000000001.json</code> file will contain one row like this.</p> <pre><code>{\n  \"add\":{\n    \"path\":\"year=2019/part-00000-some-stuff.c000.snappy.parquet\",\n    \"partitionValues\":{\n      \"year\":\"2019\"\n    },\n    \"size\":57516,\n    \"modificationTime\":1567645788000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>The <code>00000000000000000001.json</code> file will also contain five rows like this.</p> <pre><code>{\n  \"remove\":{\n    \"path\":\"year=2019/part-uncompacted-data.c000.snappy.parquet\",\n    \"deletionTimestamp\":1567645789342,\n    \"dataChange\":true\n  }\n}\n</code></pre>"},{"location":"delta-lake/compact-small-files/#next-steps","title":"Next steps","text":"<p>Delta makes compaction easy and it's going to get even better when users have the option to set the dataChange flag to false, so compaction isn't breaking for streaming customers.</p> <p>Keep studying the transaction log to learn more about how Delta works!</p>"},{"location":"delta-lake/introduction-time-travel/","title":"Introduction to Delta Lake and Time Travel","text":"<p>Delta Lake is a wonderful technology that adds powerful features to Parquet data lakes.</p> <p>This blog post demonstrates how to create and incrementally update Delta lakes.</p> <p>We will learn how the Delta transaction log stores data lake metadata.</p> <p>Then we'll see how the transaction log allows us to time travel and explore our data at a given point in time.</p>"},{"location":"delta-lake/introduction-time-travel/#creating-a-delta-data-lake","title":"Creating a Delta data lake","text":"<p>Let's create a Delta lake from a CSV file with data on people. Here's the CSV data we'll use:</p> <pre><code>first_name,last_name,country\nmiguel,cordoba,colombia\nluisa,gomez,colombia\nli,li,china\nwang,wei,china\nhans,meyer,germany\nmia,schmidt,germany\n</code></pre> <p>Here's the code that'll read the CSV file into a DataFrame and write it out as a Delta data lake (all of the code in this post in stored in this GitHub repo).</p> <pre><code>val path = new java.io.File(\"./src/main/resources/person_data/\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/person_delta_lake/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Overwrite)\n  .save(outputPath)\n</code></pre> <p>The <code>person_data_lake</code> directory will contain these files:</p> <pre><code>person_data_lake/\n  part-00000-78f9c583-ea60-4962-af99-895f453dce23-c000.snappy.parquet\n  _delta_log/\n    00000000000000000000.json\n</code></pre> <p>The data is stored in a Parquet file and the metadata is stored in the <code>_delta_log/00000000000000000000.json</code> file.</p> <p>The JSON file contains information on the write transaction, schema of the data, and what file was added. Let's inspect the contents of the JSON file.</p> <pre><code>{\n  \"commitInfo\":{\n    \"timestamp\":1565119301357,\n    \"operation\":\"WRITE\",\n    \"operationParameters\":{\n      \"mode\":\"Overwrite\",\n      \"partitionBy\":\"[]\"\n    }\n  }\n}{\n  \"protocol\":{\n    \"minReaderVersion\":1,\n    \"minWriterVersion\":2\n  }\n}{\n  \"metaData\":{\n    \"id\":\"a3ca108e-3ba1-49dc-99a0-c9d29c8f1aec\",\n    \"format\":{\n      \"provider\":\"parquet\",\n      \"options\":{\n\n      }\n    },\n    \"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"first_name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"last_name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n    \"partitionColumns\":[\n\n    ],\n    \"configuration\":{\n\n    },\n    \"createdTime\":1565119298882\n  }\n}{\n  \"add\":{\n    \"path\":\"part-00000-78f9c583-ea60-4962-af99-895f453dce23-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":939,\n    \"modificationTime\":1565119299000,\n    \"dataChange\":true\n  }\n}\n</code></pre>"},{"location":"delta-lake/introduction-time-travel/#incrementally-updating-delta-data-lake","title":"Incrementally updating Delta data lake","text":"<p>Let's use some New York City taxi data to build and then incrementally update a Delta data lake.</p> <p>Here's the code that'll initially build the Delta data lake:</p> <pre><code>val outputPath = new java.io.File(\"./tmp/incremental_delta_lake/\").getCanonicalPath\n\nval p1 = new java.io.File(\"./src/main/resources/taxi_data/taxi1.csv\").getCanonicalPath\nval df1 = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(p1)\ndf1\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Overwrite)\n  .save(outputPath)\n</code></pre> <p>This code creates a Parquet file and a <code>_delta_log/00000000000000000000.json</code> file.</p> <pre><code>incremental_delta_lake/\n  part-00000-b38c0ad6-2e36-47a3-baa1-3f339950f931-c000.snappy.parquet\n  _delta_log/\n    00000000000000000000.json\n</code></pre> <p>Let's inspect the contents of the incremental Delta data lake.</p> <pre><code>spark\n  .read\n  .format(\"delta\")\n  .load(outputPath)\n  .select(\"passenger_count\", \"fare_amount\")\n  .show()\n\n+---------------+-----------+\n|passenger_count|fare_amount|\n+---------------+-----------+\n|              2|          4|\n|              1|        4.5|\n|              4|         12|\n|              2|       10.5|\n|              1|          5|\n+---------------+-----------+\n</code></pre> <p>The Delta lake contains 5 rows of data after the first load.</p> <p>Let's load another file into the Delta data lake with <code>SaveMode.Append</code>:</p> <pre><code>val p2 = new java.io.File(\"./src/main/resources/taxi_data/taxi2.csv\").getCanonicalPath\nval df2 = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(p2)\ndf2\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Append)\n  .save(outputPath)\n</code></pre> <p>This code creates a Parquet file and a <code>_delta_log/00000000000000000001.json</code> file. The <code>incremental_data_lake</code> contains these files now:</p> <pre><code>incremental_delta_lake/\n  part-00000-b38c0ad6-2e36-47a3-baa1-3f339950f931-c000.snappy.parquet\n  part-00000-fda221a5-1ec6-4320-bd1d-e767f7ee4799-c000.snappy.parquet\n  _delta_log/\n    00000000000000000000.json\n    00000000000000000001.json\n</code></pre> <p>The Delta lake contains 10 rows of data after the file is loaded:</p> <pre><code>spark\n  .read\n  .format(\"delta\")\n  .load(outputPath)\n  .select(\"passenger_count\", \"fare_amount\")\n  .show()\n\n+---------------+-----------+\n|passenger_count|fare_amount|\n+---------------+-----------+\n|              2|         52|\n|              3|       43.5|\n|              2|       24.5|\n|              1|         52|\n|              1|          4|\n|              2|          4|\n|              1|        4.5|\n|              4|         12|\n|              2|       10.5|\n|              1|          5|\n+---------------+-----------+\n</code></pre>"},{"location":"delta-lake/introduction-time-travel/#time-travel","title":"Time travel","text":"<p>Delta lets you time travel and explore the state of the data lake as of a given data load. Let's write a query to examine the incrementally updating Delta data lake after the first data load (ignoring the second data load).</p> <pre><code>spark\n  .read\n  .format(\"delta\")\n  .option(\"versionAsOf\", 0)\n  .load(outputPath)\n  .select(\"passenger_count\", \"fare_amount\")\n  .show()\n\n+---------------+-----------+\n|passenger_count|fare_amount|\n+---------------+-----------+\n|              2|          4|\n|              1|        4.5|\n|              4|         12|\n|              2|       10.5|\n|              1|          5|\n+---------------+-----------+\n</code></pre> <p>The <code>option(\"versionAsOf\", 0)</code> tells Delta to only grab the files in <code>_delta_log/00000000000000000000.json</code> and ignore the files in <code>_delta_log/00000000000000000001.json</code>.</p> <p>Let's say you're training a machine learning model off of a data lake and want to hold the data constant while experimenting. Delta lake makes it easy to use a single version of the data when you're training your model.</p> <p>You can easily access a full history of the Delta lake transaction log.</p> <pre><code>import io.delta.tables._\n\nval lakePath = new java.io.File(\"./tmp/incremental_delta_lake/\").getCanonicalPath\nval deltaTable = DeltaTable.forPath(spark, lakePath)\nval fullHistoryDF = deltaTable.history()\nfullHistoryDF.show()\n</code></pre> <pre><code>+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n|      2|2019-08-15 16:55:51|  null|    null|    WRITE|[mode -&gt; Append, ...|null|    null|     null|          1|          null|         true|\n|      1|2019-08-15 16:55:38|  null|    null|    WRITE|[mode -&gt; Append, ...|null|    null|     null|          0|          null|         true|\n|      0|2019-08-15 16:55:29|  null|    null|    WRITE|[mode -&gt; Overwrit...|null|    null|     null|       null|          null|        false|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n</code></pre> <p>The schema of the Delta history table is as follows:</p> <pre><code>fullHistoryDF.printSchema()\n\nroot\n |-- version: long (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- userId: string (nullable = true)\n |-- userName: string (nullable = true)\n |-- operation: string (nullable = true)\n |-- operationParameters: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- job: struct (nullable = true)\n |    |-- jobId: string (nullable = true)\n |    |-- jobName: string (nullable = true)\n |    |-- runId: string (nullable = true)\n |    |-- jobOwnerId: string (nullable = true)\n |    |-- triggerType: string (nullable = true)\n |-- notebook: struct (nullable = true)\n |    |-- notebookId: string (nullable = true)\n |-- clusterId: string (nullable = true)\n |-- readVersion: long (nullable = true)\n |-- isolationLevel: string (nullable = true)\n |-- isBlindAppend: boolean (nullable = true)\n</code></pre> <p>We can also grab a Delta table version by timestamp.</p> <pre><code>val lakePath = new java.io.File(\"./tmp/incremental_delta_lake/\").getCanonicalPath\nspark\n  .read\n  .format(\"delta\")\n  .option(\"timestampAsOf\", \"2019-08-15 16:55:38\")\n  .load(lakePath)\n</code></pre> <p>This is the same as grabbing version 1 of our Delta table (examine the transaction log history output to see why):</p> <pre><code>val lakePath = new java.io.File(\"./tmp/incremental_delta_lake/\").getCanonicalPath\nspark\n  .read\n  .format(\"delta\")\n  .option(\"versionAsOf\", 1)\n  .load(lakePath)\n  .show()\n</code></pre>"},{"location":"delta-lake/introduction-time-travel/#next-steps","title":"Next steps","text":"<p>This blog post just scratches the surface on the host of features offered by Delta Lake.</p> <p>In the coming blog posts we'll explore how to compact Delta lakes, schema evolution, schema enforcement, updates, deletes, and streaming.</p>"},{"location":"delta-lake/merge-update-upserts/","title":"Using Delta lake merge to update columns and perform upserts","text":"<p>This blog posts explains how to update a table column and perform upserts with the <code>merge</code> command.</p> <p>We explain how to use the <code>merge</code> command and what the command does to the filesystem under the hood.</p> <p>Parquet files are immutable, so <code>merge</code> provides an update-like interface, but doesn't actually mutate the underlying files. <code>merge</code> is slow on large datasets because Parquet files are immutable and the entire file needs to be rewritten, even if you only want to update a single column.</p> <p>All of the examples in this post generally follow the examples in the Delta update documentation.</p>"},{"location":"delta-lake/merge-update-upserts/#merge-example","title":"Merge example","text":"<p>Let's convert some CSV data into a Delta lake so we have some data to play with:</p> <pre><code>eventType,websitePage\nclick,homepage\nclck,about page\nmouseOver,logo\n</code></pre> <p>Here's the code to create the Delta lake:</p> <pre><code>val path = new java.io.File(\"./src/main/resources/event_data/\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/event_delta_lake/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .save(outputPath)\n</code></pre> <p>Let's take a look at what is stored in the <code>_delta_log/00000000000000000000.json</code> transaction log file.</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-f960ca7c-eff0-40d0-b753-1f99ea4ffb9f-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":673,\n    \"modificationTime\":1569079218000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>Let's display the contents of our data lake:</p> <pre><code>val path = new java.io.File(\"./tmp/event_delta_lake/\").getCanonicalPath\nval df = spark.read.format(\"delta\").load(path)\ndf.show()\n\n+---------+-----------+\n|eventType|websitePage|\n+---------+-----------+\n|    click|   homepage|\n|     clck| about page|\n|mouseOver|       logo|\n+---------+-----------+\n</code></pre> <p>The second row of data has a typo in the eventType field. It says \"clck\" instead of \"click\".</p> <p>Let's write a little code that'll update the typo.</p> <pre><code>val path = new java.io.File(\"./tmp/event_delta_lake/\").getCanonicalPath\nval deltaTable = DeltaTable.forPath(spark, path)\n\ndeltaTable.updateExpr(\n  \"eventType = 'clck'\",\n  Map(\"eventType\" -&gt; \"'click'\")\n)\n\ndeltaTable.update(\n  col(\"eventType\") === \"clck\",\n  Map(\"eventType\" -&gt; lit(\"click\"))\n)\n</code></pre> <p>We can check the contents of the Delta lake to confirm the spelling error has been fixed.</p> <pre><code>val path = new java.io.File(\"./tmp/event_delta_lake/\").getCanonicalPath\nval df = spark.read.format(\"delta\").load(path)\ndf.show()\n</code></pre> <pre><code>+---------+-----------+\n|eventType|websitePage|\n+---------+-----------+\n|    click|   homepage|\n|    click| about page|\n|mouseOver|       logo|\n+---------+-----------+\n</code></pre> <p>Parquet files are immutable\u2026 what does Delta do underneath the hood to fix the spelling error?</p> <p>Let's take a look at the <code>_delta_log/00000000000000000001.json</code> to figure out what's going on underneath the hood.</p> <pre><code>{\n  \"remove\":{\n    \"path\":\"part-00000-f960ca7c-eff0-40d0-b753-1f99ea4ffb9f-c000.snappy.parquet\",\n    \"deletionTimestamp\":1569079467662,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"add\":{\n    \"path\":\"part-00000-bcb431ea-f9d1-4399-9da5-3abfe5178d32-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":694,\n    \"modificationTime\":1569079467000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>The merge command writes a new file to the filesystem. Let's inspect the contents of the new file.</p> <pre><code>val path = new java.io.File(\"./tmp/event_delta_lake/part-00000-bcb431ea-f9d1-4399-9da5-3abfe5178d32-c000.snappy.parquet\").getCanonicalPath\nval df = spark.read.parquet(path)\ndf.show()\n</code></pre> <pre><code>+---------+-----------+\n|eventType|websitePage|\n+---------+-----------+\n|    click|   homepage|\n|    click| about page|\n|mouseOver|       logo|\n+---------+-----------+\n</code></pre> <p>Here are all the files in the filesystem after running the merge command.</p> <pre><code>event_delta_lake/\n  _delta_log/\n    00000000000000000000.json\n    00000000000000000001.json\n  part-00000-bcb431ea-f9d1-4399-9da5-3abfe5178d32-c000.snappy.parquet\n  part-00000-f960ca7c-eff0-40d0-b753-1f99ea4ffb9f-c000.snappy.parquet\n</code></pre> <p>So the merge command is writing all the data in an entirely new file. It's unfortunately not able to go into the existing Parquet file and only update the cells that need to be changed.</p> <p>Writing out all the data will make <code>merge</code> run a lot more slowly than you might expect.</p>"},{"location":"delta-lake/merge-update-upserts/#upsert-example","title":"upsert example","text":"<p>Let's take the following data set and build another little Delta lake:</p> <pre><code>date,eventId,data\n2019-01-01,4,take nap\n2019-02-05,8,play smash brothers\n2019-04-24,9,speak at spark summit\n</code></pre> <p>Here's the code to build the Delta lake:</p> <pre><code>val path = new java.io.File(\"./src/main/resources/upsert_event_data/original_data.csv\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/upsert_event_delta_lake/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .save(outputPath)\n</code></pre> <p>Let's inspect the starting state of the Delta lake:</p> <pre><code>val path = new java.io.File(\"./tmp/upsert_event_delta_lake/\").getCanonicalPath\nval df = spark.read.format(\"delta\").load(path)\ndf.show(false)\n</code></pre> <pre><code>+----------+-------+---------------------+\n|date      |eventId|data                 |\n+----------+-------+---------------------+\n|2019-01-01|4      |take nap             |\n|2019-02-05|8      |play smash brothers  |\n|2019-04-24|9      |speak at spark summit|\n+----------+-------+---------------------+\n</code></pre> <p>Let's update the Delta lake with a different phrasing of the events that's more \"mom-friendly\". We'll use this mom friendly data:</p> <pre><code>date,eventId,data\n2019-01-01,4,set goals\n2019-02-05,8,bond with nephew\n2019-08-10,66,think about my mommy\n</code></pre> <p>Events 4 and 8 will be rephrased with descriptions that'll make mom proud. Event 66 will be added to the lake to make mom feel good.</p> <p>Here's the code that'll perform the upsert:</p> <pre><code>val updatesPath = new java.io.File(\"./src/main/resources/upsert_event_data/mom_friendly_data.csv\").getCanonicalPath\nval updatesDF = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(updatesPath)\n\nval path = new java.io.File(\"./tmp/upsert_event_delta_lake/\").getCanonicalPath\n\nimport io.delta.tables._\n\nDeltaTable.forPath(spark, path)\n  .as(\"events\")\n  .merge(\n    updatesDF.as(\"updates\"),\n    \"events.eventId = updates.eventId\"\n  )\n  .whenMatched\n  .updateExpr(\n    Map(\"data\" -&gt; \"updates.data\")\n  )\n  .whenNotMatched\n  .insertExpr(\n    Map(\n      \"date\" -&gt; \"updates.date\",\n      \"eventId\" -&gt; \"updates.eventId\",\n      \"data\" -&gt; \"updates.data\")\n  )\n  .execute()\n</code></pre> <p>Let's take a look at the contents of the Delta lake after the upsert:</p> <pre><code>val path = new java.io.File(\"./tmp/upsert_event_delta_lake/\").getCanonicalPath\nval df = spark.read.format(\"delta\").load(path)\ndf.show(false)\n</code></pre> <pre><code>+----------+-------+---------------------+\n|date      |eventId|data                 |\n+----------+-------+---------------------+\n|2019-08-10|66     |think about my mommy |\n|2019-04-24|9      |speak at spark summit|\n|2019-02-05|8      |bond with nephew     |\n|2019-01-01|4      |set goals            |\n+----------+-------+---------------------+\n</code></pre>"},{"location":"delta-lake/merge-update-upserts/#transaction-log-for-upserts","title":"Transaction log for upserts","text":"<p>The <code>_delta_log/00000000000000000000.json</code> file contains a single entry for the single Parquet file that was added.</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-7eaa0d54-4dba-456a-ab80-b17f9aa7b583-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":900,\n    \"modificationTime\":1569177685000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>The <code>_delta_log/00000000000000000001.json</code> file reveals that upserts surprisingly add a lot of records to the transaction log.</p> <pre><code>{\n  \"remove\":{\n    \"path\":\"part-00000-7eaa0d54-4dba-456a-ab80-b17f9aa7b583-c000.snappy.parquet\",\n    \"deletionTimestamp\":1569177701037,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"add\":{\n    \"path\":\"part-00000-36aafda3-530d-4bd7-a29b-9c1716f18389-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":433,\n    \"modificationTime\":1569177698000,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"add\":{\n    \"path\":\"part-00026-fcb37eb4-165f-4402-beb3-82d3d56bfe0c-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":968,\n    \"modificationTime\":1569177700000,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"add\":{\n    \"path\":\"part-00139-eab3854f-4ed4-4856-8268-c89f0efe977c-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":1013,\n    \"modificationTime\":1569177700000,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"add\":{\n    \"path\":\"part-00166-0e9cddc8-9104-4c11-8b7f-44a6441a95fb-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":905,\n    \"modificationTime\":1569177700000,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"add\":{\n    \"path\":\"part-00178-147c78fa-dad2-4a1c-a4c5-65a1a647a41e-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":1013,\n    \"modificationTime\":1569177701000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>Let's create a little helper method that'll let us easily inspect the content of these parquet files:</p> <pre><code>def displayEventParquetFile(filename: String): Unit = {\n  val path = new java.io.File(s\"./tmp/upsert_event_delta_lake/$filename.snappy.parquet\").getCanonicalPath\n  val df = spark.read.parquet(path)\n  df.show(false)\n}\n</code></pre> <pre><code>UpsertEventProcessor.displayEventParquetFile(\"part-00000-36aafda3-530d-4bd7-a29b-9c1716f18389-c000\")\n\n+----+-------+----+\n|date|eventId|data|\n+----+-------+----+\n+----+-------+----+\n\nUpsertEventProcessor.displayEventParquetFile(\"part-00026-fcb37eb4-165f-4402-beb3-82d3d56bfe0c-c000\")\n\n+----------+-------+----------------+\n|date      |eventId|data            |\n+----------+-------+----------------+\n|2019-02-05|8      |bond with nephew|\n+----------+-------+----------------+\n\nUpsertEventProcessor.displayEventParquetFile(\"part-00139-eab3854f-4ed4-4856-8268-c89f0efe977c-c000\")\n\n+----------+-------+---------------------+\n|date      |eventId|data                 |\n+----------+-------+---------------------+\n|2019-04-24|9      |speak at spark summit|\n+----------+-------+---------------------+\n\nUpsertEventProcessor.displayEventParquetFile(\"part-00166-0e9cddc8-9104-4c11-8b7f-44a6441a95fb-c000\")\n\n+----------+-------+---------+\n|date      |eventId|data     |\n+----------+-------+---------+\n|2019-01-01|4      |set goals|\n+----------+-------+---------+\n\nUpsertEventProcessor.displayEventParquetFile(\"part-00178-147c78fa-dad2-4a1c-a4c5-65a1a647a41e-c000\")\n\n+----------+-------+--------------------+\n|date      |eventId|data                |\n+----------+-------+--------------------+\n|2019-08-10|66     |think about my mommy|\n+----------+-------+--------------------+\n</code></pre> <p>This update code creates a surprising number of Parquet files. Will need to test this code on a bigger dataset to see if this strangeness is intentional.</p>"},{"location":"delta-lake/merge-update-upserts/#conclusion","title":"Conclusion","text":"<p>Parquet files are immutable and don't support updates.</p> <p>Delta lake provides <code>merge</code> statements to provide an update-like interface, but under the hood, these aren't real updates.</p> <p>Delta lake is simply rewriting the entire Parquet files. This'll make an upsert or update column statement on a large dataset quite slow.</p>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/","title":"Delta Lake schema enforcement and evolution with mergeSchema and overwriteSchema","text":"<p>Delta lakes prevent data with incompatible schema from being written, unlike Parquet lakes which allow for any data to get written.</p> <p>Let's demonstrate how Parquet allows for files with incompatible schemas to get written to the same data store. Then let's explore how Delta prevents incompatible data from getting written with schema enforcement.</p> <p>We'll finish with an explanation of schema evolution.</p>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/#parquet-allows-for-incompatible-schemas","title":"Parquet allows for incompatible schemas","text":"<p>Let's create a Parquet with <code>num1</code> and <code>num2</code> columns:</p> <p>We'll use the spark-daria createDF method to build DataFrames for these examples.</p> <pre><code>val df = spark.createDF(\n  List(\n    (1, 2),\n    (3, 4)\n  ), List(\n    (\"num1\", IntegerType, true),\n    (\"num2\", IntegerType, true)\n  )\n)\n\nval parquetPath = new java.io.File(\"./tmp/parquet_schema/\").getCanonicalPath\n\ndf.write.parquet(parquetPath)\n</code></pre> <p>Let's view the contents of the Parquet lake.</p> <pre><code>spark.read.parquet(parquetPath).show()\n\n+----+----+\n|num1|num2|\n+----+----+\n|   1|   2|\n|   3|   4|\n+----+----+\n</code></pre> <p>Let's create another Parquet file with only a <code>num2</code> column and append it to the same folder.</p> <pre><code>val df2 = spark.createDF(\n  List(\n    88,\n    99\n  ), List(\n    (\"num2\", IntegerType, true)\n  )\n)\n\ndf2.write.mode(\"append\").parquet(parquetPath)\n</code></pre> <p>Let's read the Parquet lake into a DataFrame and view the output that's undesirable.</p> <pre><code>spark.read.parquet(parquetPath).show()\n\n+----+\n|num2|\n+----+\n|   2|\n|   4|\n|  88|\n|  99|\n+----+\n</code></pre> <p>We lost the <code>num1</code> column! <code>spark.read.parquet</code> is only returning a DataFrame with the <code>num2</code> column.</p> <p>This isn't ideal. Let's see if Delta provides a better result.</p>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/#delta-automatic-schema-updates","title":"Delta automatic schema updates","text":"<p>Let's create the same <code>df</code> as earlier and write out a Delta data lake.</p> <pre><code>val df = spark.createDF(\n  List(\n    (1, 2),\n    (3, 4)\n  ), List(\n    (\"num1\", IntegerType, true),\n    (\"num2\", IntegerType, true)\n  )\n)\n\nval deltaPath = new java.io.File(\"./tmp/schema_example/\").getCanonicalPath\n\ndf.write.format(\"delta\").save(deltaPath)\n</code></pre> <p>The Delta table starts with two columns, as expected:</p> <pre><code>spark.read.format(\"delta\").load(deltaPath).show()\n\n+----+----+\n|num1|num2|\n+----+----+\n|   1|   2|\n|   3|   4|\n+----+----+\n</code></pre> <p>Let's append a file with only the <code>num1</code> column to the Delta lake and see how Delta handles the schema mismatch.</p> <pre><code>val df2 = spark.createDF(\n  List(\n    88,\n    99\n  ), List(\n    (\"num1\", IntegerType, true)\n  )\n)\n\ndf2.write.format(\"delta\").mode(\"append\").save(deltaPath)\n</code></pre> <p>Delta gracefully fills in missing column values with <code>nulls</code>.</p> <pre><code>spark.read.format(\"delta\").load(deltaPath).show()\n\n+----+----+\n|num1|num2|\n+----+----+\n|   1|   2|\n|   3|   4|\n|  88|null|\n|  99|null|\n+----+----+\n</code></pre> <p>Let's append a DataFrame that only has a <code>num2</code> column to make sure Delta also handles that case gracefully.</p> <pre><code>val df3 = spark.createDF(\n  List(\n    101,\n    102\n  ), List(\n    (\"num2\", IntegerType, true)\n  )\n)\n\ndf3.write.format(\"delta\").mode(\"append\").save(deltaPath)\n</code></pre> <p>We can see Delta gracefully populates the <code>num2</code> values and nulls out the <code>num1</code> values.</p> <pre><code>spark.read.format(\"delta\").load(deltaPath).show()\n\n+----+----+\n|num1|num2|\n+----+----+\n|   1|   2|\n|   3|   4|\n|  88|null|\n|  99|null|\n|null| 101|\n|null| 102|\n+----+----+\n</code></pre> <p>Let's see if Delta can handle a DataFrame with <code>num1</code>, <code>num2</code>, and <code>num3</code> columns.</p> <pre><code>val df4 = spark.createDF(\n  List(\n    (7, 7, 7),\n    (8, 8, 8)\n  ), List(\n    (\"num1\", IntegerType, true),\n    (\"num2\", IntegerType, true),\n    (\"num3\", IntegerType, true)\n  )\n)\n\ndf4.write.format(\"delta\").mode(\"append\").save(deltaPath)\n</code></pre> <p>This causes the code to error out with the following message:</p> <pre><code>org.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table.\nTo enable schema migration, please set:\n'.option(\"mergeSchema\", \"true\")'.\n\nTable schema:\nroot\n-- num1: integer (nullable = true)\n-- num2: integer (nullable = true)\n\n\nData schema:\nroot\n-- num1: integer (nullable = true)\n-- num2: integer (nullable = true)\n-- num3: integer (nullable = true)\n</code></pre>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/#mergeschema","title":"mergeSchema","text":"<p>We can fix this by setting <code>mergeSchema</code> to <code>true</code>, as indicated by the error message.</p> <p>The codes works perfectly once the option is set:</p> <pre><code>df4\n  .write\n  .format(\"delta\")\n  .mode(\"append\")\n  .option(\"mergeSchema\", \"true\")\n  .save(deltaPath)\n\nspark.read.format(\"delta\").load(deltaPath).show()\n\n+----+----+----+\n|num1|num2|num3|\n+----+----+----+\n|   7|   7|   7|\n|   8|   8|   8|\n|   1|   2|null|\n|   3|   4|null|\n|null| 101|null|\n|null| 102|null|\n|  88|null|null|\n|  99|null|null|\n+----+----+----+\n</code></pre>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/#replace-table-schema","title":"Replace table schema","text":"<p><code>mergeSchema</code> will work when you append a file with a completely different schema, but it probably won't give you the result you're looking for.</p> <pre><code>val df5 = spark.createDF(\n  List(\n    (\"nice\", \"person\"),\n    (\"like\", \"madrid\")\n  ), List(\n    (\"word1\", StringType, true),\n    (\"word2\", StringType, true)\n  )\n)\n\ndf5\n  .write\n  .format(\"delta\")\n  .mode(\"append\")\n  .option(\"mergeSchema\", \"true\")\n  .save(deltaPath)\n</code></pre> <p><code>mergeSchema</code> appends two new columns to the DataFrame because the save mode was set to append.</p> <pre><code>spark.read.format(\"delta\").load(deltaPath).show()\n\n+----+----+----+-----+------+\n|num1|num2|num3|word1| word2|\n+----+----+----+-----+------+\n|   7|   7|   7| null|  null|\n|   8|   8|   8| null|  null|\n|   1|   2|null| null|  null|\n|   3|   4|null| null|  null|\n|null|null|null| nice|person|\n|null|null|null| like|madrid|\n|  88|null|null| null|  null|\n|  99|null|null| null|  null|\n|null| 101|null| null|  null|\n|null| 102|null| null|  null|\n+----+----+----+-----+------+\n</code></pre> <p>Let's see how <code>mergeSchema</code> behaves when using a completely different schema and setting the save mode to overwrite.</p> <pre><code>df5\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"mergeSchema\", \"true\")\n  .save(deltaPath)\n\nspark.read.format(\"delta\").load(deltaPath).show()\n\n+----+----+----+-----+------+\n|num1|num2|num3|word1| word2|\n+----+----+----+-----+------+\n|null|null|null| nice|person|\n|null|null|null| like|madrid|\n+----+----+----+-----+------+\n</code></pre> <p><code>mergeSchema</code> isn't the best when the schemas are completely different. It's better for incremental schema changes.</p>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/#overwriteschema","title":"overwriteSchema","text":"<p>Setting <code>overwriteSchema</code> to true will wipe out the old schema and let you create a completely new table.</p> <pre><code>df5\n  .write\n  .format(\"delta\")\n  .option(\"overwriteSchema\", \"true\")\n  .mode(\"overwrite\")\n  .save(deltaPath)\n</code></pre> <pre><code>spark.read.format(\"delta\").load(deltaPath).show()\n\n+-----+------+\n|word1| word2|\n+-----+------+\n| nice|person|\n| like|madrid|\n+-----+------+\n</code></pre>"},{"location":"delta-lake/schema-enforcement-evolution-mergeschema-overwriteschema/#conclusion","title":"Conclusion","text":"<p>Delta lakes offer powerful schema evolution features that are not available in Parquet lakes.</p> <p>Delta lakes also enforce schemas and make it less likely that a bad write will mess up your entire lake.</p> <p>Delta offers some great features that are simply not available in plain vanilla Parquet lakes.</p>"},{"location":"delta-lake/type-2-scd-upserts/","title":"Type 2 Slowly Changing Dimension Upserts with Delta Lake","text":"<p>This post explains how to perform type 2 upserts for slowly changing dimension tables with Delta Lake.</p> <p>We'll start out by covering the basics of type 2 SCDs and when they're advantageous.</p> <p>This post is inspired by the Databricks docs, but contains significant modifications and more context so the example is easier to follow.</p> <p>Delta lake upserts are challenging. You'll need to study this post carefully.</p>"},{"location":"delta-lake/type-2-scd-upserts/#type-2-scd-basics","title":"Type 2 SCD basics","text":"<p>Take a look at the following data on tech celebrities:</p> <pre><code>+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|     true|   1971-06-28|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n+--------+----------+------------+-----------+---------+-------------+----------+\n</code></pre> <p><code>personId</code>, <code>personName</code>, <code>country</code>, and <code>region</code> are traditional database columns.</p> <p><code>isCurrent</code>, <code>effectiveDate</code>, and <code>endDate</code> are columns to make this a type 2 SCD table.</p> <p>The type 2 SCD fields let you see the history of your data, not just the current state.</p> <p>It lets you run queries to find where Elon Musk currently currently lives and also where he lived in May 1993.</p>"},{"location":"delta-lake/type-2-scd-upserts/#upserts","title":"Upserts","text":"<p>Suppose you'd like to update your table with the following data:</p> <pre><code>+--------+----------+-------+--------+-------------+\n|personId|personName|country|  region|effectiveDate|\n+--------+----------+-------+--------+-------------+\n|       1| elon musk| canada|montreal|   1989-06-01|\n|       4|       dhh|     us| chicago|   2005-11-01|\n+--------+----------+-------+--------+-------------+\n</code></pre> <p>Elon Musk is in the original data table. DHH is not in the original table.</p> <p>You'd like to update the Elon Musk record and insert a new row for DHH. These update/insert operations are referred to as upserts.</p> <p>Here's the end table we'd like to create:</p> <pre><code>+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|    false|   1971-06-28|1989-06-01|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n|       1| elon musk|      canada|   montreal|     true|   1989-06-01|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       4|       dhh|          us|    chicago|     true|   2005-11-01|      null|\n+--------+----------+------------+-----------+---------+-------------+----------+\n</code></pre> <p>The Elon Musk South Africa row was updated to reflect that he lived there from 1971 till 1989. A new row was added for when Elon moved to Canada. We'd need to add additional rows for when he moved to California and Texas to make this table complete, but you get the idea.</p> <p>A new row was added for DHH. There wasn't any existing data for DHH, so this was a plain insert.</p> <p>The rest of this post explains how to make this upsert.</p>"},{"location":"delta-lake/type-2-scd-upserts/#create-delta-table","title":"Create Delta table","text":"<p>Let's create a Delta table with our example data:</p> <pre><code>val df = Seq(\n  (1, \"elon musk\", \"south africa\", \"pretoria\", true, \"1971-06-28\", null),\n  (2, \"jeff bezos\", \"us\", \"albuquerque\", true, \"1964-01-12\", null),\n  (3, \"bill gates\", \"us\", \"seattle\", false, \"1955-10-28\", \"1973-09-01\")\n).toDF(\"personId\", \"personName\", \"country\", \"region\", \"isCurrent\", \"effectiveDate\", \"endDate\")\n\nval path = os.pwd/\"tmp\"/\"tech_celebs\"\n\ndf\n  .write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(path.toString())\n\ndf.show()\n</code></pre> <pre><code>+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|     true|   1971-06-28|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n+--------+----------+------------+-----------+---------+-------------+----------+\n</code></pre> <p>The elegant <code>os.pwd</code> syntax is powered by os-lib, as explained in this post.</p>"},{"location":"delta-lake/type-2-scd-upserts/#build-update-table","title":"Build update table","text":"<p>Create another DataFrame with the update data so we can perform an upsert.</p> <pre><code>val updatesDF = Seq(\n  (1, \"elon musk\", \"canada\", \"montreal\", \"1989-06-01\"),\n  (4, \"dhh\", \"us\", \"chicago\", \"2005-11-01\")\n).toDF(\"personId\", \"personName\", \"country\", \"region\", \"effectiveDate\")\n\nupdatesDF.show()\n</code></pre> <pre><code>+--------+----------+-------+--------+-------------+\n|personId|personName|country|  region|effectiveDate|\n+--------+----------+-------+--------+-------------+\n|       1| elon musk| canada|montreal|   1989-06-01|\n|       4|       dhh|     us| chicago|   2005-11-01|\n+--------+----------+-------+--------+-------------+\n</code></pre>"},{"location":"delta-lake/type-2-scd-upserts/#build-staged-update-table","title":"Build staged update table","text":"<p>We'll need to modify the update table, so it's properly formatted for the upsert. We need three rows in the staged upsert table:</p> <ul> <li>Elon Musk update South Africa row</li> <li>Elon Must insert Canada row</li> <li>DHH insert Chicago row</li> </ul> <p>Delta uses Parquet files, which are immutable, so updates aren't performed in the traditional sense. Updates are really deletes then inserts. Study up on Delta basics if you're new to the technology.</p> <p>Let's compute the rows for existing people in the data that have new data. This is the existing Elon Musk row. Notice that the <code>mergeKey</code> is intentionally set to <code>null</code> in the following code.</p> <pre><code>val stagedPart1 = updatesDF\n  .as(\"updates\")\n  .join(techCelebsTable.toDF.as(\"tech_celebs\"), \"personId\")\n  .where(\"tech_celebs.isCurrent = true AND (updates.country &lt;&gt; tech_celebs.country OR updates.region &lt;&gt; tech_celebs.region)\")\n  .selectExpr(\"NULL as mergeKey\", \"updates.*\")\n\nstagedPart1.show()\n</code></pre> <pre><code>+--------+--------+----------+-------+--------+-------------+\n|mergeKey|personId|personName|country|  region|effectiveDate|\n+--------+--------+----------+-------+--------+-------------+\n|    null|       1| elon musk| canada|montreal|   1989-06-01|\n+--------+--------+----------+-------+--------+-------------+\n</code></pre> <p>Here are the other rows that need to be inserted.</p> <pre><code>val stagedPart2 = updatesDF.selectExpr(\"personId as mergeKey\", \"*\")\n\nstagedPart2.show()\n</code></pre> <pre><code>+--------+--------+----------+-------+--------+-------------+\n|mergeKey|personId|personName|country|  region|effectiveDate|\n+--------+--------+----------+-------+--------+-------------+\n|       1|       1| elon musk| canada|montreal|   1989-06-01|\n|       4|       4|       dhh|     us| chicago|   2005-11-01|\n+--------+--------+----------+-------+--------+-------------+\n</code></pre> <p>Create the staged update table by unioning the two DataFrames.</p> <pre><code>val stagedUpdates = stagedPart1.union(stagedPart2)\n\nstagedUpdates.show()\n</code></pre> <pre><code>+--------+--------+----------+-------+--------+-------------+\n|mergeKey|personId|personName|country|  region|effectiveDate|\n+--------+--------+----------+-------+--------+-------------+\n|    null|       1| elon musk| canada|montreal|   1989-06-01|\n|       1|       1| elon musk| canada|montreal|   1989-06-01|\n|       4|       4|       dhh|     us| chicago|   2005-11-01|\n+--------+--------+----------+-------+--------+-------------+\n</code></pre> <p>The two Elon Musk rows in the staged upsert table are important. We need both, one with the <code>mergeKey</code> set to <code>null</code> and another with a populated <code>mergeKey</code> value.</p> <p>We're ready to perform the upsert now that the staged upsert table is properly formatted.</p>"},{"location":"delta-lake/type-2-scd-upserts/#perform-the-upsert","title":"Perform the upsert","text":"<p>Delta exposes an elegant Scala DSL for performing upserts.</p> <pre><code>techCelebsTable\n  .as(\"tech_celebs\")\n  .merge(stagedUpdates.as(\"staged_updates\"), \"tech_celebs.personId = mergeKey\")\n  .whenMatched(\"tech_celebs.isCurrent = true AND (staged_updates.country &lt;&gt; tech_celebs.country OR staged_updates.region &lt;&gt; tech_celebs.region)\")\n  .updateExpr(Map(\n    \"isCurrent\" -&gt; \"false\",\n    \"endDate\" -&gt; \"staged_updates.effectiveDate\"))\n  .whenNotMatched()\n  .insertExpr(Map(\n    \"personId\" -&gt; \"staged_updates.personId\",\n    \"personName\" -&gt; \"staged_updates.personName\",\n    \"country\" -&gt; \"staged_updates.country\",\n    \"region\" -&gt; \"staged_updates.region\",\n    \"isCurrent\" -&gt; \"true\",\n    \"effectiveDate\" -&gt; \"staged_updates.effectiveDate\",\n    \"endDate\" -&gt; \"null\"))\n  .execute()\n\nval resDF =  spark\n  .read\n  .format(\"delta\")\n  .load(path.toString())\n\nresDF.show()\n</code></pre> <pre><code>+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|    false|   1971-06-28|1989-06-01|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n|       1| elon musk|      canada|   montreal|     true|   1989-06-01|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       4|       dhh|          us|    chicago|     true|   2005-11-01|      null|\n+--------+----------+------------+-----------+---------+-------------+----------+\n</code></pre> <p>Remember that Elon Musk has two rows in the staging table:</p> <ul> <li>One with a <code>mergeKey</code> of <code>null</code></li> <li>Another with a <code>mergeKey</code> of 1</li> </ul> <p>When the <code>mergeKey</code> is 1, then the row is considered matched, and only the <code>isCurrent</code> and <code>endDate</code> fields are updated. That's how the Elon Musk South Africa row is updated.</p> <p>The Elon Musk Canada and DHH rows are considered \"not matched\" and are inserted with different logic.</p>"},{"location":"delta-lake/type-2-scd-upserts/#tying-everything-together","title":"Tying everything together","text":"<p>Study the initial table, the staged update table, and the final result side-by-side to understand the result:</p> <pre><code>+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|     true|   1971-06-28|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n+--------+----------+------------+-----------+---------+-------------+----------+\n\n+--------+--------+----------+-------+--------+-------------+\n|mergeKey|personId|personName|country|  region|effectiveDate|\n+--------+--------+----------+-------+--------+-------------+\n|    null|       1| elon musk| canada|montreal|   1989-06-01| NOT matched\n|       1|       1| elon musk| canada|montreal|   1989-06-01| matched\n|       4|       4|       dhh|     us| chicago|   2005-11-01| NOT matched\n+--------+--------+----------+-------+--------+-------------+\n\n+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|    false|   1971-06-28|1989-06-01|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n|       1| elon musk|      canada|   montreal|     true|   1989-06-01|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       4|       dhh|          us|    chicago|     true|   2005-11-01|      null|\n+--------+----------+------------+-----------+---------+-------------+----------+\n</code></pre>"},{"location":"delta-lake/type-2-scd-upserts/#inspect-the-filesystem-output","title":"Inspect the filesystem output","text":"<p>The Delta transaction log has two entries. The first transaction adds a single Parquet file: <code>part-00000-2cc6a8d9-86ee-4292-a850-9f5e01918c0d-c000.snappy.parquet</code>.</p> <p>The second transaction performed these filesystem operations:</p> <ul> <li>remove file <code>part-00000-2cc6a8d9-86ee-4292-a850-9f5e01918c0d-c000.snappy.parquet</code></li> <li>add file <code>part-00000-daa6c389-2894-4a6b-a012-618e830574c6-c000.snappy.parquet</code></li> <li>add file <code>part-00042-d38c2d50-7910-4658-b297-84c51cf4b196-c000.snappy.parquet</code></li> <li>add file <code>part-00043-acda103d-c5d7-4062-a6f8-0a112f4425f7-c000.snappy.parquet</code></li> <li>add file <code>part-00051-c5d71959-a214-44f9-97fc-a0b928a19393-c000.snappy.parquet</code></li> <li>add file <code>part-00102-41b139c3-a6cd-4a1a-814d-f509b84459a9-c000.snappy.parquet</code></li> <li>add file <code>part-00174-693e1b33-cbd1-4d93-b4fc-fbf4c7f00878-c000.snappy.parquet</code></li> </ul> <p>Here's the contents of all the files in the second transaction.</p> <p>part-00000-2cc6a8d9-86ee-4292-a850-9f5e01918c0d-c000.snappy.parquet (removed):</p> <pre><code>+--------+----------+------------+-----------+---------+-------------+----------+\n|personId|personName|     country|     region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+-----------+---------+-------------+----------+\n|       1| elon musk|south africa|   pretoria|     true|   1971-06-28|      null|\n|       2|jeff bezos|          us|albuquerque|     true|   1964-01-12|      null|\n|       3|bill gates|          us|    seattle|    false|   1955-10-28|1973-09-01|\n+--------+----------+------------+-----------+---------+-------------+----------+\n</code></pre> <p>part-00000-daa6c389-2894-4a6b-a012-618e830574c6-c000.snappy.parquet (not sure why an empty file is added):</p> <pre><code>+--------+----------+-------+------+---------+-------------+-------+\n|personId|personName|country|region|isCurrent|effectiveDate|endDate|\n+--------+----------+-------+------+---------+-------------+-------+\n+--------+----------+-------+------+---------+-------------+-------+\n</code></pre> <p>part-00042-d38c2d50-7910-4658-b297-84c51cf4b196-c000.snappy.parquet (added):</p> <pre><code>+--------+----------+-------+--------+---------+-------------+-------+\n|personId|personName|country|  region|isCurrent|effectiveDate|endDate|\n+--------+----------+-------+--------+---------+-------------+-------+\n|       1| elon musk| canada|montreal|     true|   1989-06-01|   null|\n+--------+----------+-------+--------+---------+-------------+-------+\n</code></pre> <p>part-00043-acda103d-c5d7-4062-a6f8-0a112f4425f7-c000.snappy.parquet (added):</p> <pre><code>+--------+----------+------------+--------+---------+-------------+----------+\n|personId|personName|     country|  region|isCurrent|effectiveDate|   endDate|\n+--------+----------+------------+--------+---------+-------------+----------+\n|       1| elon musk|south africa|pretoria|    false|   1971-06-28|1989-06-01|\n+--------+----------+------------+--------+---------+-------------+----------+\n</code></pre> <p>part-00051-c5d71959-a214-44f9-97fc-a0b928a19393-c000.snappy.parquet (added):</p> <pre><code>+--------+----------+-------+-------+---------+-------------+----------+\n|personId|personName|country| region|isCurrent|effectiveDate|   endDate|\n+--------+----------+-------+-------+---------+-------------+----------+\n|       3|bill gates|     us|seattle|    false|   1955-10-28|1973-09-01|\n+--------+----------+-------+-------+---------+-------------+----------+\n</code></pre> <p>part-00102-41b139c3-a6cd-4a1a-814d-f509b84459a9-c000.snappy.parquet (added):</p> <pre><code>+--------+----------+-------+-------+---------+-------------+-------+\n|personId|personName|country| region|isCurrent|effectiveDate|endDate|\n+--------+----------+-------+-------+---------+-------------+-------+\n|       4|       dhh|     us|chicago|     true|   2005-11-01|   null|\n+--------+----------+-------+-------+---------+-------------+-------+\n</code></pre> <p>part-00174-693e1b33-cbd1-4d93-b4fc-fbf4c7f00878-c000.snappy.parquet (added):</p> <pre><code>+--------+----------+-------+-----------+---------+-------------+-------+\n|personId|personName|country|     region|isCurrent|effectiveDate|endDate|\n+--------+----------+-------+-----------+---------+-------------+-------+\n|       2|jeff bezos|     us|albuquerque|     true|   1964-01-12|   null|\n+--------+----------+-------+-----------+---------+-------------+-------+\n</code></pre> <p>Adding single row Parquet files seems silly, but Delta isn't optimized to run on tiny datasets.</p> <p>Delta is powerful because it can perform these upserts on huge datasets.</p>"},{"location":"delta-lake/type-2-scd-upserts/#next-steps","title":"Next steps","text":"<p>Watch the Databricks talk on type 2 SCDs and Dominique's excellent presentation on working with Delta Lake at a massive scale.</p> <p>See this commit for the code covered in this post. You can clone the repo, run this code on your local machine, and observe the files that are created. That's a great way to learn.</p> <p>Check out the other Data Lake posts for topics that aren't quite as advanced as upserts. If you're new to Delta Lake, it's best to master the introductory concepts like the basics of the transaction log and time travel, before moving to more advanced concepts.</p>"},{"location":"delta-lake/updating-partitions-with-replacewhere/","title":"Selectively updating Delta partitions with replaceWhere","text":"<p>Delta makes it easy to update certain disk partitions with the <code>replaceWhere</code> option.</p> <p>Selectively applying updates to certain partitions isn't always possible (sometimes the entire lake needs the update), but can result in significant speed gains.</p> <p>Let's start with a simple example and then explore situations where the <code>replaceWhere</code> update pattern is applicable.</p>"},{"location":"delta-lake/updating-partitions-with-replacewhere/#simple-example","title":"Simple example","text":"<p>Suppose we have the following five rows of data in a CSV file:</p> <pre><code>first_name,last_name,country\nErnesto,Guevara,Argentina\nVladimir,Putin,Russia\nMaria,Sharapova,Russia\nBruce,Lee,China\nJack,Ma,China\n</code></pre> <p>Let's create a Delta lake from the CSV file:</p> <pre><code>val df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n  .withColumn(\"continent\", lit(null).cast(StringType))\n\nval deltaPath = new java.io.File(\"./tmp/country_partitioned_lake/\").getCanonicalPath\n\ndf\n  .repartition(col(\"country\"))\n  .write\n  .partitionBy(\"country\")\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(deltaPath)\n</code></pre> <p>We're appending a blank <code>continent</code> column to the DataFrame before writing it out as a Delta table, so we won't have any schema mismatch issues.</p> <p>Let's define a custom DataFrame transformation that'll append a <code>continent</code> column to a DataFrame:</p> <pre><code>def withContinent()(df: DataFrame): DataFrame = {\n  df.withColumn(\n    \"continent\",\n    when(col(\"country\") === \"Russia\", \"Europe\")\n      .when(col(\"country\") === \"China\", \"Asia\")\n      .when(col(\"country\") === \"Argentina\", \"South America\")\n  )\n}\n</code></pre> <p>Suppose the business would like us to populate the <code>continent</code> column, but only for the China partition. We can use <code>replaceWhere</code> to only update the China partition.</p> <pre><code>spark.read.format(\"delta\").load(deltaPath)\n  .where(col(\"country\") === \"China\")\n  .transform(withContinent())\n  .write\n  .format(\"delta\")\n  .option(\"replaceWhere\", \"country = 'China'\")\n  .mode(\"overwrite\")\n  .save(deltaPath)\n</code></pre> <p>Let's view the contents of the Delta lake:</p> <pre><code>spark\n  .read\n  .format(\"delta\")\n  .load(deltaPath)\n  .show(false)\n\n+----------+---------+---------+---------+\n|first_name|last_name|country  |continent|\n+----------+---------+---------+---------+\n|Ernesto   |Guevara  |Argentina|null     |\n|Bruce     |Lee      |China    |Asia     |\n|Jack      |Ma       |China    |Asia     |\n|Vladimir  |Putin    |Russia   |null     |\n|Maria     |Sharapova|Russia   |null     |\n+----------+---------+---------+---------+\n</code></pre> <p>Let's view the transaction log and confirm that only the China partition was updated. Here are the contents of the <code>_delta_log/00000000000000000001.json</code> file:</p> <pre><code>{\n  \"add\":{\n    \"path\":\"country=China/part-00000-3abbdd5f-1f0f-48bd-8618-5992823d1a37.c000.snappy.parquet\",\n    \"partitionValues\":{\n      \"country\":\"China\"\n    },\n    \"size\":854,\n    \"modificationTime\":1571835829000,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"remove\":{\n    \"path\":\"country=China/part-00059-dfa81c0d-2a5e-443c-9e15-1e5c40834d68.c000.snappy.parquet\",\n    \"deletionTimestamp\":1571835830680,\n    \"dataChange\":true\n  }\n}\n</code></pre>"},{"location":"delta-lake/updating-partitions-with-replacewhere/#practical-use-case","title":"Practical use case","text":"<p><code>replaceWhere</code> is particularly useful when you have to run a computationally expensive algorithm, but only on certain partitions.</p> <p>Suppose you have a <code>personLikesSalsa()</code> algorithm that is super complex and cannot be run on the entire dataset for performance reasons.</p> <p>If your dataset is partitioned by country, you can specify to only run the <code>personLikesSalsa()</code> algorithm on the most relevant partitions (e.g. Puerto Rico, Colombia, and Cuba).</p> <p>It might not be ideal to only run an algorithm on a certain partition of your data, but it might be a reality you're forced to face.</p>"},{"location":"delta-lake/updating-partitions-with-replacewhere/#summary","title":"Summary","text":"<p><code>replaceWhere</code> is a powerful option when maintaining Delta data lakes. Performance optimizations like <code>replaceWhere</code> are vital when you're working with a big dataset.</p>"},{"location":"delta-lake/vacuum-command/","title":"Vacuuming Delta Lakes","text":"<p>Delta lakes are versioned so you can easily revert to old versions of the data.</p> <p>In some instances, Delta lake needs to store multiple versions of the data to enable the rollback feature.</p> <p>Storing multiple versions of the same data can get expensive, so Delta lake includes a <code>vacuum</code> command that deletes old versions of the data.</p> <p>This blog post explains how to use the <code>vacuum</code> command and situations where it is applicable.</p>"},{"location":"delta-lake/vacuum-command/#simple-example","title":"Simple example","text":"<p>Let's use the following CSV file to make a Delta lake.</p> <pre><code>first_name,last_name,country\nmiguel,cordoba,colombia\nluisa,gomez,colombia\nli,li,china\nwang,wei,china\nhans,meyer,germany\nmia,schmidt,germany\n</code></pre> <p>Here's the code to create the Delta lake.</p> <pre><code>val path = new java.io.File(\"./src/main/resources/person_data/people1.csv\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .save(outputPath)\n</code></pre> <p>Let's view the content of the Delta lake:</p> <pre><code>val path = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\nval df = spark.read.format(\"delta\").load(path)\ndf.show()\n\n+----------+---------+--------+\n|first_name|last_name| country|\n+----------+---------+--------+\n|    miguel|  cordoba|colombia|\n|     luisa|    gomez|colombia|\n|        li|       li|   china|\n|      wang|      wei|   china|\n|      hans|    meyer| germany|\n|       mia|  schmidt| germany|\n+----------+---------+--------+\n</code></pre> <p>Here are the contents of the filesystem after the first write:</p> <pre><code>vacuum_example/\n  _delta_log/\n    00000000000000000000.json\n  part-00000-d7ec54f9-eee2-40ae-b8b9-a6786214d3ac-c000.snappy.parquet\n</code></pre> <p>Let's overwrite the data in the Delta lake with another CSV file:</p> <pre><code>first_name,last_name,country\nlou,bega,germany\nbradley,nowell,usa\n</code></pre> <p>Here's the code that'll overwrite the lake:</p> <pre><code>val path = new java.io.File(\"./src/main/resources/person_data/people2.csv\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Overwrite)\n  .save(outputPath)\n</code></pre> <p>Here's what the data looks like after the overwrite:</p> <pre><code>val path = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\nval df = spark.read.format(\"delta\").load(path)\ndf.show()\n\n+----------+---------+-------+\n|first_name|last_name|country|\n+----------+---------+-------+\n|       lou|     bega|germany|\n|   bradley|   nowell|    usa|\n+----------+---------+-------+\n</code></pre> <p>Here are the contents of the filesystem after the second write:</p> <pre><code>vacuum_example/\n  _delta_log/\n    00000000000000000000.json\n    00000000000000000001.json\n  part-00000-4c44ec8c-6d02-4f01-91f7-78c05df0fd27-c000.snappy.parquet\n  part-00000-d7ec54f9-eee2-40ae-b8b9-a6786214d3ac-c000.snappy.parquet\n</code></pre> <p>So the data from the first write isn't read into our DataFrame anymore, but it's still stored in the filesystem.</p> <p>This allows us to rollback to an older version of the data.</p> <p>Let's display the contents of our Delta lake as of version 0:</p> <pre><code>val path = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path)\ndf.show()\n\n+----------+---------+--------+\n|first_name|last_name| country|\n+----------+---------+--------+\n|    miguel|  cordoba|colombia|\n|     luisa|    gomez|colombia|\n|        li|       li|   china|\n|      wang|      wei|   china|\n|      hans|    meyer| germany|\n|       mia|  schmidt| germany|\n+----------+---------+--------+\n</code></pre> <p>Delta lake provides a vacuum command that deletes older versions of the data (any data that's older than the specified retention period).</p> <p>Let's run the <code>vacuum</code> command and verify the file is deleted in the filesystem.</p> <pre><code>val path = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\nimport io.delta.tables._\nval deltaTable = DeltaTable.forPath(spark, path)\ndeltaTable.vacuum(0.000001)\n</code></pre> <p>We set the retention period to 0.000001 hours so we can run this vacuum command right away.</p> <p>Here's what the filesystem looks like after running the <code>vacuum</code> command.</p> <pre><code>vacuum_example/\n  _delta_log/\n    00000000000000000000.json\n    00000000000000000001.json\n  part-00000-4c44ec8c-6d02-4f01-91f7-78c05df0fd27-c000.snappy.parquet\n</code></pre> <p>Let's look at the <code>00000000000000000001.json</code> file to understand how Delta knows what files to delete:</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-4c44ec8c-6d02-4f01-91f7-78c05df0fd27-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":832,\n    \"modificationTime\":1568680993000,\n    \"dataChange\":true\n  }\n}\n\n{\n  \"remove\":{\n    \"path\":\"part-00000-d7ec54f9-eee2-40ae-b8b9-a6786214d3ac-c000.snappy.parquet\",\n    \"deletionTimestamp\":1568680995519,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>The <code>remove</code> part of the JSON file indicates that <code>part-00000-d7ec54f9-....snappy.parquet</code> can be deleted when the <code>vacuum</code> command is run.</p> <p>We cannot access version 0 of the Delta lake after the vacuum command has been run:</p> <pre><code>val path = new java.io.File(\"./tmp/vacuum_example/\").getCanonicalPath\nval df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path)\ndf.show()\n</code></pre> <pre><code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1645.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1645.0 (TID 21123, localhost, executor driver): java.io.FileNotFoundException: File file:/Users/powers/Documents/code/my_apps/yello-taxi/tmp/vacuum_example/part-00000-618d3c69-3d1b-402c-818f-9d3995b5639f-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\nCause: java.io.FileNotFoundException: File file:/Users/powers/Documents/code/my_apps/yello-taxi/tmp/vacuum_example/part-00000-618d3c69-3d1b-402c-818f-9d3995b5639f-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n</code></pre>"},{"location":"delta-lake/vacuum-command/#default-retention-period","title":"Default retention period","text":"<p>The retention period is 7 days by default.</p> <p>So <code>deltaTable.vacuum()</code> wouldn't do anything unless we waited 7 days to run the command.</p> <p>You need to set a special command to invoke the <code>vacuum</code> method with a retention period that's less than 7 days. Otherwise you'll get the following error message:</p> <pre><code>java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n</code></pre> <p>We need to update the Spark configuration to allow for such a short retention period.</p> <pre><code>lazy val spark: SparkSession = {\n  SparkSession\n    .builder()\n    .master(\"local\")\n    .appName(\"spark session\")\n    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n    .getOrCreate()\n}\n</code></pre>"},{"location":"delta-lake/vacuum-command/#when-vacuum-does-nothing","title":"When vacuum does nothing","text":"<p>Let's look at another example where we're simply adding data to the lake, so running the vacuum command won't do anything.</p> <p>Here's the <code>dogs1.csv</code> file:</p> <pre><code>first_name,breed\nfido,lab\nspot,bulldog\n</code></pre> <p>And here's the <code>dogs2.csv</code> file:</p> <pre><code>first_name,breed\nfido,beagle\nlou,pug\n</code></pre> <p>Here's some code to write out <code>dog1.csv</code> and <code>dogs2.csv</code> as Delta lake files.</p> <pre><code>val path = new java.io.File(\"./src/main/resources/dog_data/dogs1.csv\").getCanonicalPath\nval df = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path)\n\nval outputPath = new java.io.File(\"./tmp/vacuum_example2/\").getCanonicalPath\ndf\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .save(outputPath)\n\nval path2 = new java.io.File(\"./src/main/resources/dog_data/dogs2.csv\").getCanonicalPath\nval df2 = spark\n  .read\n  .option(\"header\", \"true\")\n  .option(\"charset\", \"UTF8\")\n  .csv(path2)\n\ndf2\n  .repartition(1)\n  .write\n  .format(\"delta\")\n  .mode(SaveMode.Append)\n  .save(outputPath)\n</code></pre> <p>Here's what the Delta lake contains after both files are written:</p> <pre><code>+----------+-------+\n|first_name|  breed|\n+----------+-------+\n|      fido|    lab|\n|      spot|bulldog|\n|      fido| beagle|\n|       lou|    pug|\n+----------+-------+\n</code></pre> <p>This is what the filesystem looks like:</p> <pre><code>vacuum_example2/\n  _delta_log/\n    00000000000000000000.json\n    00000000000000000001.json\n  part-00000-57db2297-9aaf-44b6-b940-48c504c510d1-c000.snappy.parquet\n  part-00000-6574b35c-677b-4423-95ae-993638f222cf-c000.snappy.parquet\n</code></pre> <p>Here are the contents of the <code>00000000000000000000.json</code> file:</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-57db2297-9aaf-44b6-b940-48c504c510d1-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":606,\n    \"modificationTime\":1568685380000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>Here are the contents of the <code>00000000000000000001.json</code> file.</p> <pre><code>{\n  \"add\":{\n    \"path\":\"part-00000-6574b35c-677b-4423-95ae-993638f222cf-c000.snappy.parquet\",\n    \"partitionValues\":{\n\n    },\n    \"size\":600,\n    \"modificationTime\":1568685386000,\n    \"dataChange\":true\n  }\n}\n</code></pre> <p>None of the JSON files contain any <code>remove</code> lines so a <code>vacuum</code> won't delete any files.</p> <p>This code doesn't change anything:</p> <pre><code>val path = new java.io.File(\"./tmp/vacuum_example2/\").getCanonicalPath\nimport io.delta.tables._\nval deltaTable = DeltaTable.forPath(spark, path)\ndeltaTable.vacuum(0.000001)\n</code></pre>"},{"location":"delta-lake/vacuum-command/#conclusion","title":"Conclusion","text":"<p>Use <code>vacuum()</code> to delete files from your Delta lake if you'd like to save on data storage costs.</p> <p>You'll often have duplicate files after running <code>Overwrite</code> operations. Any files that are older than the specified retention period and are marked as <code>remove</code> in the <code>_delta_log/</code> JSON files will be deleted when <code>vacuum</code> is run.</p>"},{"location":"devrel/devrel-driven-development/","title":"DevRel Driven Development","text":"<p>DevRel Driven Development is driving software development from developer advocacy activities like creating documentation, writing blog posts, and producing videos. Developers advocates frequently identify public interface warts when creating content. They can collaborate closely with devs build more intuitive APIs and give end users a better development experience.</p> <p>When done right, DevRel Driven Development creates a symbiotic collaboration between core developers, dev advocates and end users. This virtuous cycle motivates developers and delights end users. Developers love exposing elegant APIs that are quickly adopted by users. A good dev advocate will help the devs expose the right interface.</p> <p>This post explains how to perform DevRel Driven Development and provides real-world examples, so you can see the process in action.</p>"},{"location":"devrel/devrel-driven-development/#create-and-improve-documentation","title":"Create and improve documentation","text":"<p>When a developer advocate starts working on a project, they should go through all the existing docs, run the examples locally, and see if there are any small improvements that'll help onboard new users.</p> <p>It's good to inject positivity and tackle low hanging fruit when you're first starting to contribute to an open source project. You want to build goodwill with the developers before suggesting large changes.</p> <p>Suppose you read through the docs and notice some spelling and grammar issues. You can submit a pull request and add a description like this:</p> <p>This pull request fixes some minor typos in the docs. I was able to learn a lot about this project by going through the docs and running the examples locally. Thanks for making it easy for me to get started with this library!</p> <p>If the project doesn't have any documentation, then you can open an issue like this:</p> <p>Can I make a pull request to add some basic usage instructions to the README? I'd like to make it easy for new users to get up-and-running with this library. Let me know if this sounds like a good idea.</p> <p>Lots of developers prefer focusing their energy on software engineering and don't enjoy evangelizing their work. They appreciate developer advocacy activities that'll help them get users.</p> <p>You can gradually step up the scope of proposed changes after you've built a relationship with the developers and have convinced them that they'll benefit from your involvement in the project.</p> <p>You're likely to add new sections to the docs and reorganize content after a few iterations. You'll eventually find some gaps and unintuitive APIs and that's when you can start suggesting new features.</p>"},{"location":"devrel/devrel-driven-development/#fixing-bugs","title":"Fixing bugs","text":"<p>When you're going through the docs and running the code locally, you're bound to spot certain sections that are missing, unintuitive, or overly complex.</p> <p>Let's look at a real-world example to see how DevRel Driven Development can quickly squash bugs and add features.</p> <p>I was reading through the delta-rs docs and created a Jupyter Notebook, so other developers could easily follow along.</p> <p>I tried to create a Delta Lake with delta-rs, but faced an unexpected error when reading the Delta Lake. I filed a bug report and a delta-rs dev fixed the issue within a day. Turns out the test suite only checked absolute paths and the code wasn't working for relative paths. I switched the notebook to absolute paths and continued creating the demo notebook.</p> <p>The demo notebook made me realize that some features weren't implemented yet.</p>"},{"location":"devrel/devrel-driven-development/#filling-gaps","title":"Filling gaps","text":"<p>Delta Lake allows for schema enforcement, which prevents files with different schema from being added to the Delta Lake.</p> <p>I noticed schema enforcement wasn't working in the demo notebook and pinged the delta-rs devs in Slack. They noticed this was an oversight, created an issue, and quickly fixed the bug.</p> <p>Devs are highly motivated to fix oversights, especially if they feel ownership of the codebase.</p>"},{"location":"devrel/devrel-driven-development/#suggesting-interfaces","title":"Suggesting interfaces","text":"<p>Developer advocates should also provide developers with suggestions on the public interfaces that should be exposed.</p> <p>I tried to vacuum a Delta Lake with delta-rs and found that there isn't a way to bypass the retention period check. You can overcome this restriction with normal Delta Lake by setting <code>config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")</code> when creating the SparkSession. One of the delta-rs developers suggested adding an <code>enforce_retention_duration=True</code> parameter in <code>DeltaTable.vacuum()</code>. I countered in an issue by suggesting to add this parameter as <code>retention_duration_check_enabled=True</code>, so we're consistent with the Delta Lake wording.</p> <p>A health back-and-forth on the optimal interface is likely to result in an API that's intuitive for users.</p> <p>Developer advocates can take their suggesting to the next level by actually submitting pull requests to implement their suggestions.</p>"},{"location":"devrel/devrel-driven-development/#devrel-pull-requests","title":"DevRel pull requests","text":"<p>I am writing Dask: The Definitive Guide and have been creating a lot of small Dask DataFrames in the book examples.</p> <p>I've been creating Dask DataFrames as follows:</p> <pre><code>import pandas as pd\n\npandas_df = pd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n)\ndf = dd.from_pandas(pandas_df, npartitions=2)\n</code></pre> <p>This syntax confuses users. They're surprised they need to <code>import pandas</code> to create a small Dask DataFrame.</p> <p>I suggested adding the following interface to avoid importing pandas:</p> <pre><code>import dask.dataframe as dd\n\nddf = dd.DataFrame(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]},\n    npartitions=2,\n)\n</code></pre> <p>The Dask DataFrame public interface is designed to mimic the pandas API and a core Dask developer suggested a <code>from_dict</code> class method, just like pandas:</p> <pre><code>dd.DataFrame.from_dict(\n    {\"num1\": [1, 2, 3, 4], \"num2\": [7, 8, 9, 10]}, npartitions=2\n)\n</code></pre> <p>I created a pull request to add this feature. I haven't contributed to Dask extensively and ended up pairing with a Dask core contributor to get the pull request in a mergeable state.</p> <p>Actually creating pull requests is a great way to connect with developers at a deeper level. Developers typically appreciate they effort, even if they need to significantly refactor your code.</p> <p>Developer advocates can of course add a lot of value to project, even if they're not writing any code.</p>"},{"location":"devrel/devrel-driven-development/#devrel-cheerleading","title":"DevRel cheerleading","text":"<p>Dev advocates should strive to inject positivity to the project and guide the team to communicate in an upbeat manner. Everyone has their own communication style, but a little extra positive energy never hurts.</p> <p>Reacting with emojis to pull requests and issues is the lowest hanging fruit. If a developer fixes a bug, adds a feature, or raises a good issue, it's easy to react with a thumbs up. When you start adding positive emojis, other developers are likely to follow suit and do the same.</p> <p>You can also react with positive messages, like \"this is a great feature, thank you!\". You don't want to clutter inboxes with these types of messages, so use messages that generate notifications sparingly.</p> <p>Now lets turn our attention to the main dev advocacy value-add - driving usage and adoption.</p>"},{"location":"devrel/devrel-driven-development/#driving-usage-via-content","title":"Driving usage via content","text":"<p>A good dev advocate should help developers acquire new users and delight existing users.</p> <p>Dev advocates should be creating content to help onboard new users. Hopefully the engineers can easily see how the dev advocates are helping their project grow.</p> <p>Engineers are much more likely to support DevRel Driven Development if they feel like the developer advocates are adding value to the project. You can proactively share dev advocacy metrics (pageviews, button clicks, etc) with developers to help them quanity the value of your contributions to the project.</p>"},{"location":"devrel/devrel-driven-development/#conclusion","title":"Conclusion","text":"<p>DevRel Driven Development is a great way for developer advocates to forge strong connections with core library developers and end users and drive adoption of technologies. It's a more active type of advocacy that requires you to get your hands dirty instead of cheering on the sidelines.</p> <p>It's only practical for highly technical dev advocates that are developers themselves. Developer advocates with a purely marketing background won't be technical enough to suggest public interfaces or submit pull requests.</p> <p>It's best to use DevRel Driven Development in conjunction with engineering-lead feature prioritization. Engineers have plenty of features to build and technical debt to work on, independent of dev advocacy driven features. DevRel Driven Development is best layered on top of existing engineering workflows to encourage user-based focus and beautiful public interface design.</p> <p>Software engineers can fall into the trap of spending 99% of their time on programming and only 1% of their time on creating READMEs, documentation, and SEO-optimized content for end users. It's hard to get users if the onboarding materials and messaging are not on point. DevRel Driven Development encourages engineers to spend a bit more time to build an amazing user experience, which is a great way to get users to fall in love with their beautiful code.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/","title":"The Virtuous Content Cycle for Developer Advocates","text":"<p>This post explains how to scale developer advocacy by creating content in a way that answers current user questions and makes it easier to generate additional content in the future.</p> <p>Developer advocates help engineers leverage technologies to get their jobs done efficiently. Successful software projects grow exponentially, so the number of user questions also grows rapidly. The virtuous content cycle allows developer advocates to scale themselves and meet the needs of a quickly growing community.</p> <p>TL;DR:</p> <ul> <li>Answer user questions generically</li> <li>Create content that makes it easy for users to answer their own questions</li> <li>Answer questions with reusable content</li> <li>Use content to make other content (e.g. convert a successful blog post to a tech talk and a educational video)</li> <li>You will be able to handle more user questions by answering common queries in a scalable manner</li> </ul>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#start-by-answering-user-questions","title":"Start by answering user questions","text":"<p>The virtuous content cycle starts by answering user questions in generic, minimal, and reproducible manner. That'll make your answers easier to reuse by other developers.</p> <p>Question askers often intermingle business logic and unrelated details in their technical questions. They may talk about running code on a specific cloud for a question where the cloud isn't relevant for example.</p> <p>The developer advocate should simplify the user question and make it generic / minimal. You want to make easy for developers with related issues to grok the example.</p> <p>You should also publish a fully functional code snippet or notebook in the answer so interested parties can easily reproduce the example on their machines. Bonus points if the code snippet is checked into source control.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#preventatively-answer-new-questions","title":"Preventatively answer new questions","text":"<p>When new developers Google the same question in the future, you want to make it easy for them to find your answer. If they see your high quality response, chances are they'll be able to figure out the answer and won't have to ask again.</p> <p>Good content often prevents you from having to deal with repeat questions.</p> <p>Developer advocates should obsess over providing users with a wonderful experience for all commonly searched keywords related to their technology. Delighting existing users and proactively answering their questions helps developers love your tech!</p> <p>Your high quality responses will come in handy, even when developers re-ask the question.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#use-content-to-help-answer-questions","title":"Use content to help answer questions","text":"<p>Some users will inevitably re-ask questions that have already been answered in forums or in chat channels. It's possible they couldn't find your response via their Google search or they can't figure out how the generic answer relates to their particular question.</p> <p>You can help these users by sending them a link to the blog post and/or the notebook with the minimal code example. Leveraging existing content when answering questions is more efficient than starting from scratch every time.</p> <p>Sometimes you'll need to help the user connect the dots between their specific question and the generic response. You can help them simplify their question and that will usually show them the light.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#watch-out-for-the-chat-trap","title":"Watch out for the \"chat trap\"","text":"<p>The \"chat trap\" is when you answer questions in a chat application like Slack and don't reproduce that answer in a medium that'll be easily searchable by other developers in the future.</p> <p>You'll inevitably have to re-answer this question again in the future if your answer is in a chat application that's not indexed by search engines.</p> <p>In these situations it's better to generically answer the question in a blog post and send the user a link. You can occasionally answer user questions in chat applications, but should be wary because this isn't scalable.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#find-what-content-resonates-with-your-users","title":"Find what content resonates with your users","text":"<p>You want to continuously gauge how your content engages users. You'll want verbal queues from users that your responses help them get past their issue. You should get comments like \"thanks, that worked!\".</p> <p>You also want objective metrics that indicate your answers are helping. In Stackoverflow, you should check to make sure your answers are getting views and upvotes. For blog posts, you should check pageviews, time on page, and Google ranking for targeted keywords.</p> <p>Objective metrics let you periodically check in and make sure you're generating content with the highest return on investment for your developer community. It also lets you iterate and develop a communication style that's most engaging for your audience. Content with more upvotes and a longer time on page is more engaging.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#use-content-to-make-other-content","title":"Use content to make other content","text":"<p>Once you have a large body of content with performance metrics it's easy to repurpose it for other mediums.</p> <p>A blog post can easily be converted to a tech talk, a video, part of a course, or a section of a book. Some blog posts are best repurposed in a book chapter. Other blog posts can be easily adapted for an amazing talk at a programming conference.</p> <p>Once you have objective metrics indicating your content resonates with the target audience, you can be more confident that it'll also hit the mark when presented in another form. For example, suppose you have a blog post on a catchy subject with an average time on page of 7 minutes. This content is clearly engaging for your audience, so it's likely to make for a great tech talk as well.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#growing-traffic-an-inevitable-byproduct","title":"Growing traffic; an inevitable byproduct","text":"<p>When you're in the virtuous content cycle, all your metrics should get better each month. Your blog and video traffic should be growing every week. You should be able to handle more user questions without growing the team.</p> <p>Better metrics are a necessary side effect of scaling your developer advocacy efforts. The only way you can preventatively answer more user questions is by creating content that ranks well and gets more traffic. Remember that preventatively answering questions is helping a user to answer their own questions via Google searches, so they don't have to answer the question again in your chat channel.</p> <p>Traffic that grows month-over-month is a good indication you're in the virtuous content cycle.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#content-on-first-principles","title":"Content on first principles","text":"<p>A first principle is a fundamental building block. Content on first principles is the most easy to repurpose for user questions.</p> <p>Answering a user question and providing them with a few links on related first principles is a great way to teach them more about the underlying technology. When a user is trying to work through an error, they usually won't want to learn about unrelated first principles, but are willing to read a post on a related fundamental building block.</p> <p>Boiling down your tech into fundamental building block and using these first principle blog posts is a great way to enter the virtuous content cycle. Keyword research can help you identify the fundamental building blocks of your technology.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#keyword-research","title":"Keyword research","text":"<p>Keyword research is finding the search terms that users are entering into search engines related to your technology. Popular keywords are often first principle building blocks for the technology you're evangelising.</p> <p>You should prioritize your content based on the most popular keywords. This will allow you to help the greatest number of users.</p>"},{"location":"devrel/virtuous-content-cycle-developer-advocates/#thriving-with-a-virtuous-content-cycle","title":"Thriving with a virtuous content cycle","text":"<p>Developer advocates on successful software projects can easily become overwhelmed by the sheer volume of user questions.</p> <p>Answering user questions with reusable content is the best way to scale developer advocacy efforts. Tailoring responses for individual users isn't scalable or feasible at a certain point.</p> <p>Scaling your developer advocacy efforts with reusable content has a bunch of wonderful side effects:</p> <ul> <li>it gives you ideas on the best topics for tech talks</li> <li>it encourages keyword research and user empathy</li> <li>it forces you to boil down concepts into first principles</li> <li>it encourages you to create minimal, reproducible answers</li> </ul> <p>Once you're in the virtuous content cycle, your metrics will get better every month and you'll be able to keep up with all the user questions. The volume of questions should not grow excessively because many users will be answering their own questions.</p> <p>Your content will allow you to easily make other types of content, so you'll be able to evangelize the technology more extensively. Once you have all the baseline content created, you should have time to make more videos, give talks, and participate in meetups. For a passionate developer advocate, the virtuous content cycle is a wonderfully satisfying way to work.</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/","title":"Learning Emacs Keybindings After Multiple Failed Attempts","text":"<p>Have you ever tried Emacs, found the keybindings to be highly unergonomic, and wondered why so many great programmers are passionate about such a weird editor?</p> <p>You're not alone - Emacs pinky is a thing and several famous Emacs users suffer with debilitating repetitive strain injury.</p> <p>This blog post outlines a different approach to learning Emacs that won't leave you confused or with RSI.</p> <p>There is a reason tons of famous programmers use Emacs.</p> <p>I'll let you in on the secret\u2026</p> <p>If you've had a bad experience with Emacs is probably because you didn't use the right keyboard.</p> <p>Yep, that's right, people have been blaming Emacs for decades when the real problem is the keyboard they're using to write Emacs!</p> <p>TL;DR Emacs isn't great on normal keyboards but is amazing on keyboards like the Kinesis Advantage that provide easy access to modifier keys for both hands.</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#most-keyboards-suck-for-emacs","title":"Most keyboards suck for Emacs","text":"<p>Most keyboards suck for programming because the special characters and modifer keys aren't easily accessible.</p> <p>Emacs keybindings should be used with keyboards easily let you press the modifier keys with both hands.</p> <p>You haven't given Emacs a fair shot if you haven't tried it out with a suitable keyboard.</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#ergonomic-typing-basics","title":"Ergonomic typing basics","text":"<p>You should use two hands whenever typing commands that require two keys. When typing capital W, you should hold down the shift key with your right hand and type the \"w\" key with your left hand. You should not hold the shift key with your left pinky and press the \"w\" key with your left index finger.</p> <p>Lots of Emacs commands require the Control key. There is only one Control key in the lower left corner of most keyboards. This encourages programmers to press Control + c (C-c) by holding Control with their left pinky and \"c\" with their right index finger.</p> <p>Repeatedly pressing two keys with one hand is not ergonomic and will cause RSI.</p> <p>Emacs pinky is caused by the location of the control key on your keyboard. It's not a flaw of the text editor!</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#suitable-keyboards-for-emacs","title":"Suitable keyboards for Emacs","text":"<p>A good programming keyboard makes it easy to press the Control and Meta keys with both your left and right hands. Apple users also need access to the Command button with both hands.</p> <p>Your keyboard should allow you to type the modifier keys with your index fingers or thumbs. Your pinkies are weak fingers and are most susceptible to RSI. Pinkies should be used minimally when typing.</p> <p></p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#kinesis-advantage","title":"Kinesis Advantage","text":"<p>The Kinesis Advantage keyboard provides easy access to the Control, Meta, and Apple keys for both your right and left thumbs.</p> <p>With a little bit of Kinesis configuration, you can setup your keys like this:</p> <p></p> <p>Emacs shortcuts are ergonomic and intuitive with a Kinesis Advantage keyboard.</p> <p>The difference is night and day - I love Emacs on a Kinesis Advantage keyboard and don't use Emacs when I am using regular Mac keyboards.</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#sublime-text-and-emacs","title":"Sublime Text and Emacs","text":"<p>The Emacs Pro Essentials plugin makes it easy to add familiar Emacs key bindings to the modern Sublime Text editor.</p> <p>This blog post discusses moving to Sublime Text after 20 years of Emacs. Some key quotes from the article:</p> <ul> <li>\"Sublime Text 3 is a better and moderner programming editor than GNU Emacs in the 21st century.\"</li> <li>\"Unlike 1980, programming is a much more complicated task in today\u2019s world. We have too many things to learn. People should spend time on more important stuff rather than tinkering their editor anymore.\"</li> </ul> <p>You can start learning Emacs keybindings with Sublime Text and then switch to \"pure Emacs\" later if you'd like to learn Emacs Lisp and have full control to configure your text development environment.</p> <p>The Sublime Text shortcuts work seamlessly with the Emacs shortcuts. You can easily highlight text \"the Emacs way\" (C-space C-space) and then cut / copy \"the Apple way\" (Command-x / Command-v).</p> <p>Sublime offers great file navigation with Command-t and great find/replace with Command-f.</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#unexpected-benefits-of-emacs","title":"Unexpected benefits of Emacs","text":"<p>Emacs keybindings work well in a ton of unexpected places including Google Chrome input boxes, TextEdit, and the Terminal command line.</p> <p>Brian Fox created Bash and was an Emacs maintainer so it's no suprise that Emacs keybindings work seamlessly in Bash terminals.</p> <p>Standard shortcuts don't work in Terminal and you'll find it amazing to have a good set of shortcuts for editing command line text.</p>"},{"location":"emacs/learning-emacs-keybindings-after-multiple-failed-attempts/#conclusion","title":"Conclusion","text":"<p>Emacs keybindings didn't make sense to me until I got a Kinesis Advantage keyboard.</p> <p>You should learn how to use the Kinesis Advantage keyboard first and then learn Emacs. Learning both at the same time would be too much.</p> <p>I've been a happy Vim user for many years and now I'm a happy Emacs user too.</p> <p>I use Vim when typing on my Mac keyboard (when traveling with my laptop) and Emacs when I have access to my Kinesis Advantage keyboard and standing desk.</p> <p>P.S. The blog post contains affiliate links.</p>"},{"location":"github/creating-ticketing-system-with-slack/","title":"Creating an IT Ticketing System with GitHub and Slack","text":"<p>GitHub is great for an IT ticketing system - it's easy to create issues, set assignees, and prioritize the ticket with labels.</p> <p>The GitHub Slack integration makes it easy to create tickets directly from the Slack command line.</p> <p>This blog post explains how to create a lightweight ticketing system with GitHub, so you can avoid adding another technology to your organization.</p>"},{"location":"github/creating-ticketing-system-with-slack/#creating-github-tickets","title":"Creating GitHub tickets","text":"<p>Let's say your GitHub repo is named <code>bug_tracker</code>. You can easily create a ticket by logging onto GitHub, openining the <code>bug_tracker</code> repo, clicking Issues, and then clicking New Issue.</p> <p>This workflow works fine for programmers familiar with GitHub, but it's not the best for nontechnical team members.</p> <p>You might want to make the ticketing system easily accessible to product, legal, and other departments.</p> <p>Luckily the Slack GitHub integration makes it easy to create GitHub issues directly from the Slack command line.</p>"},{"location":"github/creating-ticketing-system-with-slack/#creating-tickets-from-slack","title":"Creating tickets from Slack","text":"<p>The GitHub Slack integration makes it easy to create issues in Slack.</p> <p>You link your GitHub and Slack accounts with the <code>/github signin</code> command (you can run this command from any channel).</p> <p>The \"Take action\" section of the README describes the command to create an issue: <code>/github open [owner/repo]</code></p> <p>For example, to create an issue in the spark-daria, you'd run <code>/github open mrpowers/spark-daria</code>.</p> <p>This command opens the following dialogue box where you can populate the issue title, body, and labels.</p> <p></p> <p>Just fill out the form and click Open to create the issue!</p>"},{"location":"github/creating-ticketing-system-with-slack/#creating-tickets-with-templates","title":"Creating tickets with templates","text":"<p>You can also make templates that make it easier to populate different types of tickets.</p> <p>Suppose you have a <code>mrpowers/bugs</code> repo. You can add issue templates to the <code>.github/ISSUE_TEMPLATE</code> directory. Let's create a <code>.github/ISSUE_TEMPLATE/bad_link.md</code> file with these contents:</p> <pre><code># Example issue\n\nThis is an example template\n\n## Fun\n\nI like that GitHub supports this\n</code></pre> <p>We can now run <code>/github open mrpowers/bugs bad_link</code> to open the issue form with the body pre-populated from the template.</p> <p></p>"},{"location":"github/creating-ticketing-system-with-slack/#custom-slack-slash-commands","title":"Custom Slack Slash commands","text":"<p>You can use Slack slash commands to build a custom ticketing system.</p> <p>You could make a <code>/bug_report</code> Slash command and let the user type commands like <code>/bug_report the link to the about page is broken</code>. The <code>/bug_report</code> command could be configured to send a POST request to AWS Lambda. Lambda would create a GitHub issue, add approvers, ping folks in Slack, etc.</p> <p>It's usually best to avoid building a custom system. Internal tools take a while to build and are annoying to maintain. You probably won't want to update Lambda functions once versions get deprecated or update your code when Slack changes their API.</p>"},{"location":"github/creating-ticketing-system-with-slack/#conclusion","title":"Conclusion","text":"<p>GitHub is a great tool for a lightweight ticketing system.</p> <p>The GitHub features and terminology are familiar for developers.</p> <p>The GitHub Slack integration is maintained and you can rely on third party developers to keep the code updated as Slack changes their API.</p> <p>This is a great opportunity for organizations to limit the number of technologies and operational overhead.</p>"},{"location":"go/csv-to-parquet/","title":"Converting CSV files to Parquet with Go","text":"<p>This blog post explains how to read data from a CSV file and write it out as a Parquet file.</p> <p>The Parquet file format is better than CSV for a lot of data operations. Columnar data stores allow for column pruning that massively speeds up lots of queries.</p> <p>Go is a great language for ETL. Writing out Parquet files makes it easier for downstream Spark or Python to consume data in an optimized manner.</p> <p>The parquet-go library makes it easy to convert CSV files to Parquet files.</p>"},{"location":"go/csv-to-parquet/#sample-csv-data","title":"Sample CSV data","text":"<p>Let's start with the following sample data in the <code>data/shoes.csv</code> file:</p> <pre><code>nike,air_griffey\nfila,grant_hill_2\nsteph_curry,curry7\n</code></pre> <p>Let's read this data and write it out as a Parquet file.</p> <p>Check out the parquet-go-example repo if you'd like to run this code yourself.</p>"},{"location":"go/csv-to-parquet/#create-parquet-file","title":"Create Parquet file","text":"<p>Create a <code>Shoe</code> struct that'll be used for each row of data in the CSV file:</p> <pre><code>type Shoe struct {\n    ShoeBrand string `parquet:\"name=shoe_brand, type=UTF8\"`\n    ShoeName  string `parquet:\"name=shoe_name, type=UTF8\"`\n}\n</code></pre> <p>Setup the Parquet writer so it's ready to accept data writes:</p> <pre><code>var err error\n\nfw, err := local.NewLocalFileWriter(\"tmp/shoes.parquet\")\nif err != nil {\n    log.Println(\"Can't create local file\", err)\n    return\n}\n\npw, err := writer.NewParquetWriter(fw, new(Shoe), 2)\nif err != nil {\n    log.Println(\"Can't create parquet writer\", err)\n    return\n}\n\npw.RowGroupSize = 128 * 1024 * 1024 //128M\npw.CompressionType = parquet.CompressionCodec_SNAPPY\n</code></pre> <p>Open up the CSV file, iterate over every line in the file, and then write each line to the Parquet file:</p> <pre><code>csvFile, _ := os.Open(\"data/shoes.csv\")\nreader := csv.NewReader(bufio.NewReader(csvFile))\n\nfor {\n    line, error := reader.Read()\n    if error == io.EOF {\n        break\n    } else if error != nil {\n        log.Fatal(error)\n    }\n    shoe := Shoe{\n        ShoeBrand: line[0],\n        ShoeName:  line[1],\n    }\n    if err = pw.Write(shoe); err != nil {\n        log.Println(\"Write error\", err)\n    }\n}\n</code></pre> <p>Once we've iterated over all the lines in the file, we can stop the <code>NewParquetWriter</code> and close the <code>NewLocalFileWriter</code>.</p> <pre><code>if err = pw.WriteStop(); err != nil {\n    log.Println(\"WriteStop error\", err)\n    return\n}\n\nlog.Println(\"Write Finished\")\nfw.Close()\n</code></pre> <p>The data will be written in the <code>tmp/shoes.parquet</code> file. You can run this on your local machine with the <code>go run csv_to_parquet.go</code> command.</p> <p>Let's read this Parquet file into a Spark DataFrame to verify that it's compatible with another framework. Spark loves Parquet files ;)</p>"},{"location":"go/csv-to-parquet/#read-into-spark-dataframe","title":"Read into Spark DataFrame","text":"<p>You can download Spark to run this code on your local machine if you'd like.</p> <p>The Parquet file was ouputted to <code>/Users/powers/Documents/code/my_apps/parquet-go-example/tmp/shoes.parquet</code> on my machine.</p> <p><code>cd</code> into the downloaded Spark directory (e.g. <code>cd ~/spark-2.4.0-bin-hadoop2.7/bin/</code>) and then run <code>./spark-shell</code> to start the Spark console.</p> <p>Let's read the Parquet file into a Spark DataFrame:</p> <pre><code>val path = \"/Users/powers/Documents/code/my_apps/parquet-go-example/tmp/shoes.parquet\"\nval df = spark.read.parquet(path)\n</code></pre> <p>Run the <code>show()</code> method to inspect the DataFrame contents:</p> <pre><code>df.show()\n\n+-----------+------------+\n| shoe_brand|   shoe_name|\n+-----------+------------+\n|       nike| air_griffey|\n|       fila|grant_hill_2|\n|steph_curry|      curry7|\n+-----------+------------+\n</code></pre> <p>Run the <code>printSchema()</code> method to view the DataFrame schema.</p> <pre><code>df.printSchema()\n\nroot\n |-- shoe_brand: string (nullable = true)\n |-- shoe_name: string (nullable = true)\n</code></pre> <p>You can use Go to build a Parquet data lake and then do further data analytics with Spark. Parquet is the perfect pass off between Go and Spark!</p>"},{"location":"go/csv-to-parquet/#reading-into-a-go-dataframe","title":"Reading into a Go DataFrame","text":"<p>qframe seems to be the most promising Go DataFrame library.</p> <p>It doesn't support Parquet yet, but hopefully we can get a <code>qframe.ReadParquet</code> method added ;)</p>"},{"location":"go/csv-to-parquet/#next-steps","title":"Next steps","text":"<p>We need to create more examples and demonstrate that parquet-go can also write out other column types like integers.</p> <p>Go is a great language for ETL. Parquet support makes it even better!</p>"},{"location":"go/dataframes-gota-qframe/","title":"DataFrames in Go with gota, qframe, and dataframe-go","text":"<p>Go has great DataFrame libraries that let you easily manipulate data that's stored in CSV files and databases.</p> <p>Working with CSV files directly can be burdensome. DataFrames are easier because they provide data manipulation and grouping functionality natively.</p> <p>There are three popular Go libraries:</p> <ul> <li>gota: started in January 2016</li> <li>qframe: started November 2016</li> <li>dataframe-go: started in October 2018</li> </ul> <p>This blog post shows you how to perform basic operations with each library so we can see which API is the cleanest.</p>"},{"location":"go/dataframes-gota-qframe/#why-are-dataframes-important-for-go","title":"Why are DataFrames important for Go","text":"<p>Go is a great language for ETL.</p> <p>Developers coming from other languages / frameworks love using DataFrames.</p> <p>Web development is the biggest Go domain, but there is still a nice chunk of developers that use Go for data science.</p>"},{"location":"go/dataframes-gota-qframe/#initial-impressions","title":"Initial impressions","text":"<ul> <li>qframe has the most elegant API and performs faster than gota in all benchmarks. We need to add dataframe-go benchmarks.</li> <li>no native support the Parquet file format yet</li> <li>no support for Arrow yet</li> </ul> <p>None of the libraries have stable APIs yet. Let's help add key features and move these libraries towards 1.0 releases!</p>"},{"location":"go/dataframes-gota-qframe/#qframes","title":"qframes","text":"<p>Suppose you have a <code>data/example.csv</code> file with the following contents:</p> <pre><code>first_name,favorite_number\nmatthew,23\ndaniel,8\nallison,42\ndavid,18\n</code></pre> <p>Let's open the CSV file and read it into a DataFrame:</p> <pre><code>csvfile, err := os.Open(\"data/example.csv\")\nif err != nil {\n    log.Fatal(err)\n}\n\nf := qframe.ReadCSV(csvfile)\n</code></pre> <p>We can view the data with <code>fmt.Println</code>.</p> <pre><code>fmt.Println(f)\n\nfirst_name(s) favorite_number(i)\n------------- ------------------\n      matthew                 23\n       daniel                  8\n      allison                 42\n        david                 18\n</code></pre> <p>The <code>(s)</code> next to <code>first_name</code> means it's a string column. The <code>(i)</code> next to <code>favorite_number</code> means it's an integer column.</p> <p>qframe intelligently infers the schema (it doesn't blindly assume all columns are strings).</p> <p>Let's add an <code>is_even</code> column to the DataFrame that contains <code>true</code> if <code>favorite_number</code> is even.</p> <pre><code>f = f.Apply(\n    qframe.Instruction{\n        Fn:      isEven,\n        DstCol:  \"is_even\",\n        SrcCol1: \"favorite_number\"})\n</code></pre> <p>Let's check that <code>is_even</code> has been added:</p> <pre><code>fmt.Println(f)\n\nfirst_name(s) favorite_number(i) is_even(b)\n------------- ------------------ ----------\n      matthew                 23      false\n       daniel                  8       true\n      allison                 42       true\n        david                 18       true\n</code></pre> <p>Filter out all the rows that do not have <code>is_even</code> set to <code>true</code>.</p> <pre><code>newF := f.Filter(qframe.Filter{Column: \"is_even\", Comparator: \"=\", Arg: true})\n</code></pre> <p>Let's take a look at the filtered DataFrame:</p> <pre><code>fmt.Println(newF)\n\nfirst_name(s) favorite_number(i) is_even(b)\n------------- ------------------ ----------\n       daniel                  8       true\n      allison                 42       true\n        david                 18       true\n</code></pre> <p>Let's write out this result to a CSV file:</p> <pre><code>file, err := os.Create(\"tmp/qframe_main_ouput.csv\")\nif err != nil {\n    log.Fatal(err)\n}\nnewF.ToCSV(file)\n</code></pre> <p>The <code>tmp/qframe_main_ouput.csv</code> file will look like this:</p> <pre><code>first_name,favorite_number,is_even\ndaniel,8,true\nallison,42,true\ndavid,18,true\n</code></pre> <p>qframe is easy to work with and has a great public interface.</p>"},{"location":"go/dataframes-gota-qframe/#rocketlaunchr-dataframe-go","title":"rocketlaunchr dataframe-go","text":"<p>Let's use the same dataset and run the same operations with dataframe-go.</p> <p>Read the CSV into a DataFrame.</p> <pre><code>ctx := context.TODO()\n\ncsvfile, err := os.Open(\"data/example.csv\")\nif err != nil {\n    log.Fatal(err)\n}\n\ndf, err := imports.LoadFromCSV(ctx, csvfile, imports.CSVLoadOptions{\n    DictateDataType: map[string]interface{}{\n        \"first_name\":      \"\",       // specify this column as string\n        \"favorite_number\": int64(0), // specify this column as int64\n    }})\n</code></pre> <p>View the contents of the <code>df</code>:</p> <pre><code>fmt.Print(df.Table())\n\n+-----+------------+-----------------+\n|     | FIRST NAME | FAVORITE NUMBER |\n+-----+------------+-----------------+\n| 0:  |  matthew   |       23        |\n| 1:  |   daniel   |        8        |\n| 2:  |  allison   |       42        |\n| 3:  |   david    |       18        |\n+-----+------------+-----------------+\n| 4X2 |   STRING   |      INT64      |\n+-----+------------+-----------------+\n</code></pre> <p>The <code>Print</code> output is a little confusing because the column names are actually <code>first_name</code> and <code>favorite_number</code>.</p> <p>Points to note when reading CSVs with dataframe-go:</p> <ul> <li>Schema inference is not supported, so we need to use <code>DictateDataType</code> to specify that <code>favorite_number</code> is an <code>int64</code> column</li> <li>We need to create a context to use the <code>LoadFromCSV</code> method</li> </ul> <p>Let's multiply the <code>favorite_number</code> column by two:</p> <pre><code>s := df.Series[1]\n\napplyFn := dataframe.ApplySeriesFn(func(val interface{}, row, nRows int) interface{} {\n    return 2 * val.(int64)\n})\n\ndataframe.Apply(ctx, s, applyFn, dataframe.FilterOptions{InPlace: true})\n</code></pre> <p>Let's view the contents of <code>df</code>:</p> <pre><code>fmt.Print(df.Table())\n\n+-----+------------+-----------------+\n|     | FIRST NAME | FAVORITE NUMBER |\n+-----+------------+-----------------+\n| 0:  |  matthew   |       46        |\n| 1:  |   daniel   |       16        |\n| 2:  |  allison   |       84        |\n| 3:  |   david    |       36        |\n+-----+------------+-----------------+\n| 4X2 |   STRING   |      INT64      |\n+-----+------------+-----------------+\n</code></pre> <p>Some notes on this code:</p> <ul> <li><code>df.Series[1]</code> depends on the <code>favorite_number</code> column being the second column in the DataFrame. If the columns are reordered, the code will error out or double another column.</li> <li>The empty interface is used in a couple of spots</li> </ul> <p>I couldn't figure out filtering easily.</p> <p>Here's the filtering example in the README:</p> <pre><code>filterFn := dataframe.FilterDataFrameFn(func(vals map[interface{}]interface{}, row, nRows int) (dataframe.FilterAction, error) {\n    if vals[\"title\"] == nil {\n        return dataframe.DROP, nil\n    }\n    return dataframe.KEEP, nil\n})\n\nseniors, _ := dataframe.Filter(ctx, df, filterFn)\n</code></pre> <p>We were able to filter a qframe DataFrame with only a single line of code: <code>f.Filter(qframe.Filter{Column: \"is_even\", Comparator: \"=\", Arg: true})</code>. dataframe-go is more verbose.</p> <p>The dataframe-go maintainers are great to work with. Hopefully we can add dataframe-go to qbench, so we can compare the gota, qframe, and dataframe-go performance side-by-side.</p>"},{"location":"go/dataframes-gota-qframe/#gota","title":"gota","text":"<p>Let's load the <code>data/example.csv</code> file into a gota DataFrame:</p> <pre><code>csvfile, err := os.Open(\"data/example.csv\")\nif err != nil {\n    log.Fatal(err)\n}\n\ndf := dataframe.ReadCSV(csvfile)\n</code></pre> <p>We can view the DataFrame contents with <code>Println</code>:</p> <pre><code>fmt.Println(\"df: \", df)\n\n    first_name favorite_number\n 0: matthew    23\n 1: daniel     8\n 2: allison    42\n 3: david      18\n    &lt;string&gt;   &lt;int&gt;\n</code></pre> <p>gota has smartly inferred that <code>favorite_number</code> is an integer column.</p> <p>Add an <code>is_even</code> column to the DataFrame if <code>favorite_number</code> is even:</p> <pre><code>isEven := func(s series.Series) series.Series {\n    num, _ := s.Int()\n    isFavoriteNumberEven := num[0]%2 == 0\n    return series.Bools(isFavoriteNumberEven)\n}\nisEvenSeries := df.Select(\"favorite_number\").Rapply(isEven)\nisEvenSeries.SetNames(\"is_even\")\ndf = df.CBind(isEvenSeries)\n</code></pre> <p>Email me if you know how to make this code better!</p> <p><code>df</code> now has an <code>is_even</code> column:</p> <pre><code>fmt.Println(\"df with is even: \", df)\n\ndf with is even:  [4x3] DataFrame\n\n    first_name favorite_number is_even\n 0: matthew    23              false\n 1: daniel     8               true\n 2: allison    42              true\n 3: david      18              true\n    &lt;string&gt;   &lt;int&gt;           &lt;bool&gt;\n</code></pre> <p>Let's filter the DataFrame so it only contains people with a <code>favorite_number</code> that's even (i.e. only include the rows where the <code>is_even</code> column is <code>true</code>).</p> <pre><code>df = df.Filter(dataframe.F{\"is_even\", \"==\", true})\nfmt.Println(\"df filtered: \", df)\n</code></pre> <p>Here's the output:</p> <pre><code>df filtered:  [3x3] DataFrame\n\n    first_name favorite_number is_even\n 0: daniel     8               true\n 1: allison    42              true\n 2: david      18              true\n    &lt;string&gt;   &lt;int&gt;           &lt;bool&gt;\n</code></pre> <p>Now let's write our filtered DataFrame to disk. Here's the code that'll write this data to your local filesystem:</p> <pre><code>f, err := os.Create(\"tmp/gota_example_output.csv\")\nif err != nil {\n    log.Fatal(err)\n}\n\ndf.WriteCSV(f)\n</code></pre> <p>Open the <code>tmp/gota_example_output.csv</code> file in your text editor and inspect the contents:</p> <pre><code>first_name,favorite_number,is_even\ndaniel,8,true\nallison,42,true\ndavid,18,true\n</code></pre>"},{"location":"go/dataframes-gota-qframe/#spark-scala-syntax","title":"Spark / Scala syntax","text":"<p>Spark provides an elegant API for working with DataFrames. Let's look at the Spark code to perform these operations.</p> <p>Read the data into a Spark DataFrame</p> <pre><code>val df = spark.read.option(\"header\", \"true\").csv(path)\n</code></pre> <p>Pretty print the DataFrame and the DataFrame schema:</p> <pre><code>df.show()\n\n+----------+---------------+\n|first_name|favorite_number|\n+----------+---------------+\n|   matthew|             23|\n|    daniel|              8|\n|   allison|             42|\n|     david|             18|\n+----------+---------------+\n\ndf.printSchema()\n\nroot\n |-- first_name: string (nullable = true)\n |-- favorite_number: string (nullable = true)\n</code></pre> <p>Add the <code>is_even</code> column to the DataFrame and print the output:</p> <pre><code>val df2 = df.withColumn(\"is_even\", $\"favorite_number\" % 2 === 0)\n\ndf2.show()\n\n+----------+---------------+-------+\n|first_name|favorite_number|is_even|\n+----------+---------------+-------+\n|   matthew|             23|  false|\n|    daniel|              8|   true|\n|   allison|             42|   true|\n|     david|             18|   true|\n+----------+---------------+-------+\n</code></pre> <p>Filter out all the values where <code>is_even</code> is <code>false</code>:</p> <pre><code>val filteredDF = df2.where($\"is_even\" === true)\n\nfilteredDF.show()\n\n+----------+---------------+-------+\n|first_name|favorite_number|is_even|\n+----------+---------------+-------+\n|    daniel|              8|   true|\n|   allison|             42|   true|\n|     david|             18|   true|\n+----------+---------------+-------+\n</code></pre> <p>Write the data to disk:</p> <pre><code>filteredDF.repartition(1).write.csv(outputPath)\n</code></pre> <p>Spark is optimized to write multiple files in parallel. We've used <code>repartition(1)</code> to write out a single file, but this is bad practice for bigger datasets.</p> <p>Here's how Spark will write the data in this example:</p> <pre><code>some_spark_example/\n  _SUCCESS\n  part-00000-43fad235-8734-4270-9fed-bf0d3b3eda77-c000.csv\n</code></pre> <p>Check out Writing Beautiful Apache Spark Code if you'd like to quickly learn how to use Apache Spark.</p>"},{"location":"go/dataframes-gota-qframe/#next-steps","title":"Next steps","text":"<p>A lot of people want to use DataFrames in Go - the existing repos have a lot of stars.</p> <p>Go is a great language for ETL and a robust DataFrame library will make it even better!</p> <p>Suggested next steps:</p> <ul> <li>Study the Spark DataFrame API and see if we can make a Go DataFrame API that's equally elegant (qframe is close!)</li> <li>Add native Parquet support</li> <li>Add Parquet column pruning for a big speed bump</li> <li>Integrate Apache Arrow</li> <li>Make 1.0 release</li> </ul>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/","title":"Ultra-cheap international real estate markets in 2022","text":"<p>This post explains how to identify ultra-cheap international real estate markets and when you can capitalize on deeply discounted prices.</p> <p>Let's borrow Andrew Henderson's definition of an ultra-cheap real estate market:</p> <ul> <li>Less than $1,000 per square meter ($93 per square foot)</li> <li>Near the city center</li> </ul> <p>This article gives a high level overview of pricing per square meter in markets across the world and demonstrates how pricing varies greatly across cities.</p> <p>Some ultra-cheap markets have bad fundamentals and will stay cheap. Other ultra-cheap markets are set to grow and offer attractive returns for investors. This blog post will help you discover the markets that are destined to be winners.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#price-per-square-meter-for-different-cities","title":"Price per square meter for different cities","text":"<p>Numbeo shows the price per square meter for real estate in 493 cities around the world. Here are a few of the prices as of December 2021 (with their Numbeo price ranking):</p> <ul> <li>Hong Kong: 33,596 (1)</li> <li>New York: 15,576 (11)</li> <li>San Diego: 8,754 (43)</li> <li>Montreal: 5,693 (101)</li> <li>Kansas City: 2,819 (267)</li> <li>Detroit: 1,320 (412)</li> <li>Medellin: 974 (456)</li> <li>Caracas: 946 (462)</li> </ul> <p>The price discrepancies between cities is vast - you can either buy one apartment in Hong Kong or 35 equal sized apartments in Medellin for the same price!</p> <p>The following graph illustrates the number of cities in different price ranges:</p> <p></p> <p>Only 40 (8%) of the 493 cities in the list have a price per square meter less than $1,000. Andrew Henderson's \"ultra-cheap rule of thumb\" seems to nicely capture the bottom sliver of the market.</p> <p>A relatively small number of cities cost more than $10,000 per square meter. 66% of cities have apartments that cost between $1,000 and $5,000 per square meter.</p> <p>Let's take a look at some actual listings in different markets to sanity check the Numbeo estimates.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#hong-kong","title":"Hong Kong","text":"<p>Robinson Place - Block 2 costs $5,335,000 and is 195 square meters, so it costs $27,000 per square meter. Imperial Kennedy costs $6,412,000 and is smaller, so its cost per square meter is even higher. The Numbeo estimate of 33,596 per square meter in Hong Kong seems accurate.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#new-york-city","title":"New York City","text":"<p>1601 3rd Ave is 73 square meters and costs $885,000, so the price per square meter is $12,000. That's similar to $15,576 as predicted by Numbeo.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#san-diego","title":"San Diego","text":"<p>3078 Broadway is 77 square meters and costs $489,900, so the price per square meter is $6,500. It's priced similar to the Numbeo prediction of $8,754.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#medellin","title":"Medellin","text":"<p>This Medellin apartment costs $295,000 and is 300 square meters (983 per square meter), almost identical to $982 per square meter as computed by Numbeo.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#ultra-cheap-breakdown","title":"Ultra-cheap breakdown","text":"<p>Take a look at the number of ultra-cheap cities by country bearing in mind that most countries don't have any ultra-cheap cities:</p> <p></p> <p>India, Pakistan, Colombia, and Egypt are the only countries with multiple ultra-cheap cities.</p> <p>There are other countries that only have one ultra-cheap city like Iraq, Venezuela, Nigeria, and South Africa. They've been grouped into the Others bucket of the pie chart.</p> <p>Let's look at the GDP per capita for the countries with multiple ultra-cheap cities:</p> <ul> <li>India: $1,900</li> <li>Pakistan: $1,200</li> <li>Egypt: $3,500</li> <li>Colombia: $5,300</li> </ul> <p>While India has a lot of ultra-cheap cities, it's notable that the India brand name cities like Delhi ($3,005 per square meter), Mumbai ($7,185), and Bangalore ($1,800) are not ultra-cheap. Lesser know Indian cities like Vadodara ($584) are the ones that fall in the ultra-cheap bucket. Here's a video of Vadodara, so you can get an idea of the market.</p> <p>Multan, Pakistan is the cheapest city on the list ($288). Here's a video of Multan. This low pricing is probably mainly driven by the low GDP per capita in Pakistan.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#what-causes-markets-to-be-ultra-cheap","title":"What causes markets to be ultra-cheap?","text":"<p>Crime, economic crises, political instability, and low economic output are correlated with cheap cities.</p> <p>Caracas ($946), Cali ($852), and Johannesburg ($847) are on the top 50 cities with highest murder rates list, so it's not surprising they're ultra-cheap. However, Saint Louis ($1,967), Baltimore ($2,635), and San Juan ($2,788) are also on the top 50 murder list, but they're not nearly as cheap, so violence isn't the only factor.</p> <p>Venezuela is in the midst of a major economic crisis, so that's another contributing factor why Caracas is ultra-cheap. However, Lebanon is also in an economic crisis and Beirut ($3,761) isn't that cheap, so economic crises can't explain cheap housing single handedly either.</p> <p>Colombia is a curious case because the economy is growing, crime is falling, and demographics are favorable. The property market is also booming.</p> <p>Remember that the Numbeo figures are priced in US dollars, not in Colombian pesos. The Colombian peso has deprecated significantly against the US dollar from July 2014 to December 2021. The exchange rate has gone from 1,861 COP / USD to 4000 COP / USD. A Colombian property that rose 60% from July 2014 to December 2021 in Colombian peso terms actually fell in value when converted to US dollars due to exchange rate fluctuations.</p> <p>Not all currencies have been depreciating against the dollar. The Swiss Franc and the Singapore dollar haven't depreciated for example. The price per square meter isn't dragged down for Switzerland or Singapore when the figures are converted to US dollars.</p> <p>In summary, ultra-cheap real estate markets usually have bargain basement prices for a good reason. Finding the attractive investment opportunity requires identifying the markets that will get better or are unjustifiably battered down by false economic narratives.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#nomad-capitalist-video-on-ultra-cheap-markets","title":"Nomad Capitalist video on ultra-cheap markets","text":"<p>Andrew Henderson has a great video from 2019 on ultra-cheap international real estate:</p> <p>https://youtu.be/YyzsSQcDCvo</p> <p>Henderson recommends buying property in capital cities or business centers. He focuses on the core city center / tourist section of these cities. Capital cities tend to have the most population and economic growth in emerging markets.</p> <p>Some cities Henderson mentions in his 2019 video (with December 2021 Numbeo prices for comparison):</p> <ul> <li>Phnom Penh, Cambodia ($2,412)</li> <li>Tblisi, Georgia ($1,263)</li> <li>Chisinau, Moldova ($1,116)</li> <li>Bishkek, Kyrgyzstan (no data)</li> <li>Tashkent, Uzbekistan ($1,195)</li> <li>Cairo, Egypt ($850)</li> <li>Alexandria, Egypt ($923)</li> <li>Islamabad, Pakistan ($607)</li> <li>Tunis, Tunisia ($927)</li> </ul> <p>Some of his picks are already starting to climb out of the ultra-cheap club.</p> <p>Henderson likens investing in these markets to call options. Some of these markets will languish, but others will take off. The downside risk is limited because the prices are already as cheap as they get.</p> <p>You'll need to actually visit these countries or have a trusted local advisor to find the good deals of course. There aren't high quality websites with neatly organized listings in emerging markets. You won't find the good deals online.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#best-case-scenario-when-investing-in-ultra-cheap-markets","title":"Best case scenario when investing in ultra-cheap markets","text":"<p>You'd be fabulously wealthy if you bought real estate in Singapore back in 1980.</p> <p>Singapore's GDP per capita grew from $5,000 a year in 1980 to $60,000 a year in 2020. Their miraculous economic growth was accompanied with real estate price appreciation. Singapore is now one of the most expensive real estate markets in the world with a price per square meter of $19,000.</p> <p>If you bought a few apartments in Singapore in 1980, you'd have cash flow generating assets and would be sitting on massive capital gains.</p> <p>Singapore's success is obvious now, but it wasn't clear back in 1980.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#economic-growth-stories-arent-obvious","title":"Economic growth stories aren't obvious","text":"<p>Watch this video from the founding father of Singapore, Lee Kuan Yew (LKY), which contains some older clips of LKY, including the famous interview where he reflects on his failure to unify Malaysia:</p> <p>https://youtu.be/anAYPAmg0IM</p> <p>LKY is crying in the older clip and he sounds like a man that failed his life mission. Few people would watch the 1965 speech and think \"LKY will lead Singapore to be one of the greatest economic success stories of the 20th century\".</p> <p>Don't expect an ultra-cheap market to have an \"obvious path paved with gold\". That's not how it works. These markets are ultra-cheap for good reasons and you need to pick the ones that have good prospects.</p> <p>Most ultra-cheap markets won't turn it around like Singapore. They'll continue to languish.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#ultra-cheap-markets-can-stay-cheap","title":"Ultra-cheap markets can stay cheap","text":"<p>Kabul ($664) is an ultra-cheap market has been for many decades. Afghanistan had a brutal war in the 1980s and has continued to struggle with violent conflict ever since.</p> <p>As of 2020, the GDP per capita in Afghanistan is only $509 USD.</p> <p>Kabul seems likely to remain ultra-cheap.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#the-next-ultra-cheap-growth-story","title":"The next ultra-cheap growth story","text":"<p>Colombian is well poised to climb out of ultra-cheap territory.</p> <p>Colombia has a lot of positive fundamentals:</p> <ul> <li>low debt</li> <li>strong economic growth</li> <li>good demographics</li> <li>growing tourism</li> <li>close to US &amp; Canada</li> <li>open to foreign investment</li> </ul> <p>Andrew Henderson posits that Colombia is cheap because it's still shrouded by the \"it's a drug capital\" false narrative.</p> <p>I've been living in Colombia for the past several years and believe in the Colombian growth story. The Colombia Peso has weakened considerably from 2011 till 2021 and a slight strenghening of the Colombia Peso is all it will take to take the Colombian markets our of ultra-cheap territory. I'm buying properties in Colombia at current valuations.</p>"},{"location":"investing/ultra-cheap-real-estate-1000-square-meter/#conclusion","title":"Conclusion","text":"<p>Ultra-cheap real estate markets will be inexpensive for good reasons. Don't expect safe, beautiful, sunny, and rich cities like Santa Barbera, California ($19,000 per square meter) to be ultra-cheap.</p> <p>Some ultra-cheap real estate markets are beaten down for reasons that will persist in the future.</p> <p>Invest in the markets that are battered down by false narratives and have good economic growth stories.</p> <p>You'll need solid local contacts to invest well of course. An understanding of the culture, language, and local laws is also helpful.</p> <p>International real estate investing is only appropriate for a small fraction of investors that have a global mindset, are adventurous, and have a high risk tolerance. If you buy into an ultra-cheap market and it becomes the next Singapore, then you'll obviously make a lot of money.</p>"},{"location":"java/jenv-multiple-versions-java/","title":"Running Multiple Versions of Java on MacOS with jenv","text":"<p>jenv makes it easy to run multiple versions of Java on a Mac computer. It also makes it easy to seamlessly switch between Java versions when you switch projects.</p> <p>Running multiple Java versions is important for Android and Apache Spark developers. Spark developers should use Java 8 for Spark 2 projects and Java 11 for Spark 3 projects for example.</p> <p>This blog post shows you how to get jenv setup on your computer and how to use the important commands.</p>"},{"location":"java/jenv-multiple-versions-java/#jenv-setup","title":"jenv setup","text":"<p>Install jenv with <code>brew install jenv</code>. This is a Homebrew command.</p> <p>jenv uses the shim design pattern to route commands to the appropriate Java version. Run these commands to update your <code>PATH</code>:</p> <pre><code>echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' &gt;&gt; ~/.zshrc\necho 'eval \"$(jenv init -)\"' &gt;&gt; ~/.zshrc\n</code></pre> <p>Restart your Terminal, run <code>echo $PATH</code>, and verify the output contains <code>.jenv</code> paths that are prepended to the standard PATH directories. Here's the output on my machine <code>/Users/powers/.jenv/shims:/Users/powers/.jenv/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin</code>.</p>"},{"location":"java/jenv-multiple-versions-java/#install-java-8","title":"Install Java 8","text":"<p>Here's the latest command to install Java 8: <code>brew cask install adoptopenjdk/openjdk/adoptopenjdk8</code>.</p> <p><code>brew cask install adoptopenjdk8</code> used to work, but now returns <code>Error: Cask 'adoptopenjdk8' is unavailable: No Cask with this name exists.</code></p> <p><code>brew cask install caskroom/versions/adoptopenjdk8</code> also used to work, but now returns <code>Error: caskroom/versions was moved. Tap homebrew/cask-versions instead.</code></p> <p>Once Java is downloaded, we need to manually add it to jenv. List the Java virtual machines with <code>ls -1 /Library/Java/JavaVirtualMachines</code>.</p> <p>Add Java 8 to jenv with <code>jenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/</code>.</p> <p>Set the global Java version on your computer with <code>jenv global openjdk64-1.8.0.265</code>. The exact command on your machine might be something slightly different like <code>jenv global openjdk64-1.8.0.272</code>. Find the exact name of the version with <code>jenv versions</code>.</p> <p>Check to make sure the <code>javac -version</code> and <code>java -version</code> commands are working.</p>"},{"location":"java/jenv-multiple-versions-java/#set-global-java-version","title":"Set global Java version","text":"<p>Macs come with Java preinstalled. It's always good to avoid using system installed programming language versions (applies to Python and Ruby too). jenv makes it easy to avoid using the system Java.</p> <p>Set the global Java version to Java 8 with <code>jenv global openjdk64-1.8.0.265</code>.</p> <p>This command simply writes the version to the <code>/Users/powers/.jenv/version</code> file. Type <code>cat /Users/powers/.jenv/version</code> to see it's just a file with a single line</p> <pre><code>openjdk64-1.8.0.272\n</code></pre> <p>All Java commands will be routed to Java 8 now that the global version is set. This'll make sure we avoid hitting the system Java version.</p>"},{"location":"java/jenv-multiple-versions-java/#set-java_home","title":"Set JAVA_HOME","text":"<p>Lots of Java libraries depend on having a <code>JAVA_HOME</code> environment variable set. Set the environment variable by running these commands:</p> <pre><code>jenv enable-plugin export\nexec $SHELL -l\n</code></pre> <p>Run <code>echo $JAVA_HOME</code> and verify that it returns something like <code>/Users/powers/.jenv/versions/openjdk64-1.8.0.272</code>. Now any library that's looking for the <code>JAVA_HOME</code> environment to be set won't error out.</p> <p>Run <code>jenv doctor</code> to confirm your setup is good. You should get output like this:</p> <pre><code>[OK]    JAVA_HOME variable probably set by jenv PROMPT\n[OK]    Java binaries in path are jenv shims\n[OK]    Jenv is correctly loaded\n</code></pre>"},{"location":"java/jenv-multiple-versions-java/#install-java-11","title":"Install Java 11","text":"<p>Here's the command to install Java 11: <code>brew cask install adoptopenjdk/openjdk/adoptopenjdk11</code>.</p> <p>Remember that Java versions need to be manually added to jenv. List the Java virtual machines with <code>ls -1 /Library/Java/JavaVirtualMachines</code>.</p> <p>Add Java 11 to jenv with <code>jenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home/</code>.</p> <p>The <code>jenv versions</code> command should now output this:</p> <pre><code>  system\n  1.8\n  1.8.0.272\n  11\n  11.0\n  11.0.9\n* openjdk64-1.8.0.272 (set by /Users/powers/.jenv/version)\n  openjdk64-11.0.9\n</code></pre>"},{"location":"java/jenv-multiple-versions-java/#setting-up-maven","title":"Setting up Maven","text":"<p><code>which mvn</code> returns <code>/usr/local/bin/mvn</code>, which is the system version of Maven. Similar to Java, we never want to run commands using the system Maven. Let's use jenv to get a different version of Maven.</p> <p>Enable the jenv Maven plugin with <code>jenv enable-plugin maven</code> and then run <code>which mvn</code> to verify that the <code>mvn</code> commands are being properly captured by a jenv shim. The <code>which mvn</code> command should return something like <code>/Users/powers/.jenv/shims/mvn</code>.</p> <p>You can verify that your Maven installation is working properly by cloning a project and running the test suite. Clone the JavaSpark project with the <code>git clone git@github.com:MrPowers/JavaSpark.git</code> command.</p> <p><code>cd</code> into the project directory and run the test suite with <code>mvn test</code>.</p> <p>You can type <code>mvn -v</code> to see the Maven version that's being used. My machine is using Maven 3.6.3 with Java 8.</p> <p>You can also clone the deequ repo and verify that <code>mvn test</code> is working on that repo as well.</p>"},{"location":"java/jenv-multiple-versions-java/#setting-local-java-version-for-projects","title":"Setting local Java version for projects","text":"<p>Use the <code>jenv local openjdk64-11.0.9</code> command to set a given project to use Java 11 by default.</p> <p>This will add a <code>.java-version</code> file in the root project folder. Here's an example.</p> <p>You can clone the delta-examples repo with <code>git clone git@github.com:MrPowers/delta-examples.git</code>, cd into the directory, and run <code>jenv versions</code> to verify that the project is automatically using Java 11.</p> <p>Here's the <code>jenv versions</code> output from the delta-examples project root directory:</p> <pre><code>  system\n  1.8\n  1.8.0.272\n  11\n  11.0\n  11.0.9\n  openjdk64-1.8.0.272\n* openjdk64-11.0.9 (set by /Users/powers/Documents/code/my_apps/delta-examples/.java-version)\n</code></pre> <p>jenv's ability to automatically switch Java versions for different projects is quite convenient. You don't need to think about manually setting the Java version when you change projects.</p>"},{"location":"java/jenv-multiple-versions-java/#other-ways-to-switch-java-versions","title":"Other ways to switch Java versions","text":"<p>The AdoptOpenJDK project provides guidance on how to manually switch between Java versions if you don't want to use jenv. Here's the function they provide:</p> <pre><code>jdk() {\n        version=$1\n        export JAVA_HOME=$(/usr/libexec/java_home -v\"$version\");\n        java -version\n }\n</code></pre> <p>Switching manually is possible, but who wants to waste mental energy thinking about Java versions every time they switch a project?</p>"},{"location":"java/jenv-multiple-versions-java/#next-steps","title":"Next steps","text":"<p>jenv will help you manage Java on your Mac, even if you only need to use a single version.</p> <p>Managing different Java versions on a given machine was a huge pain before jenv came along. Now, you only need to run a few commands and your machine can be configured to run any Java version. jenv makes it easy to avoid accidentally using the system installed Java packages.</p> <p>jenv has built in plugins for SBT, Scala, Groovy, and more. Make sure to enable the plugins that are relevant for your workflows.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/","title":"Managing Multiple Java, SBT, and Scala Versions with SDKMAN","text":"<p>SDKMAN is an amazing project that makes it easy to manage multiple versions of Java and Scala on a single computer.</p> <p>It also provides support for other Java ecosystem projects like Maven, SBT, and Spark.</p> <p>SDKMAN stands for Software Development Kit MANager.</p> <p>SDKMAN is the best Java version management solution because it works out of the box for a variety of different programs.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#install-sdkman","title":"Install SDKMAN","text":"<p>Install SDKMAN with <code>curl -s \"https://get.sdkman.io\" | bash</code>.</p> <p>Run <code>source \"$HOME/.sdkman/bin/sdkman-init.sh\"</code> to load SDKMAN (this step is optional - you can also just close your Terminal and reopen it).</p> <p>Run <code>sdk version</code> and make sure <code>SDKMAN 5.9.1+575</code> is output to verify the installation was successful.</p> <p>The installation script will append these lines to your <code>~/.zshrc</code> file (you don't need to add these lines - the installer will add them for you):</p> <pre><code>#THIS MUST BE AT THE END OF THE FILE FOR SDKMAN TO WORK!!!\nexport SDKMAN_DIR=\"/Users/powers/.sdkman\"\n[[ -s \"/Users/powers/.sdkman/bin/sdkman-init.sh\" ]] &amp;&amp; source \"/Users/powers/.sdkman/bin/sdkman-init.sh\"\n</code></pre> <p>Run <code>echo $PATH</code> and see that .sdkman is prepended to the PATH as follows: <code>/Users/powers/.sdkman/candidates/java/current/bin:/Users/powers/.pyenv/shims:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin</code>.</p> <p>SDKMAN uses the shim design pattern to intercept Java relevant commands and route them to the correct software versions. This design pattern is also used in other version management tools like rbenv (Ruby) and pyenv (Python).</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#install-java","title":"Install Java","text":"<p>List the available Java versions with <code>sdk list java</code>:</p> <pre><code>================================================================================\nAvailable Java Versions\n================================================================================\n Vendor        | Use | Version      | Dist    | Status     | Identifier\n--------------------------------------------------------------------------------\n AdoptOpenJDK  |     | 15.0.1.j9    | adpt    |            | 15.0.1.j9-adpt\n               |     | 15.0.1.hs    | adpt    |            | 15.0.1.hs-adpt\n               |     | 14.0.2.j9    | adpt    |            | 14.0.2.j9-adpt\n               |     | 14.0.2.hs    | adpt    |            | 14.0.2.hs-adpt\n               |     | 13.0.2.j9    | adpt    |            | 13.0.2.j9-adpt\n               |     | 13.0.2.hs    | adpt    |            | 13.0.2.hs-adpt\n               |     | 12.0.2.j9    | adpt    |            | 12.0.2.j9-adpt\n               |     | 12.0.2.hs    | adpt    |            | 12.0.2.hs-adpt\n               |     | 11.0.9.j9    | adpt    |            | 11.0.9.j9-adpt\n               |     | 11.0.9.hs    | adpt    |            | 11.0.9.hs-adpt\n               |     | 8.0.272.j9   | adpt    |            | 8.0.272.j9-adpt\n               |     | 8.0.272.hs   | adpt    |            | 8.0.272.hs-adpt\n Amazon        |     | 15.0.1       | amzn    |            | 15.0.1-amzn\n               |     | 11.0.9       | amzn    |            | 11.0.9-amzn\n               |     | 8.0.275      | amzn    |            | 8.0.275-amzn\nmore versions not listed...\n</code></pre> <p>Install Java 8 with <code>sdk install java 8.0.272.hs-adpt</code>. Here's the terminal output when the installation is complete:</p> <pre><code>Repackaging Java 8.0.272.hs-adpt...\n\nDone repackaging...\nCleaning up residual files...\n\nInstalling: java 8.0.272.hs-adpt\nDone installing!\n</code></pre> <p>SDKMAN stores files it downloads in the <code>/Users/powers/.sdkman</code> directory.</p> <p>Now install Java 11 with <code>sdk install java 11.0.9.hs-adpt</code>. Type <code>sdk list java</code> to show the Java versions that have been installed and the Java version that's currently being used.</p> <pre><code>================================================================================\nAvailable Java Versions\n================================================================================\n Vendor        | Use | Version      | Dist    | Status     | Identifier\n--------------------------------------------------------------------------------\n AdoptOpenJDK  |     | 15.0.1.j9    | adpt    |            | 15.0.1.j9-adpt\n               |     | 15.0.1.hs    | adpt    |            | 15.0.1.hs-adpt\n               |     | 14.0.2.j9    | adpt    |            | 14.0.2.j9-adpt\n               |     | 14.0.2.hs    | adpt    |            | 14.0.2.hs-adpt\n               |     | 13.0.2.j9    | adpt    |            | 13.0.2.j9-adpt\n               |     | 13.0.2.hs    | adpt    |            | 13.0.2.hs-adpt\n               |     | 12.0.2.j9    | adpt    |            | 12.0.2.j9-adpt\n               |     | 12.0.2.hs    | adpt    |            | 12.0.2.hs-adpt\n               |     | 11.0.9.j9    | adpt    |            | 11.0.9.j9-adpt\n               |     | 11.0.9.hs    | adpt    | installed  | 11.0.9.hs-adpt\n               |     | 8.0.272.j9   | adpt    |            | 8.0.272.j9-adpt\n               | &gt;&gt;&gt; | 8.0.272.hs   | adpt    | installed  | 8.0.272.hs-adpt\n</code></pre> <p>You can also see the Java version that's being used by running <code>sdk current</code>:</p> <pre><code>Using java version 8.0.272.hs-adpt\n</code></pre> <p>Switch to Java 11 with <code>sdk use java 11.0.9.hs-adpt</code>. Run <code>sdk current</code> and confirm that Java 11 is being used.</p> <p>The <code>sdk use java</code> command will only switch the Java version for the current shell. You'll lose those settings when the shell is closed.</p> <p>You can set a default Java version for whenever shells are started. Set SDKMAN to default to Java 8 with <code>sdk default java 8.0.272.hs-adpt</code>.</p> <p>Close the shell, reopen it, and run <code>sdk current java</code> to confirm SDKMAN is using Java version 8.0.272.hs-adpt by default now.</p> <p>You can also check the version by running <code>java -version</code>. You should see output like this:</p> <pre><code>openjdk version \"1.8.0_272\"\nOpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_272-b10)\nOpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.272-b10, mixed mode)\n</code></pre>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#java_home","title":"JAVA_HOME","text":"<p>Lots of programs require the <code>JAVA_HOME</code> environment variable to be set.</p> <p>Lucky for you, SDKMAN automatically sets the <code>JAVA_HOME</code> environment variable.</p> <p>When you run <code>echo $JAVA_HOME</code>, you should see something like <code>/Users/powers/.sdkman/candidates/java/current</code>.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#install-sbt","title":"Install SBT","text":"<p>View the available SBT versions with <code>sdk list sbt</code>:</p> <pre><code>================================================================================\nAvailable Sbt Versions\n================================================================================\n     1.4.2               1.3.1               1.1.0\n     1.4.1               1.3.0               1.0.4\n     1.4.0               1.2.8               1.0.3\n     1.4.0-RC2           1.2.7               1.0.2\n     1.3.13              1.2.6               1.0.1\n     1.3.12              1.2.5               1.0.0\n     1.3.11              1.2.4               0.13.18\n     1.3.10              1.2.3               0.13.17\n     1.3.9               1.2.1\n     1.3.8               1.2.0\n     1.3.7               1.1.6\n     1.3.6               1.1.5\n     1.3.5               1.1.4\n     1.3.4               1.1.2\n     1.3.3               1.1.1\n</code></pre> <p>Install SBT 1.3.13 with <code>sdk install sbt 1.3.13</code>. After installing, respond with a \"Y\" to set 1.3.13 as your default SBT.</p> <p>You can clone a SBT project and run the test suite if you'd like to verify that your SBT version was installed correctly. Here are the optional verification steps.</p> <ul> <li>Clone spark-daria with <code>git clone git@github.com:MrPowers/spark-daria.git</code></li> <li><code>cd</code> into the spark-daria directory</li> <li>Run <code>sbt test</code> and make sure the test suite finishes successfully</li> </ul>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#install-scala","title":"Install Scala","text":"<p>List the available Scala versions with <code>sdk list scala</code>:</p> <pre><code>================================================================================\nAvailable Scala Versions\n================================================================================\n     2.13.3              2.12.1\n     2.13.2              2.12.0\n     2.13.1              2.11.12\n     2.13.0              2.11.11\n     2.12.12             2.11.8\n     2.12.11             2.11.7\n     2.12.10             2.11.6\n     2.12.9              2.11.5\n     2.12.8              2.11.4\n     2.12.7              2.11.3\n     2.12.6              2.11.2\n     2.12.5              2.11.1\n     2.12.4              2.11.0\n     2.12.3\n     2.12.2\n</code></pre> <p>Install Scala 2.11.12 with <code>sdk install scala 2.11.12</code>.</p> <p>Type <code>sdk use scala 2.11.12</code> to switch to the Scala version you just downloaded. Type <code>scala</code> in the Terminal and it'll open up an interactive Scala REPL with Scala 2.11.12. Type <code>:quit</code> in the REPL to exit.</p> <p>As you might have guessed, you can set a default Scala version with <code>sdk default scala 2.11.12</code>.</p> <p>SDKMAN's command line interface is consistent for all the SDKs. The commands to list, install, change, and set default versions follow the same syntax for each SDK. It's easy to memorize the commands.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#install-spark","title":"Install Spark","text":"<p>Run <code>sdk list spark</code> to see the versions of Spark you can download:</p> <pre><code>================================================================================\nAvailable Spark Versions\n================================================================================\n     3.0.1               2.2.0\n     3.0.0               2.1.3\n     2.4.7               2.1.2\n     2.4.6               2.1.1\n     2.4.5               2.0.2\n     2.4.4               1.6.3\n     2.4.3               1.5.2\n     2.4.2               1.4.1\n     2.4.1\n     2.4.0\n     2.3.3\n     2.3.2\n     2.3.1\n     2.3.0\n     2.2.1\n</code></pre> <p>Run <code>sdk install spark 2.4.7</code> to install the latest version of Spark 2.</p> <p>Spark is a great example of a project that needs specific Java, Scala, and SBT versions to work properly. Spark 2 projects should be run with Java 8, Scala 2.11, and SBT 1.3 for example. Run <code>sdk current</code> to show all the current versions that SDKMAN is currently using.</p> <pre><code>java: 8.0.272.hs-adpt\nsbt: 1.3.13\nscala: 2.11.12\nspark: 2.4.7\n</code></pre> <p>This machine currently has the right dependencies setup for a Spark 2 project.</p> <p>Run <code>spark-shell</code> to start a Spark REPL. You can see that it's using the right versions:</p> <pre><code>Spark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n      /_/\n\nUsing Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_272)\nType in expressions to have them evaluated.\nType :help for more information.\n</code></pre>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#other-supported-sdks","title":"Other supported SDKs","text":"<p>SDKMAN also supports a bunch of other SDKs.</p> <p>It supports projects such as Maven, Groovy, Kotlin, Spring Boot, and more.</p> <p>You can use the same syntax patterns we've seen in this blog post to install the SDKs, change versions, and set defaults.</p> <p>SDKMAN has an elegant, consistent public interface and the commands are easy to remember.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#where-sdkman-stores-packages","title":"Where SDKMAN stores packages","text":"<p>SDKMAN stores file in <code>$HOME/.sdkman/candidates/</code>.</p> <p>To find where SBT 1.3.13 is installed, type <code>sdk home sbt 1.3.13</code>. It'll return something like <code>/Users/powers/.sdkman/candidates/sbt/1.3.13</code>.</p> <p>The arguments to the <code>sdk install</code> command align with where the files are stored in <code>$HOME/.sdkman/candidates</code>.</p> <ul> <li><code>sdk install java 8.0.272.hs-adpt</code> stores files in <code>$HOME/.sdkman/candidates/java/8.0.272.hs-adpt</code>.</li> <li><code>sdk install sbt 1.3.13</code> stores files in <code>$HOME/.sdkman/candidates/sbt/1.3.13</code>.</li> </ul> <p>When you run <code>sdk install</code>, the downloaded binaries get saved in <code>$HOME/.sdkman/archives</code>. For example, <code>$HOME/.sdkman/archives/java-8.0.272.hs-adpt.zip</code> and <code>$HOME/.sdkman/archives/sbt-1.3.13.zip</code>.</p> <p>Some of the binaries are pretty big and can end up taking a lot of space on your computer. You should periodically delete them with the <code>sdk flush archives</code> command. Once you install the software, you don't need the binaries anymore.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#other-alternatives-for-managing-java-sdk-versions","title":"Other alternatives for managing Java / SDK versions","text":"<p>There are other ways to download and switch Java versions.</p> <p>You can download Java versions with Homebrew and switch versions with jenv. This approach is a little harder because jenv is not as user friendly and the Homebrew commands to download Java tend to change. It's easier to just use SDKMAN.</p> <p>Jabba helps you download different version of Java and lets you switch between them, but it doesn't provide access to any of the SDK versioning. You'll rarely use Java in isolation and will often want to switch between versions for core libraries like SBT and Scala. SDKMAN saves you from reinventing the wheel by providing a single, consistent interface that works for a variety of different projects.</p> <p>Jabba's main advantage is that's it's cross platform (because it's written in Go). Jabba provides a unified interface on Mac, Windows, and Linux. If you need a cross platform solution, go with Jabba.</p>"},{"location":"java/sdkman-multiple-versions-java-sbt-scala/#conclusion","title":"Conclusion","text":"<p>Managing Java versions was painful for me until I started using SDKMAN.</p> <p>I struggled with manual installations. The Homebrew downloads / jenv switching workflow helped, but didn't solve versioning for SBT, Scala, or Spark. jenv is a great project, but it has some quirks that make it slightly more difficult to work with. Downloading Java versions via Homebrew is annoying because the commands change frequently so the Stackoverflow answers are often stale.</p> <p>SDKMAN will let you manage multiple versions of Java and other core libraries on your machine with minimal hassle. Try it out and you'll be sure to love it.</p>"},{"location":"numpy/save-numpy-text-txt/","title":"Writing NumPy Array to Text Files","text":"<p>This post explains the different ways to save a NumPy array to text files.</p> <p>After showing the different syntax options the post will teach you some better ways to write NumPy data:</p> <ul> <li>using binary file formats that are more flexible</li> <li>writing multiple files in parallel so the operation is faster</li> </ul> <p>Let's dig in!</p>"},{"location":"numpy/save-numpy-text-txt/#writing-one-dimensional-array","title":"Writing one dimensional array","text":"<p>Let's create a 1D array and write it to the <code>Documents</code> directory.</p> <pre><code>import os\nimport numpy as np\n\na = np.array([1, 2, 3])\n\nhome = os.path.expanduser(\"~\")\nnp.savetxt(f\"{home}/Documents/numpy/cat.txt\", a)\n</code></pre> <p>Here are the contents of the <code>cat.txt</code> file:</p> <pre><code>1\n2\n3\n</code></pre> <p>Here's how to get the data to write row-wise instead of column-wise:</p> <pre><code>np.savetxt(f\"{home}/Documents/numpy/dog.txt\", a, newline=\" \")\n</code></pre> <p>Here are the contents of <code>dog.txt</code> (removed zeros for brevity):</p> <pre><code>1 2 3\n</code></pre> <p>You can also convert the 1D array to a 2D array to get the data to write row-wise.</p> <pre><code>np.savetxt(f\"{home}/Documents/numpy/dog2.txt\", [a])\n</code></pre>"},{"location":"numpy/save-numpy-text-txt/#writing-two-dimensional-array","title":"Writing two dimensional array","text":"<p>2D arrays get written row-wise, as would be expected. Let's create a two dimensional array and write it out to demonstrate.</p> <pre><code>b = np.array([[1, 2], [3, 4]])\n\nnp.savetxt(f\"{home}/Documents/numpy/racoon.txt\", b)\n</code></pre> <p>Here are the contents of <code>racoon.txt</code>.</p> <pre><code>1 2\n3 4\n</code></pre> <p>The default save behavior for 2D arrays is more intuitive than for 1D arrays.</p>"},{"location":"numpy/save-numpy-text-txt/#writing-three-dimensional-array","title":"Writing three dimensional array","text":"<p>Let's create a 3D array and try to write it out to a text file.</p> <pre><code>c = np.array([[[1, 2], [3, 4]]])\n\nnp.savetxt(f\"{home}/Documents/numpy/fox.txt\", c)\n</code></pre> <p>Here's the error message that's thrown:</p> <pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/var/folders/d2/116lnkgd0l7f51xr7msb2jnh0000gn/T/ipykernel_10594/1463971899.py in &lt;module&gt;\n----&gt; 1 np.savetxt(f\"{home}/Documents/numpy/fox.txt\", c)\n\n&lt;__array_function__ internals&gt; in savetxt(*args, **kwargs)\n\n~/opt/miniconda3/envs/standard-coiled/lib/python3.9/site-packages/numpy/lib/npyio.py in savetxt(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\n   1380         # Handle 1-dimensional arrays\n   1381         if X.ndim == 0 or X.ndim &gt; 2:\n-&gt; 1382             raise ValueError(\n   1383                 \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n   1384         elif X.ndim == 1:\n\nValueError: Expected 1D or 2D array, got 3D array instead\n</code></pre> <p>Text files cannot handle three dimensional data structures and that's why this operation errors out.</p>"},{"location":"numpy/save-numpy-text-txt/#writing-to-numpy-file-format","title":"Writing to NumPy file format","text":"<p>The NumPy file format can handle three dimensional arrays without issue.</p> <pre><code>np.save(f\"{home}/Documents/numpy/parrot.npy\", c)\n</code></pre> <p>The <code>parrot.npy</code> file is binary and isn't human readable. You can easily read in the file to a NumPy array to inspect the contents instead of opening the file itself.</p> <pre><code>np.load(f\"{home}/Documents/numpy/parrot.npy\")\n\n# array([[[1, 2],\n#         [3, 4]]])\n</code></pre> <p>It's typically best to store data in binary file formats. The files are smaller and human readability isn't usually important.</p>"},{"location":"numpy/save-numpy-text-txt/#scaling-numpy","title":"Scaling NumPy","text":"<p>NumPy is a great library, but it has limitations.</p> <p>NumPy can only handle datasets that are smaller than the memory on your machine. Real world datasets are often too big for NumPy to handle.</p> <p>NumPy analyses can also be slow because they don't process the data in parallel.</p> <p>It's better to process larger datasets with parallel processing frameworks like Dask. Dask splits up the data into multiple underlying NumPy arrays, each of which can be operated on in parallel by the different cores of a machine or cluster.</p>"},{"location":"numpy/save-numpy-text-txt/#conclusion","title":"Conclusion","text":"<p>There are a variety of options when writing NumPy arrays to text files.</p> <p>Text files are not a great file format for binary data. CSV files are slightly better, but face similar limitations, as described here.</p> <p>NumPy binary files or Zarr files are much better. See this post on why Zarr is the best option.</p> <p>Writing data to a single file is slow and can cause memory errors if the dataset is large. It's best to write data to multiple files, in parallel, when the dataset is large relative to the memory on your machine. For really big datasets, it's best to leverage the power of a cluster when writing files in parallel.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/","title":"Creating open source software is a delight","text":"<p>Writing open source software gives you the opportunity to collaborate with highly motivated developers and build awesome code that's used by folks around the world.</p> <p>You can influence community best practices and help others be more productive.</p> <p>At random intervals, you're surprised with wonderful commits from internet strangers that make your code better or offer feedback on how to improve your library.</p> <p>You're forced to write better code by clearly separating application logic from open-sourceable, generic functionality.</p> <p>The open source community will give you great feedback on the quality of your code. You can iterate based on this feedback and build better interfaces (assuming you're psychologically capable of taking feedback).</p> <p>Some folks think open sourcing code is a bad deal. You don't make any money, might be saddled with a difficult maintenance task, and will receive more complaints than positive feedback. This post teaches you how to manage the negatives.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#great-collaboration","title":"Great collaboration","text":"<p>The folks that contribute to open source code smart and motivated.</p> <p>They help you make contributions to your libraries that you might not have the time or technical chops to add yourself.</p> <p>Once establishing a positive working relationship with your contributors, you can bounce ideas off them, even if they aren't directly related to your project.</p> <p>I just emailed one of my contributors a really hard Spark question earlier today. The type of question that is unlikely to get answered on Stackoverflow. Thanks to my open source work, I have great Spark friends who are always willing to help.</p> <p>You can also turn your contributors into IRL friends. One of my Armenian contributors has already offered to give me a tour of Yerevan when I visit his country. He's the main organizer of the Armenian data science community and he'll be able to introduce me to a bunch of great devs.</p> <p>When I go to conferences, I get to catch up with my library users around the world. It's awesome.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#career-growth","title":"Career growth","text":"<p>Developers that build popular open source libraries get high quality job offers, especially if their library is a core dependency.</p> <p>I'm not talking out the \"recruiters\" that send Linkedin messages trying to trick you into applying for a job. I'm talking about messages from the CTO or engineers that are really trying to recruit you. They've seen your code and will actively assist you through the hiring process.</p> <p>One of my open source friends created a great project, donated it to Apache, and was then recruited to a high-paying, interesting, prestigious job. His wonderful open source work is what gave him that opportunity.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#forcing-better-abstractions","title":"Forcing better abstractions","text":"<p>Open source code forces you to separate application logic from generic functionality.</p> <p>An open source abstraction makes your private business logic cleaner because it removes all the generic logic. Private repos that mix generic functions with business logic are unnecessarily complex.</p> <p>Your mileage may vary. Prefer duplication over the wrong abstraction. Developers are generally good at generating business value, but they're not great at making clean abstractions.</p> <p>It hard to acquire library users if you don't provide them with an elegant API.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#is-open-source-extra-work","title":"Is open source extra work?","text":"<p>The open source code you write should be code you'd write in a private repo anyways.</p> <p>Copy pasting generic functions to an open source repo isn't extra work.</p> <p>The challenges you face in private repos should be the motivation for your open source code.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#maintenance-burden","title":"Maintenance burden","text":"<p>Maintaining open source code ranges from a trivial task to a full-time job.</p> <p>If you build an open source project on an unstable API, it'll be more work.</p> <p>Some languages and frameworks are less stable than others. Don't depend on volatile code if you don't want to do a lot of maintenance work.</p> <p>As the open source maintainer, you choose what versions you support. You don't need to support old versions if you don't want to.</p> <p>If you leave an ecosystem or simply don't want to maintain the project anymore, you're free to archive it or tell people to fork it. You don't have any obligations to maintain a project for unpaid users.</p> <p>The maintainer of faker.js recently made an announcement that companies would have to start paying him or fork the lib. That's great, maintainers can quit whenever they want.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#money","title":"Money","text":"<p>You shouldn't build open source code with the intention of making money.</p> <p>Open source code is to help other developers be more productive and to altruistically give back to the community.</p> <p>An open source project that's vital for companies can be a great way to launch a successful company. Apache Spark was started as an open source project and the founder used the open source momentum to launch Databricks, a multi-billion dollar enterprise.</p> <p>While an open source project might turn into a big company, that shouldn't be your motivation.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#unhappy-users","title":"Unhappy users","text":"<p>You won't make all users happy and the more popular your project, the more complaints you'll face.</p> <p>Happy users will use your code and never say anything to you. The unhappy folks may open rude issues or send you nasty emails. Even if the happy/unhappy ratio is 100/1, all you'll get is bad feedback.</p> <p>Open source developers need to accept this asymmetrical feedback as an unfortunate reality of life.</p> <p>If you're happy with an open source project, go out of your way to email the creator to thank them. They put a lot of effort into the project and a nice email goes a long way.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#hypersensitive-developers","title":"Hypersensitive developers","text":"<p>Some developers are really sensitive when it comes to feedback, especially feedback on their code.</p> <p>When you submit a pull request on their code, they almost view it as an invasion of privacy. How dare you refactor their beautiful artwork!</p> <p>If you really don't like people touching your code, then just keep it private.</p> <p>I worked in finance for 5 years and we were always heavily \"refactoring\" presentations, documents, and spreadsheets of other team members. There wasn't a big \"finance documents ownership\" problem.</p> <p>A greater percentage of developers seem to struggle with code ownership. If you struggle with feedback on your code, you might want to stick away from open source development.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#why-open-source-projects-fail","title":"Why open source projects fail","text":"<p>Some developers are disenchanted with open source work because they tried to build a project and it wasn't successful. They worked hard, built something they thought was great, and the users never came.</p> <p>Open source projects can fail for a variety of reasons.</p> <p>The README needs to offer a compelling value proposition to users. Something like \"this library will make your life better for XyzReason\".</p> <p>Most READMEs are not compelling. A lot of them are hard to follow. Most READMEs don't even state the problem the library is designed to solve.</p> <p>Other READMEs give you a quick glimpse into a complicated user interface. Developers don't adopt libraries that are hard to use.</p> <p>You need to market open source projects. You should have a reliable traffic source that guides users to your README, so they can learn all about your great code and start using it.</p> <p>Open source failure is a difficult reality for some developers to face. You put yourself out there, thought you were enlightening the world, and were shunned by potential users.</p> <p>It easier to succeed with closed source projects because you have a captive audience. You might build a component that's not particularity easy to use, but still get adopters because they're basically forced to use your solution. Open source users are not captive and will quickly click the back button on your README if they're not impressed.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#you-dont-need-to-be-a-genius-developer","title":"You don't need to be a genius developer","text":"<p>You don't need to be a great developer to build awesome open source code.</p> <p>Simple functions that make common tasks easier are fine.</p> <p>API wrappers work.</p> <p>Creating revolutionary open source libraries might be your end goal. Start by building a trivial library that gets a few hundred stars.</p>"},{"location":"oss/creating-open-source-software-is-a-delight/#conclusion","title":"Conclusion","text":"<p>Open source development isn't for everyone. Hypersensitive developers don't appreciate feedback from strangers. Developers who think they're code is better than it actually is will be disappointed by lack of adoption.</p> <p>A large chunk of developers can happily exist in the open source ecosystem. They'll find great collaborators to help them with hard questions. They'll find joy in seeing their code collect hundreds of stars and get downloaded thousands of times a day.</p> <p>They'll also be ecstatic when the CTO of a company tracks them down and offers them their dream job out of the blue.</p> <p>It takes years to get traction in open source. Learning how to build code that other developers love takes time. Start by trying to get a few stars, then a hundred, then thousands.</p> <p>Writing open source code and participating in such a collaborative community is my favorite part of being a developer.</p>"},{"location":"pandas/dataframe-cut-category/","title":"Add Category Column to pandas DataFrame with cut","text":"<p>This post explains how to add a category column to a pandas DataFrame with <code>cut()</code>.</p> <p><code>cut</code> makes it easy to categorize numerical values in buckets.</p> <p>Let's look at a a DataFrame of people and categorize them into \"child\", \"teenager\", and \"adult\" buckets based on their age.</p>"},{"location":"pandas/dataframe-cut-category/#simple-example","title":"Simple example","text":"<p>Suppose you have the following DataFrame.</p> <p></p> <p>Here's the code to create the DataFrame.</p> <pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\"first_names\": [\"Todd\", \"Juan\", \"Maria\", \"Hi\", \"Sal\"], \"age\": [12, 13, 19, 20, 42]}\n)\n</code></pre> <p>Add an <code>age_bins</code> column to the DataFrame that categorizes the people into these buckets: child, teenager, or adult.</p> <pre><code>df[\"age_bins\"] = pd.cut(\n    x=df[\"age\"],\n    bins=[1, 12, 19, np.inf],\n    labels=[\"child\", \"teenager\", \"adult\"],\n)\n</code></pre> <p>Here's what the resulting DataFrame contains.</p> <p></p> <p>The rightmost value is inclusive in the <code>bins</code> argument, so the buckets are 1-12, 13-19, and 20-infinity.</p>"},{"location":"pandas/dataframe-cut-category/#dtypes","title":"dtypes","text":"<p>Let's inspect the <code>dtypes</code> of the resulting DataFrame.</p> <pre><code>df.dtypes\n</code></pre> <pre><code>first_names      object\nage               int64\nage_bins       category\ndtype: object\n</code></pre> <p>You can see that <code>age_bins</code> is a category column.</p>"},{"location":"pandas/dataframe-cut-category/#when-to-use-cut","title":"When to use cut","text":"<p>You can use <code>cut</code> when building features for model training.</p> <p>You may want to train a machine learning model based on bucket labels, not actual numerical values. <code>cut</code> makes it really easy for you to transform a numerical value into categories.</p>"},{"location":"pandas/dataframe-cut-category/#conclusion","title":"Conclusion","text":"<p><code>cut</code> is a convenient way to convert a numerical column into different value buckets.</p> <p>You'll find it especially useful when feature engineering.</p>"},{"location":"pandas/large-data-workflow-dask/","title":"Scale big data pandas workflows with Dask","text":"<p>pandas is a great DataFrame library for datasets that fit comfortably in memory, but throws out of memory exceptions for datasets that are too large.</p> <p>This post shows how pandas works well for a small dataset, but can't run a query on a big datasets. It then shows how Dask can run the query on the large dataset, which has a familiar pandas-like API.</p> <p>Dask DataFrames scale workflows by splitting up the dataset into partitions and performing computations on each partition in parallel. Each partition in a Dask DataFrame is a pandas DataFrame. Dask's reliance on pandas is what makes it feel so familiar for pandas users.</p> <p>Let's look at a query that works with pandas on a small dataset and errors out on a large dataset.</p>"},{"location":"pandas/large-data-workflow-dask/#data-query","title":"Data query","text":"<p>Here's a subset of the data that will be used in the queries for this blog post.</p> <p></p> <p>All the queries will use datasets that have this schema. The only difference will be the size of the datasets that are used.</p> <p>We'll be querying the number of unique values in the <code>id</code> column.</p> <p>Follow the instructions in the coiled-datasets repo if you'd like to create the synthetic datasets and run these queries on your machine.</p>"},{"location":"pandas/large-data-workflow-dask/#querying-data-with-pandas","title":"Querying data with pandas","text":"<p>Let's run a query to compute the number of unique values in the <code>id</code> column with pandas on the small dataset. The <code>1-month</code> directory contains 4 Parquet files with 228 MB of data, so they can easily get loaded into a pandas DataFrame.</p> <pre><code>import os\nimport glob\n\nimport pandas as pd\n\nhome = os.path.expanduser(\"~\")\n\npath = f\"{home}/data/timeseries/1-month/parquet\"\nall_files = glob.glob(path + \"/*.parquet\")\n\ndf = pd.concat((pd.read_parquet(f) for f in all_files))\n\n# let's time this query\ndf[[\"id\"]].nunique()\n</code></pre> <p>It only takes 0.4 seconds to find the number of unique elements in the <code>id</code> column when querying this small dataset with pandas.</p> <p>Now let's run the same query on a large dataset with 1,094 files and 58.2 GB of data.</p> <pre><code>path = f\"{home}/data/timeseries/20-years/parquet\"\nall_files = glob.glob(path + \"/*.parquet\")\n\ndf = pd.concat((pd.read_parquet(f) for f in all_files))\n\n# the next line causes a memory error\ndf[[\"id\"]].nunique()\n</code></pre> <p>This query causes the computer to run out of memory, so Python will kill the process.</p> <p>This query was run on a computer with 8 GB of RAM. That's not even close to big enough to handle a query on a 58.2 GB dataset with pandas.</p> <p>Let's see how we can run this query the large <code>20-years</code> dataset using Dask.</p>"},{"location":"pandas/large-data-workflow-dask/#big-data-query-with-dask","title":"Big data query with Dask","text":"<p>Let's run the same query on the large dataset with Dask. The syntax for loading multiple files into a Dask DataFrame is more elegant.</p> <pre><code>import dask\nimport dask.dataframe as dd\n\nfrom dask.distributed import Client\nclient = Client()\n\nddf = dd.read_parquet(\n    f\"{home}/data/timeseries/20-years/parquet\",\n    engine=\"pyarrow\",\n)\n\n# time the following query\nddf[\"id\"].nunique().compute()\n</code></pre> <p>Dask runs this query in 9 seconds.</p> <p>You don't need to explicitly glob files when loading multiple Parquet files into a Dask DataFrame. Dask is built to load multiple files in parallel. It handles the file listing operation for you, under the hood.</p> <p>This example shows how Dask can handle \"out of core\" datasets, which is data that's bigger than RAM. The RAM on my machine is 8 GB and Dask can run queries on a dataset that's 58.2 GB.</p> <p>Dask allows you to \"scale up\" you computations and run computations on each core of your computer in parallel. It also lets you \"scale out\" your computations and distribute workloads to multiple computers in a cluster. This example uses Dask to run computations on a single machine, so it's an example of scaling up.</p> <p>Let's look at when you can expect to run into pandas memory issues and when you should consider switching to Dask.</p>"},{"location":"pandas/large-data-workflow-dask/#how-much-data-can-pandas-handle","title":"How much data can pandas handle?","text":"<p>In the 10 things I hate about pandas article, the author of pandas outlines a RAM / dataset size rule of thumb:</p> <p>pandas rule of thumb: have 5 to 10 times as much RAM as the size of your dataset</p> <p>The article is from 2017 and pandas has gotten more efficient, so the ratio is probably better now. In any case, pandas can't handle datasets that are bigger than the RAM on your machine.</p> <p>There are lots of real world datasets that are much larger than what pandas can handle, so Dask is helpful in a lot of real life scenarios.</p>"},{"location":"pandas/large-data-workflow-dask/#conclusion","title":"Conclusion","text":"<p>pandas can be used for datasets that fit comfortably in memory but will error out for larger datasets.</p> <p>When your data is too large for pandas, you can use Dask instead.</p> <p>Dask uses pandas under the hood and it's easy to convert Dask DataFrames to pandas DataFrames and vice versa. pandas library interoperability is also good with Dask.</p> <p>The Dask DataFrame API intentionally mimics the pandas API, so it'll feel familiar.</p> <p>When your datasets are really huge, you can scale out Dask analyses to the cluster. Dask can easily process terabytes of data.</p> <p>Stick with pandas when your data is sufficiently small and use Dask when your data is large.</p>"},{"location":"pandas/read-delta-lake-dataframe/","title":"Reading Delta Lakes into pandas DataFrames","text":"<p>This post explains how to read Delta Lakes into pandas DataFrames.</p> <p>The delta-rs library makes this incredibly easy and doesn't require any Spark dependencies.</p> <p>Let's look at some simple examples, explore when this is viable, and clarify the limitations of this approach.</p> <p>Delta Lakes are better for managing data compared to vanilla CSV or Parquet lakes. Delta solves a lot of common data management problems, which saves users from addressing these concerns themselves. This post should excite pandas users with an easier way to manage datasets.</p>"},{"location":"pandas/read-delta-lake-dataframe/#create-delta-lake","title":"Create Delta Lake","text":"<p>Let's start by creating a Delta Lake so we can read it into pandas DataFrames in our examples.</p> <p>Use PySpark to create the a Delta Lake:</p> <pre><code>data = [(\"jose\", 10), (\"li\", 12), (\"luisa\", 14)]\ndf = spark.createDataFrame(data, [\"name\", \"num\"])\ndf.write.format(\"delta\").save(\"resources/delta/1\")\n</code></pre> <p>Feel free to clone the dask-interop project and use the example Delta Lakes that are checked into the repo if you don't want to setup Spark on your local machine.</p>"},{"location":"pandas/read-delta-lake-dataframe/#read-delta-lake-into-pandas","title":"Read Delta Lake into pandas","text":"<p>Here's how to read the Delta Lake into a pandas DataFrame.</p> <pre><code>from deltalake import DeltaTable\n\ndt = DeltaTable(\"resources/delta/1\")\ndf = dt.to_pandas()\nprint(df)\n\n    name  num\n0   jose   10\n1     li   12\n2  luisa   14\n</code></pre> <p>You need to run <code>pip install deltalake</code> to install the delta-rs library.</p> <p>delta-rs makes it really easy to read a Delta Lake into a pandas table.</p>"},{"location":"pandas/read-delta-lake-dataframe/#time-travel","title":"Time travel","text":"<p>Delta Lakes also allow for time travel between different versions of the data. Time travel is great when you'd like to use a fixed dataset version for model training.</p> <p>Let's build another Delta Lake with two transactions and demonstrate how to time travel between the two different versions of the data. Start by creating a Delta Lake with two write transactions.</p> <pre><code>data = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\ndf.write.format(\"delta\").save(\"resources/delta/2\")\n\ndata = [(\"d\", 4), (\"e\", 5), (\"f\", 6)]\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\ndf.write.mode(\"append\").format(\"delta\").save(\"resources/delta/2\")\n</code></pre> <p>The first transaction wrote this data (version 0 of the dataset):</p> letter number a 1 b 2 c 3 <p>The second transaction wrote this data (version 1 of the dataset):</p> letter number d 4 e 5 f 6 <p>Read in the entire dataset to a pandas DataFrame.</p> <pre><code>dt = DeltaTable(\"resources/delta/2\")\ndf = dt.to_pandas()\nprint(df)\n\n  letter  number\n0      a       1\n1      b       2\n2      c       3\n3      d       4\n4      e       5\n5      f       6\n</code></pre> <p>Delta Lakes will read the latest version of the data by default.</p> <p>Now time travel back to the first version of the data.</p> <pre><code>dt.load_version(0)\ndf = dt.to_pandas()\nprint(df)\n</code></pre> <pre><code>  letter  number\n0      a       1\n1      b       2\n2      c       3\n</code></pre> <p>You can keep adding additional data to the lake while maintaing easy access to a specific version of the data, which is useful when model training and debugging data weirdness. Vanilla Parquet lakes don't support time travel, which makes debugging painful.</p> <p>delta-rs makes it really easy to time travel between different versions of the data.</p>"},{"location":"pandas/read-delta-lake-dataframe/#schema-evolution","title":"Schema Evolution","text":"<p>Delta also supports schema evolution which makes it possible to read Parquet files with different schemas into the same pandas DataFrame.</p> <p>Perform two transactions to a Delta Lake, one that writes a two column dataset, and another that writes a 3 column dataset. Verify that Delta can use schema evolution to read the different Parquet files into a single pandas DataFrame.</p> <p>Here's the data that'll be written with the two transactions.</p> <p>First transaction:</p> letter number a 1 b 2 c 3 <p>Second transaction:</p> letter number color d 4 red e 5 green f 6 blue <p>Here's the PySpark code to create the Delta Lake:</p> <pre><code>data = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\ndf = spark.createDataFrame(data, [\"letter\", \"number\"])\ndf.write.format(\"delta\").save(\"resources/delta/3\")\n\ndata = [(\"d\", 4, \"red\"), (\"e\", 5, \"blue\"), (\"f\", 6, \"green\")]\ndf = spark.createDataFrame(data, [\"letter\", \"number\", \"color\"])\ndf.write.mode(\"append\").format(\"delta\").save(\"resources/delta/3\")\n</code></pre> <p>Let's read the Delta Lake into a pandas DataFrame and print the results.</p> <pre><code>dt = DeltaTable(\"resources/delta/3\")\ndf = dt.to_pandas()\nprint(df)\n</code></pre> <pre><code>  letter  number  color\n0      a       1   None\n1      b       2   None\n2      c       3   None\n3      d       4    red\n4      e       5   blue\n5      f       6  green\n</code></pre> <p>A normal Parquet reader cannot handle files that have different schemas. Delta allow for columns to be added to data lakes without having to rewrite existing files. This is a handy feature that delta-rs provides out of the box.</p>"},{"location":"pandas/read-delta-lake-dataframe/#limitations-of-pandas","title":"Limitations of pandas","text":"<p>Delta Lakes are normally used for huge datasets and won't be readable into pandas DataFrames. The design pattern outlined in this post will only work for Delta Lakes that are relatively small.</p> <p>If you want to read a large Delta lake, you'll need to use a cluster computing technology like Spark or Dask.</p>"},{"location":"pandas/read-delta-lake-dataframe/#reading-delta-lakes-with-pyspark","title":"Reading Delta Lakes with PySpark","text":"<p>You can also read Delta Lakes and convert them to pandas DataFrames with PySpark.</p> <p>Here's an example:</p> <pre><code>pyspark_df = (\n    spark.read.format(\"delta\").option(\"mergeSchema\", \"true\").load(\"resources/delta/3\")\n)\npandas_df = pyspark_df.toPandas()\n</code></pre> <p>This is the best approach if you have access to a Spark runtime. delta-rs is the best option if you don't have access to Spark (and want to avoid the heavy dependency).</p> <p>You can't convert huge Delta Lakes to pandas DataFrames with PySpark either. When you convert a PySpark DataFrame to pandas, it collects all the data on the driver node and is bound by the memory of the driver node.</p>"},{"location":"pandas/read-delta-lake-dataframe/#conclusion","title":"Conclusion","text":"<p>Delta Lakes are almost always preferable to plain vanilla CSV or Parquet lakes. They allow for time travel, schema evolution, versioned data, and more.</p> <p>This blog post demonstrates how easy it is to read Delta Lakes into pandas DataFrames with delta-rs.</p> <p>Let's hope this post motivates the Python community to start using Delta Lakes so they can save time on \"common data challenges\" that can be offloaded to the file format! It'll be even easier to encourage the Python community to switch to Delta once delta-rs support writes to Delta via Pandas!</p>"},{"location":"pandas/read-multiple-csv-pandas-dataframe/","title":"Read multiple CSVs into pandas DataFrame","text":"<p>This post explains how to read multiple CSVs into a pandas DataFrame.</p> <p>pandas filesystem APIs make it easy to load multiple files stored in a single directory or in nested directories.</p> <p>Other Python libraries can even make this easier and more scalable. Let's take a look at an example on a small dataset.</p>"},{"location":"pandas/read-multiple-csv-pandas-dataframe/#reading-csvs-with-filesystem-functions","title":"Reading CSVs with filesystem functions","text":"<p>Suppose you have the following files.</p> <pre><code>animals/\n  file1.csv\n  file2.csv\n</code></pre> <p>Here's how to load the files into a pandas DataFrame.</p> <pre><code>import glob\nimport os\nimport pandas as pd\n\nall_files = glob.glob(\"animals/*.csv\")\ndf = pd.concat((pd.read_csv(f) for f in all_files))\nprint(df)\n</code></pre> <p>Here's what's printed:</p> <pre><code>  animal_name animal_type\n0        susy     sparrow\n1       larry        lion\n0         dan     dolphin\n1      camila         cat\n</code></pre> <p>This script loads each file into a separate pandas DataFrames and then concatenates all the individual DataFrames into one final result.</p> <p>Here's how to load the files into a pandas DataFrame when the files aren't located in the present working directory. The files are located in the <code>~/Documents/code/coiled/coiled-datasets/data/animals</code> directory on my machine.</p> <pre><code>home = os.path.expanduser(\"~\")\n\npath = f\"{home}/Documents/code/coiled/coiled-datasets/data/animals/\"\nall_files = glob.glob(path + \"/*.csv\")\n\ndf = pd.concat((pd.read_csv(f) for f in all_files))\n\nprint(df)\n</code></pre> <p>This script gives the same result.</p> <p>You'd need to tweak the script to make it multiplatform. It's tedious to write logic to list the files when creating a Pandas DataFrame from multiple files. Let's try Dask which doesn't require us to write the file listing code or worry ourselves with multiplatform compatibility.</p>"},{"location":"pandas/read-multiple-csv-pandas-dataframe/#loading-multiple-files-with-dask","title":"Loading multiple files with Dask","text":"<p>Read the files into a Dask DataFrame with Dask's <code>read_csv</code> method.</p> <pre><code>import dask.dataframe as dd\n\nddf = dd.read_csv(f\"{path}/*.csv\")\n</code></pre> <p>Now convert the Dask DataFrame to a pandas DataFrame with the <code>compute()</code> method and print the contents.</p> <pre><code>df = ddf.compute()\n\nprint(df)\n</code></pre> <p>Here's what's printed:</p> <pre><code>  animal_name animal_type\n0        susy     sparrow\n1       larry        lion\n0         dan     dolphin\n1      camila         cat\n</code></pre> <p>Dask takes care of the file listing operation and doesn't require you to perform it manually. Dask makes it a lot easier to read and write multiple files compared to pandas.</p>"},{"location":"pandas/read-multiple-csv-pandas-dataframe/#benefits-of-dask","title":"Benefits of Dask","text":"<p>Dask is also designed to handle large datasets without erroring out like pandas.</p> <p>Dask splits up data into partitions so it can be processed in parallel. It also allows for computations to be performed in a streaming manner without loading all the data in memory at once.</p> <p>Dask computations can be scaled up to use all the cores of a single machine or scaled out to leverage a cluster of multiple computers in parallel. Dask is a good option whenever you're facing pandas related scaling issues.</p>"},{"location":"pandas/read-multiple-csv-pandas-dataframe/#reading-nested-csvs","title":"Reading nested CSVs","text":"<p>Suppose you'd like to read CSV data into a pandas DataFrame that's stored on disk as follows:</p> <pre><code>fish/\n  files/\n    file1.csv\n  more-files/\n    file2.csv\n    file3.csv\n</code></pre> <p>Load all of these files into a pandas DataFrame and print the result.</p> <pre><code>path = f\"{home}/Documents/code/coiled/coiled-datasets/data/fish/\"\nall_files = glob.glob(path + \"/**/*.csv\")\n\ndf = pd.concat((pd.read_csv(f) for f in all_files))\n\nprint(df)\n</code></pre> <p>Here's the result that's printed:</p> <pre><code>  fish_name fish_type\n0     carol   catfish\n1     maria  mackerel\n0     betty      bass\n1     sally   snapper\n0    marvin    marlin\n1      tony      tuna\n</code></pre> <p><code>glob</code> makes it relatively easy to fetch CSVs that are stored in a nested directory structure.</p>"},{"location":"pandas/read-multiple-csv-pandas-dataframe/#conclusion","title":"Conclusion","text":"<p>This post demonstrates how it's straightforward to load multiple CSV files in a pandas DataFrame.</p> <p>Dask makes it even easier to load multiple files into a Dask DataFrame, which can easily be converted to a pandas DataFrame.</p> <p>pandas can only handle datasets that are small enough to fit into memory (the rule of thumb from 2017 was data should be 5-10 times smaller than RAM). When you're loading multiple CSV files, it's more likely that you're working with a bigger dataset that'll cause pandas memory issues.</p> <p>When datasets are small enough to comfortably fit into memory, pandas is the best option. If you start running into memory issues, or would like you analysis to run faster with parallel computations, try scaling up with Dask.</p>"},{"location":"pandas/rename-columns/","title":"Renaming Columns in Pandas DataFrames","text":"<p>This article explains how to rename a single or multiple columns in a Pandas DataFrame.</p> <p>There are multiple different ways to rename columns and you'll often want to perform this operation, so listen up.</p>"},{"location":"pandas/rename-columns/#simple-example","title":"Simple example","text":"<p>Create a Pandas DataFrame and print the contents.</p> <pre><code>df = pd.DataFrame({\"num\": [1, 2], \"let\": [\"a\", \"b\"]})\nprint(df)\n</code></pre> <pre><code>   num let\n0    1   a\n1    2   b\n</code></pre> <p>Let's rename the columns to be <code>number</code> and <code>letter</code> so they're more descriptive.</p> <pre><code>df.rename({\"num\": \"number\", \"let\": \"letter\"}, axis=\"columns\", inplace=True)\nprint(df)\n</code></pre> <pre><code>   number letter\n0       1      a\n1       2      b\n</code></pre> <p><code>inplace=True</code> causes the DataFrame to be mutated.</p>"},{"location":"pandas/rename-columns/#rename-single-column","title":"Rename single column","text":"<p>Create a DataFrame with <code>first_name</code> and <code>last-name</code> columns.</p> <pre><code>df = pd.DataFrame({\"first_name\": [\"li\", \"karol\"], \"last-name\": [\"Fung\", \"G\"]})\nprint(df)\n</code></pre> <pre><code>  first_name last-name\n0         li      Fung\n1      karol         G\n</code></pre> <p>We don't want one column in our DataFrame to use underscores and the other to use hyphens. Rename the <code>last-name</code> column to be <code>last_name</code>.</p> <pre><code>df.rename({\"last-name\": \"last_name\"}, axis=\"columns\", inplace=True)\nprint(df)\n</code></pre> <pre><code>  first_name last_name\n0         li      Fung\n1      karol         G\n</code></pre> <p>It's easy to rename a single column in a DataFrame and leave the other column names unchanged.</p>"},{"location":"pandas/rename-columns/#apply-function-to-all-column-names","title":"Apply function to all column names","text":"<p>You can also apply a function to all column names. This is especially useful when you have a lot of columns or want to build a reusable transformation that can be applied to different DataFrames.</p> <p>Create a DataFrame with spaces in the column names.</p> <pre><code>df = pd.DataFrame({\"some place\": [\"hawaii\", \"costa rica\"], \"fun activity\": [\"surfing\", \"zip lining\"]})\nprint(df)\n</code></pre> <pre><code>   some place fun activity\n0      hawaii      surfing\n1  costa rica   zip lining\n</code></pre> <p>Write a function that'll replace all the spaces with underscores in the column names.</p> <pre><code>df.rename(lambda x: x.replace(\" \", \"_\"), axis=\"columns\", inplace=True)\nprint(df)\n</code></pre> <pre><code>   some_place fun_activity\n0      hawaii      surfing\n1  costa rica   zip lining\n</code></pre>"},{"location":"pandas/rename-columns/#next-steps","title":"Next steps","text":"<p>Stick to the column renaming methods mentioned in this post and don't use the techniques that were popular in earlier versions of Pandas.</p> <p>This article intentionally omits legacy approaches that shouldn't be used anymore.</p> <p>The Pandas API is flexible and supports all common column renaming use cases:</p> <ul> <li>renaming multiple columns with user specifed names</li> <li>renaming some columns</li> <li>applying a function to all column names</li> </ul> <p>Be happy that Pandas is providing you with such a nice user interface!</p>"},{"location":"pandas/unit-testing-pandas/","title":"Testing Pandas Code","text":"<p>This post explains how to test Pandas code with the built-in test helper methods and with the beavis functions that give more readable error messages.</p> <p>Unit testing helps you write Pandas code that more modular and easier to refactor. It's especially important to test your core Pandas helper methods.</p> <p>This post shows the two most popular types of DataFrame tests:</p> <ol> <li>Column equality assertions</li> <li>Checking entire DataFrame equality</li> </ol>"},{"location":"pandas/unit-testing-pandas/#column-equality","title":"Column equality","text":"<p>Let's create a function that adds a <code>starts_with_s</code> column to a DataFrame that returns <code>True</code> if a string starts with the letter \"s\".</p> <pre><code>def startswith_s(df, input_col, output_col):\n    df[output_col] = df[input_col].str.startswith(\"s\")\n</code></pre> <p>Now let's write a unit test that runs the <code>startswith_s</code> function and make sure it returns the correct result. We'll start with the built-in <code>pd.testing.assert_series_equal</code> method.</p> <pre><code>df = pd.DataFrame({\"col1\": [\"sap\", \"hi\"], \"col2\": [3, 4]})\nstartswith_s(df, \"col1\", \"col1_startswith_s\")\nexpected = pd.Series([True, False], name=\"col1_startswith_s\")\npd.testing.assert_series_equal(df[\"col1_startswith_s\"], expected)\n</code></pre> <p><code>assert_series_equal</code> isn't easy to use cause it requires you to manually create a Pandas Series. Let's write the same test with beavis.</p> <pre><code>df = pd.DataFrame({\"col1\": [\"sap\", \"hi\"], \"col2\": [3, 4], \"expected\": [True, False]})\nstartswith_s(df, \"col1\", \"col1_startswith_s\")\nbeavis.assert_pd_column_equality(df, \"col1_startswith_s\", \"expected\")\n</code></pre> <p>This is cleaner because we can create a single DataFrame and don't need to build a separate Pandas Series. We can make it even cleaner with another helper method for constructing Pandas DataFrames.</p> <p>The built in Pandas constructor forces you to create DataFrames with columns of data. Let's use another beavis helper method to create DataFrames with rows of data and write the same test.</p> <pre><code>df = beavis.create_pdf([(\"sap\", 3, True), (\"hi\", 4, False)], [\"col1\", \"col2\", \"expected\"])\nstartswith_s(df, \"col1\", \"col1_startswith_s\")\nbeavis.assert_pd_column_equality(df, \"col1_startswith_s\", \"expected\")\n</code></pre> <p><code>create_pdf</code> makes the test more readable because it groups the input data and expected output in the same tuple, which is easier to visually parse.</p>"},{"location":"pandas/unit-testing-pandas/#beavis-error-messages","title":"Beavis error messages","text":"<p>The beavis error messages are also more descriptive compared to the built-in Pandas errors.</p> <p>Here's the built-in error message when comparing series that are not equal.</p> <pre><code>df = pd.DataFrame({\"col1\": [1042, 2, 9, 6], \"col2\": [5, 2, 7, 6]})\npd.testing.assert_series_equal(df[\"col1\"], df[\"col2\"])\n</code></pre> <pre><code>&gt;   ???\nE   AssertionError: Series are different\nE\nE   Series values are different (50.0 %)\nE   [index]: [0, 1, 2, 3]\nE   [left]:  [1042, 2, 9, 6]\nE   [right]: [5, 2, 7, 6]\n</code></pre> <p>It's hard to tell which columns are mismatched because they're not aligned.</p> <p>Here's the beavis error message that aligns rows and highlights the mismatches in red.</p> <pre><code>import beavis\n\nbeavis.assert_pd_column_equality(df, \"col1\", \"col2\")\n</code></pre> <p></p> <p>This descriptive error message makes it easier to debug errors and maintain flow.</p>"},{"location":"pandas/unit-testing-pandas/#dataframe-equality","title":"DataFrame equality","text":"<p>You may want to compare the equality of two entire DataFrames as well, not just individual columns.</p> <p>Here's how to compare DataFrame equality with the built-in <code>pandas.testing.assert_frame_equal</code> function.</p> <pre><code>df1 = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\ndf2 = pd.DataFrame({'col1': [5, 2], 'col2': [3, 4]})\npd.testing.assert_frame_equal(df1, df2)\n</code></pre> <pre><code>E   AssertionError: DataFrame.iloc[:, 0] (column name=\"col1\") are different\nE\nE   DataFrame.iloc[:, 0] (column name=\"col1\") values are different (50.0 %)\nE   [index]: [0, 1]\nE   [left]:  [1, 2]\nE   [right]: [5, 2]\n</code></pre> <p><code>assert_frame_equal</code> doesn't provide an error message that's easy to read.</p> <p>beavis provides a nicer error message.</p> <pre><code>beavis.assert_pd_equality(df1, df2)\n</code></pre> <p></p>"},{"location":"pandas/unit-testing-pandas/#custom-equality","title":"Custom equality","text":"<p>You may want to compare DataFrame equality with a custom equality function for certain columns. Here's how to compare two DataFrames with a custom equality function for the float columns.</p> <pre><code>def approx_equality(a, b):\n    return math.isclose(a, b, rel_tol=0.1)\n\ndf1 = beavis.create_pdf([(1.0, \"bye\"), (2.4, \"bye\")], [\"col1\", \"col2\"])\ndf2 = beavis.create_pdf([(1.05, \"bye\"), (3.9, \"bye\")], [\"col1\", \"col2\"])\nbeavis.assert_pd_equality(df1, df2, equality_funs = {\"col1\": approx_equality})\n</code></pre> <p>The <code>approx_equality</code> function returns <code>True</code> if the floating point numbers are within 0.1 of each other. Approximate equality is important for floating point comparisons.</p> <p>These DataFrames aren't considered equal in this example because the floating point numbers are sufficiently far apart. Here's the error message:</p> <p></p>"},{"location":"pandas/unit-testing-pandas/#approximate-equality","title":"Approximate equality","text":"<p>You can also use the <code>assert_approx_pd_equality</code> method to automatically perform approximate equality comparisons for all floating point columns in the DataFrames.</p> <p>Here's an example:</p> <pre><code>df1 = pd.DataFrame({\"col1\": [\"hi\", \"aa\"], \"col2\": [3.05, 3.99]})\ndf2 = pd.DataFrame({\"col1\": [\"hi\", \"aa\"], \"col2\": [3, 4]})\nbeavis.assert_approx_pd_equality(df1, df2, 0.1, check_dtype=False)\n</code></pre> <p><code>col1</code> is a string column so the default equality operator is used. <code>col2</code> is a floating point column, so approximate equality is used with the specified precision (which is <code>0.1</code> in this example).</p>"},{"location":"pandas/unit-testing-pandas/#why-testing-is-important","title":"Why testing is important","text":"<p>Testing help you verify the accuracy of your code locally before running it in production. Your test suite documents the functionality of your codebase and serves as a safety blanket when you're refactoring.</p> <p>If your test suite doesn't break when you refactor, you can rest assured that the refactoring hasn't introduced a breaking change.</p> <p>Refactoring also encourages you to write code that's designed better. Code that's poorly written is harder to test. You'll be forced to rework awkward parts of your code, so the tests are easier to write.</p> <p>Many data engineers / data scientists still haven't embraced testing as part of their normal workflow. In the long run, the time invested in writing tests pays for itself in the form of less production errors / less time dealing with bugs.</p>"},{"location":"pandas/unit-testing-pandas/#great-expectations","title":"Great expectations","text":"<p>The great_expectations library provides an elegant API for performing complex data assertions, similar to the unit tests we've been writing. Great Expectations is more flexible and can accommodate a wider variety of use cases.</p> <p>beavis is designed to be used on small datasets to verify the correctness of your code via unit tests.</p> <p>Great Expectations can be used on large datasets to verify the accuracy of the underlying data. Great Expectations is a great library to use in an ETL pipeline to avoid ingesting a large amount of bad data for example.</p> <p>Both libraries have a place in your PyData stack.</p>"},{"location":"pandas/unit-testing-pandas/#conclusion","title":"Conclusion","text":"<p>Pandas doesn't have a big testing culture like other tech stacks. Most Pandas developers never write tests.</p> <p>Localhost code experimentation and exploratory data analyses don't need tests. Testing is best avoided when experimenting.</p> <p>You should seriously consider writing tests when you're ready to start writing production-grade code. Good software engineering practices also make for more robust data pipelines.</p> <p>The PyData ecosystem has a variety of tools that make it easy to build an automated pipeline that's high quality and handles edge cases with ease. You should be thankful for all the amazing open source contributions that make this tech stack so accessible and easy to use!</p>"},{"location":"pyarrow/arbitrary-metadata-parquet-table/","title":"Writing Custom Metadata to Parquet Files and Columns with PyArrow","text":"<p>Metadata can be written to Parquet files or columns. This blog post explains how to write Parquet files with metadata using PyArrow.</p> <p>Here are some powerful features that Parquet files allow for because they support metadata:</p> <ul> <li>the schema is defined in the footer, so it doesn't need to be inferred</li> <li>the columns have min / max statistics, which allows for parquet predicate pushdown filtering</li> </ul> <p>CSV files don't support these features:</p> <ul> <li>there is no schema, so it needs to be inferred (slow / error prone) or explicitly defined (tedious)</li> <li>there are no column statistics, so predicate pushdown filtering isn't an option</li> </ul> <p>You can add custom metadata to your Parquet files to make your lakes even more powerful for your specific query patterns. For example, you can store additional column metadata to allow for predicate pushdown filters that are more expansive than what can be supported by the min/max column statistics. Predicate pushdown filtering can make some queries run a lot faster.</p> <p>This blog post demonstrates how to add file metadata and column metadata to your Parquet files.</p> <p>If you're just getting started with PyArrow, read this article on analyzing metadata before reading the rest of this article.</p>"},{"location":"pyarrow/arbitrary-metadata-parquet-table/#creating-a-file-with-arbitrary-metadata","title":"Creating a file with arbitrary metadata","text":"<p>Let's read a CSV file into a PyArrow table and write it out as a Parquet file with custom metadata appended to the columns and file schema.</p> <p>Suppose you have the following <code>movies.csv</code> file:</p> <pre><code>movie,release_year\nthree idiots,2009\nher,2013\n</code></pre> <p>Import the necessary PyArrow code libraries and read the CSV file into a PyArrow table:</p> <pre><code>import pyarrow.csv as pv\nimport pyarrow.parquet as pq\nimport pyarrow as pa\n\ntable = pv.read_csv('movies.csv')\n</code></pre> <p>Define a custom schema for the table, with metadata for the columns and the file itself.</p> <pre><code>my_schema = pa.schema([\n    pa.field(\"movie\", \"string\", False, metadata={\"spanish\": \"pelicula\"}),\n    pa.field(\"release_year\", \"int64\", True, metadata={\"portuguese\": \"ano\"})],\n    metadata={\"great_music\": \"reggaeton\"})\n\nt2 = table.cast(my_schema)\n</code></pre> <p>Write out the table as a Parquet file.</p> <pre><code>pq.write_table(t2, 'movies.parquet')\n</code></pre> <p>Let's inspect the metadata of the Parquet file:</p> <pre><code>s = pq.read_table('movies.parquet').schema\n\ns.metadata # =&gt; {b'great_music': b'reggaeton'}\ns.metadata[b'great_music'] # =&gt; b'reggaeton'\n</code></pre> <p>Notice that b-strings, aka byte strings, are used in the metadata dictionaries. Parquet is a binary format and you can't store regular strings in binary file types.</p> <p>Fetch the metadata associated with the <code>release_year</code> column:</p> <pre><code>parquet_file = pq.read_table('movies.parquet')\nparquet_file.schema.field('release_year').metadata[b'portuguese'] # =&gt; b'ano'\n</code></pre>"},{"location":"pyarrow/arbitrary-metadata-parquet-table/#updating-schema-metadata","title":"Updating schema metadata","text":"<p>You can also merge your custom metadata with the existing file metadata.</p> <p>Suppose you have another CSV file with some data on pets:</p> <pre><code>nickname,age\nfofo,3\ntio,1\nlulu,9\n</code></pre> <p>Read in the CSV data to a PyArrow table and demonstrate that the schema metadata is <code>None</code>:</p> <pre><code>table = pv.read_csv('pets1.csv')\ntable.schema.metadata # =&gt; None\n</code></pre> <p>Define some custom metadata and merge it with the existing metadata.</p> <pre><code>custom_metadata = {b'is_furry': b'no_fluffy', b'likes_cats': b'negative'}\nmerged_metadata = { **custom_metadata, **(table.schema.metadata or {}) }\n</code></pre> <p>Create a new PyArrow table with the <code>merged_metadata</code>, write it out as a Parquet file, and then fetch the metadata to make sure it was written out correctly.</p> <pre><code>fixed_table = table.replace_schema_metadata(merged_metadata)\npq.write_table(fixed_table, 'pets1_with_metadata.parquet')\nparquet_table = pq.read_table('pets1_with_metadata.parquet')\nparquet_table.schema.metadata # =&gt; {b'is_furry': b'no_fluffy', b'likes_cats': b'negative'}\nparquet_table.schema.metadata[b'is_furry'] # =&gt; b'no_fluffy'\n</code></pre>"},{"location":"pyarrow/arbitrary-metadata-parquet-table/#closing-thoughts","title":"Closing thoughts","text":"<p>Parquet is a powerful file format, partially because it supports metadata for the file and columns.</p> <p>Storing the data schema in a file is more accurate than inferring the schema and less tedious than specifying the schema when reading the file.</p> <p>Column statistics allow for features like predicate pushdown filtering that significantly speed up some queries.</p> <p>PyArrow makes it easy for you to add your own metadata to the Parquet file or columns, so you can make your Parquet data lakes even more powerful.</p>"},{"location":"pyarrow/parquet-metadata-min-max-statistics/","title":"Analyzing Parquet Metadata and Statistics with PyArrow","text":"<p>The PyArrow library makes it easy to read the metadata associated with a Parquet file.</p> <p>This blog post shows you how to create a Parquet file with PyArrow and review the metadata that contains important information like the compression algorithm and the min / max value of a given column.</p> <p>Parquet files are vital for a lot of data analyses. Knowing how to read Parquet metadata will enable you to work with Parquet files more effectively.</p>"},{"location":"pyarrow/parquet-metadata-min-max-statistics/#converting-a-csv-to-parquet-with-pyarrow","title":"Converting a CSV to Parquet with PyArrow","text":"<p>PyArrow makes it really easy to convert a CSV file into a Parquet file. Suppose you have the following <code>data/people/people1.csv</code> file:</p> <pre><code>first_name,last_name\njose,cardona\njon,smith\n</code></pre> <p>You can read in this CSV file and write out a Parquet file with just a few lines of PyArrow code:</p> <pre><code>import pyarrow.csv as pv\nimport pyarrow.parquet as pq\n\ntable = pv.read_csv('./data/people/people1.csv')\npq.write_table(table, './tmp/pyarrow_out/people1.parquet')\n</code></pre> <p>Let's look at the metadata associated with the Parquet file we just wrote out.</p>"},{"location":"pyarrow/parquet-metadata-min-max-statistics/#fetching-metadata-of-parquet-file","title":"Fetching metadata of Parquet file","text":"<p>Let's create a PyArrow Parquet file object to inspect the metadata:</p> <pre><code>import pyarrow.parquet as pq\n\nparquet_file = pq.ParquetFile('./tmp/pyarrow_out/people1.parquet')\n</code></pre> <pre><code>parquet_file.metadata\n\n&lt;pyarrow._parquet.FileMetaData object at 0x10a3d8650&gt;\n  created_by: parquet-cpp version 1.5.1-SNAPSHOT\n  num_columns: 2\n  num_rows: 2\n  num_row_groups: 1\n  format_version: 1.0\n  serialized_size: 531\n</code></pre> <p>Use the <code>row_group</code> method to get row group metadata:</p> <pre><code>parquet_file.metadata.row_group(0)\n\n&lt;pyarrow._parquet.RowGroupMetaData object at 0x10a3dcdc0&gt;\n  num_columns: 2\n  num_rows: 2\n  total_byte_size: 158\n</code></pre> <p>You can use the <code>column</code> method to get column chunk metadata:</p> <pre><code>parquet_file.metadata.row_group(0).column(0)\n\n&lt;pyarrow._parquet.ColumnChunkMetaData object at 0x10a413a00&gt;\n  file_offset: 78\n  file_path:\n  physical_type: BYTE_ARRAY\n  num_values: 2\n  path_in_schema: first_name\n  is_stats_set: True\n  statistics:\n    &lt;pyarrow._parquet.Statistics object at 0x10a413a50&gt;\n      has_min_max: True\n      min: jon\n      max: jose\n      null_count: 0\n      distinct_count: 0\n      num_values: 2\n      physical_type: BYTE_ARRAY\n      logical_type: String\n      converted_type (legacy): UTF8\n  compression: SNAPPY\n  encodings: ('PLAIN_DICTIONARY', 'PLAIN', 'RLE')\n  has_dictionary_page: True\n  dictionary_page_offset: 4\n  data_page_offset: 35\n  total_compressed_size: 74\n  total_uncompressed_size: 70\n</code></pre> <p>The compression algorithm used by the file is stored in the column chunk metadata and you can fetch it as follows:</p> <pre><code>parquet_file.metadata.row_group(0).column(0).compression # =&gt; 'SNAPPY'\n</code></pre>"},{"location":"pyarrow/parquet-metadata-min-max-statistics/#fetching-parquet-column-statistics","title":"Fetching Parquet column statistics","text":"<p>The min and max values for each column are stored in the metadata as well.</p> <p>Let's create another Parquet file and fetch the min / max statistics via PyArrow.</p> <p>Here's the CSV data.</p> <pre><code>nickname,age\nfofo,3\ntio,1\nlulu,9\n</code></pre> <p>Convert the CSV file to a Parquet file.</p> <pre><code>table = pv.read_csv('./data/pets/pets1.csv')\npq.write_table(table, './tmp/pyarrow_out/pets1.parquet')\n</code></pre> <p>Inspect the Parquet metadata statistics to see the min and max values of the <code>age</code> column.</p> <pre><code>parquet_file = pq.ParquetFile('./tmp/pyarrow_out/pets1.parquet')\nprint(parquet_file.metadata.row_group(0).column(1).statistics)\n</code></pre> <pre><code>&lt;pyarrow._parquet.Statistics object at 0x11ac17eb0&gt;\n  has_min_max: True\n  min: 1\n  max: 9\n  null_count: 0\n  distinct_count: 0\n  num_values: 3\n  physical_type: INT64\n  logical_type: None\n  converted_type (legacy): NONE\n</code></pre> <p>The Parquet metadata statistics can make certain types of queries a lot more efficient. Suppose you'd like to find all the pets that are 10 years or older in a Parquet data lake containing thousands of files. You know that the max age in the <code>tmp/pyarrow_out/pets1.parquet</code> file is 9 based on the Parquet metadata, so you know that none of the data in that file is relevant for your analysis of pets that are 10 or older. You can simply skip the file entirely.</p>"},{"location":"pyarrow/parquet-metadata-min-max-statistics/#num_rows-and-serialized_size","title":"<code>num_rows</code> and <code>serialized_size</code>","text":"<p>The number of rows and dataset size are also included in the Parquet metadata.</p> <p>Big data systems are known to accumulate small files over time with incremental updates. Too many small files can cause performance bottlenecks, so the small files should periodically get compacted into bigger files.</p> <p>You can query the metadata of all the Parquet files in a lake to identify the small files and determine how they should be compacted so the Parquet lake can be queried efficiently.</p>"},{"location":"pyarrow/parquet-metadata-min-max-statistics/#next-steps","title":"Next steps","text":"<p>Parquet files are important when performing analyses with Pandas, Dask, Spark, or AWS services like Athena.</p> <p>Most Parquet file consumers don't know how to access the file metadata. This blog post has taught you an important trick that'll put you ahead of your competition ;)</p>"},{"location":"pyspark/array-arraytype-list/","title":"Working with PySpark ArrayType Columns","text":"<p>This post explains how to create DataFrames with ArrayType columns and how to perform common data processing operations.</p> <p>Array columns are one of the most useful column types, but they're hard for most Python programmers to grok. The PySpark array syntax isn't similar to the list comprehension syntax that's normally used in Python.</p> <p>This post covers the important PySpark array operations and highlights the pitfalls you should watch out for.</p>"},{"location":"pyspark/array-arraytype-list/#create-arraytype-column","title":"Create ArrayType column","text":"<p>Create a DataFrame with an array column.</p> <pre><code>df = spark.createDataFrame(\n    [(\"abc\", [1, 2]), (\"cd\", [3, 4])], [\"id\", \"numbers\"]\n)\ndf.show()\n</code></pre> <pre><code>+---+-------+\n| id|numbers|\n+---+-------+\n|abc| [1, 2]|\n| cd| [3, 4]|\n+---+-------+\n</code></pre> <p>Print the schema of the DataFrame to verify that the <code>numbers</code> column is an array.</p> <pre><code>df.printSchema()\n\nroot\n |-- id: string (nullable = true)\n |-- numbers: array (nullable = true)\n |    |-- element: long (containsNull = true)\n</code></pre> <p><code>numbers</code> is an array of long elements.</p> <p>We can also create this DataFrame using the explicit <code>StructType</code> syntax.</p> <pre><code>from pyspark.sql.types import *\nfrom pyspark.sql import Row\n\nrdd = spark.sparkContext.parallelize(\n    [Row(\"abc\", [1, 2]), Row(\"cd\", [3, 4])]\n)\n\nschema = StructType([\n    StructField(\"id\", StringType(), True),\n    StructField(\"numbers\", ArrayType(IntegerType(), True), True)\n])\n\ndf = spark.createDataFrame(rdd, schema)\n\ndf.show()\n</code></pre> <pre><code>+---+-------+\n| id|numbers|\n+---+-------+\n|abc| [1, 2]|\n| cd| [3, 4]|\n+---+-------+\n</code></pre> <p>The explicit syntax makes it clear that we're creating an <code>ArrayType</code> column.</p>"},{"location":"pyspark/array-arraytype-list/#fetch-value-from-array","title":"Fetch value from array","text":"<p>Add a <code>first_number</code> column to the DataFrame that returns the first element in the <code>numbers</code> array.</p> <pre><code>df.withColumn(\"first_number\", df.numbers[0]).show()\n</code></pre> <pre><code>+---+-------+------------+\n| id|numbers|first_number|\n+---+-------+------------+\n|abc| [1, 2]|           1|\n| cd| [3, 4]|           3|\n+---+-------+------------+\n</code></pre> <p>The PySpark array indexing syntax is similar to list indexing in vanilla Python.</p>"},{"location":"pyspark/array-arraytype-list/#combine-columns-to-array","title":"Combine columns to array","text":"<p>The <code>array</code> method makes it easy to combine multiple DataFrame columns to an array.</p> <p>Create a DataFrame with <code>num1</code> and <code>num2</code> columns:</p> <pre><code>df = spark.createDataFrame(\n    [(33, 44), (55, 66)], [\"num1\", \"num2\"]\n)\ndf.show()\n</code></pre> <pre><code>+----+----+\n|num1|num2|\n+----+----+\n|  33|  44|\n|  55|  66|\n+----+----+\n</code></pre> <p>Add a <code>nums</code> column, which is an array that contains <code>num1</code> and <code>num2</code>:</p> <pre><code>from pyspark.sql.functions import *\n\ndf.withColumn(\"nums\", array(df.num1, df.num2)).show()\n</code></pre> <pre><code>+----+----+--------+\n|num1|num2|    nums|\n+----+----+--------+\n|  33|  44|[33, 44]|\n|  55|  66|[55, 66]|\n+----+----+--------+\n</code></pre>"},{"location":"pyspark/array-arraytype-list/#list-aggregations","title":"List aggregations","text":"<p>Collecting values into a list can be useful when performing aggregations. This section shows how to create an <code>ArrayType</code> column with a group by aggregation that uses <code>collect_list</code>.</p> <p>Create a DataFrame with <code>first_name</code> and <code>color</code> columns that indicate colors some individuals like.</p> <pre><code>df = spark.createDataFrame(\n    [(\"joe\", \"red\"), (\"joe\", \"blue\"), (\"lisa\", \"yellow\")], [\"first_name\", \"color\"]\n)\n\ndf.show()\n</code></pre> <pre><code>+----------+------+\n|first_name| color|\n+----------+------+\n|       joe|   red|\n|       joe|  blue|\n|      lisa|yellow|\n+----------+------+\n</code></pre> <p>Group by <code>first_name</code> and create an <code>ArrayType</code> column with all the colors a given <code>first_name</code> likes.</p> <pre><code>res = (df\n    .groupBy(df.first_name)\n    .agg(collect_list(col(\"color\")).alias(\"colors\")))\n\nres.show()\n</code></pre> <pre><code>+----------+-----------+\n|first_name|     colors|\n+----------+-----------+\n|      lisa|   [yellow]|\n|       joe|[red, blue]|\n+----------+-----------+\n</code></pre> <p>Print the schema to verify that <code>colors</code> is an <code>ArrayType</code> column.</p> <pre><code>res.printSchema()\n\nroot\n |-- first_name: string (nullable = true)\n |-- colors: array (nullable = false)\n |    |-- element: string (containsNull = false)\n</code></pre> <p><code>collect_list</code> shows that some of Spark's API methods take advantage of <code>ArrayType</code> columns as well.</p>"},{"location":"pyspark/array-arraytype-list/#exploding-an-array-into-multiple-rows","title":"Exploding an array into multiple rows","text":"<p>A PySpark array can be exploded into multiple rows, the opposite of <code>collect_list</code>.</p> <p>Create a DataFrame with an <code>ArrayType</code> column:</p> <pre><code>df = spark.createDataFrame(\n    [(\"abc\", [1, 2]), (\"cd\", [3, 4])], [\"id\", \"numbers\"]\n)\n\ndf.show()\n</code></pre> <pre><code>+---+-------+\n| id|numbers|\n+---+-------+\n|abc| [1, 2]|\n| cd| [3, 4]|\n+---+-------+\n</code></pre> <p>Explode the array column, so there is only one number per DataFrame row.</p> <pre><code>df.select(col(\"id\"), explode(col(\"numbers\")).alias(\"number\")).show()\n</code></pre> <pre><code>+---+------+\n| id|number|\n+---+------+\n|abc|     1|\n|abc|     2|\n| cd|     3|\n| cd|     4|\n+---+------+\n</code></pre> <p><code>collect_list</code> collapses multiple rows into a single row. <code>explode</code> does the opposite and expands an array into multiple rows.</p>"},{"location":"pyspark/array-arraytype-list/#advanced-operations","title":"Advanced operations","text":"<p>You can manipulate PySpark arrays similar to how regular Python lists are processed with <code>map()</code>, <code>filter()</code>, and <code>reduce()</code>.</p> <p>Complete discussions for these advance operations are broken out in separate posts:</p> <ul> <li>filtering PySpark arrays</li> <li>mapping PySpark arrays with transform</li> <li>reducing PySpark arrays with aggregate</li> <li>merging PySpark arrays</li> <li>exists and forall</li> </ul> <p>These methods make it easier to perform advance PySpark array operations. In earlier versions of PySpark, you needed to use user defined functions, which are slow and hard to work with.</p> <p>A PySpark DataFrame column can also be converted to a regular Python list, as described in this post. This only works for small DataFrames, see the linked post for the detailed discussion.</p>"},{"location":"pyspark/array-arraytype-list/#writing-to-files","title":"Writing to files","text":"<p>You can write DataFrames with array columns to Parquet files without issue.</p> <pre><code>df = spark.createDataFrame(\n    [(\"abc\", [1, 2]), (\"cd\", [3, 4])], [\"id\", \"numbers\"]\n)\n\nparquet_path = \"/Users/powers/Documents/tmp/parquet_path\"\ndf.write.parquet(parquet_path)\n</code></pre> <p>You cannot write DataFrames with array columns to CSV files:</p> <pre><code>csv_path = \"/Users/powers/Documents/tmp/csv_path\"\ndf.write.csv(csv_path)\n</code></pre> <p>Here's the error you'll get:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 1372, in csv\n    self._jwrite.csv(path)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n    raise converted from None\npyspark.sql.utils.AnalysisException: CSV data source does not support array&lt;bigint&gt; data type.\n</code></pre> <p>This isn't a limitation of Spark - it's a limitation of the CSV file format. CSV files can't handle complex column types like arrays. Parquet files are able to handle complex columns.</p>"},{"location":"pyspark/array-arraytype-list/#unanticipated-type-conversions","title":"Unanticipated type conversions","text":"<p>Let's create a DataFrame with an integer column and a string column to demonstrate the surprising type conversion that takes place when different types are combined in a PySpark array.</p> <pre><code>df = spark.createDataFrame(\n    [(\"a\", 8), (\"b\", 9)], [\"letter\", \"number\"]\n)\ndf.show()\n</code></pre> <pre><code>+------+------+\n|letter|number|\n+------+------+\n|     a|     8|\n|     b|     9|\n+------+------+\n</code></pre> <p>Combine the <code>letter</code> and <code>number</code> columns into an array and then fetch the number from the array.</p> <pre><code>res = (df\n  .withColumn(\"arr\", array(df.letter, df.number))\n  .withColumn(\"number2\", col(\"arr\")[1]))\n\nres.show()\n</code></pre> <pre><code>+------+------+------+-------+\n|letter|number|   arr|number2|\n+------+------+------+-------+\n|     a|     8|[a, 8]|      8|\n|     b|     9|[b, 9]|      9|\n+------+------+------+-------+\n</code></pre> <p>Print the schema to observe the <code>number2</code> column is string type.</p> <pre><code>res.printSchema()\n\nroot\n |-- letter: string (nullable = true)\n |-- number: long (nullable = true)\n |-- arr: array (nullable = false)\n |    |-- element: string (containsNull = true)\n |-- number2: string (nullable = true)\n</code></pre> <p>Regular Python lists can hold values with different types. <code>my_arr = [1, \"a\"]</code> is valid in Python.</p> <p>PySpark arrays can only hold one type. In order to combine <code>letter</code> and <code>number</code> in an array, PySpark needs to convert <code>number</code> to a string.</p> <p>PySpark's type conversion causes you to lose valuable type information. It's arguable that the <code>array</code> function should error out when joining columns with different types, rather than implicitly converting types.</p> <p>It's best for you to explicitly convert types when combining different types into a PySpark array rather than relying on implicit conversions.</p>"},{"location":"pyspark/array-arraytype-list/#next-steps","title":"Next steps","text":"<p>PySpark arrays are useful in a variety of situations and you should master all the information covered in this post.</p> <p>Always use the built-in functions when manipulating PySpark arrays and avoid UDFs whenever possible.</p> <p>PySpark isn't the best for truly massive arrays. As the <code>explode</code> and <code>collect_list</code> examples show, data can be modelled in multiple rows or in an array. You'll need to tailor your data model based on the size of your data and what's most performant with Spark.</p> <p>Grok the advanced array operations linked in this article. The native PySpark array API is powerful enough to handle almost all use cases without requiring UDFs.</p>"},{"location":"pyspark/avoid-dots-periods-column-names/","title":"Avoiding Dots / Periods in PySpark Column Names","text":"<p>Dots / periods in PySpark column names need to be escaped with backticks which is tedious and error-prone.</p> <p>This blog post explains the errors and bugs you're likely to see when you're working with dots in column names and how to eliminate dots from column names.</p>"},{"location":"pyspark/avoid-dots-periods-column-names/#simple-example","title":"Simple example","text":"<p>Let's create a DataFrame with <code>country.name</code> and <code>continent</code> columns.</p> <pre><code>df = spark.createDataFrame(\n    [(\"china\", \"asia\"), (\"colombia\", \"south america\")],\n    [\"country.name\", \"continent\"]\n)\ndf.show()\n</code></pre> <pre><code>+------------+-------------+\n|country.name|    continent|\n+------------+-------------+\n|       china|         asia|\n|    colombia|south america|\n+------------+-------------+\n</code></pre> <p>Here's the error message you'll get when you select <code>country.name</code> without backticks: <code>df.select(\"country.name\")</code>.</p> <pre><code>pyspark.sql.utils.AnalysisException: \"cannot resolve '`country.name`' given input columns: [country.name, continent];;\\n'Project ['country.name]\\n+- LogicalRDD [country.name#0, continent#1], false\\n\"\n</code></pre> <p>Here's how you need to select the column to avoid the error message: <code>df.select(\"</code>country.name<code>\")</code>.</p> <p>Having to remember to enclose a column name in backticks every time you want to use it is really annoying. It's also error prone.</p> <p>Dot notation is used to fetch values from fields that are nested. When you use <code>country.name</code> without backticks, Spark thinks you're trying to fetch the <code>name</code> field that's nested in the <code>country</code> column. Let's create another example with a nested schema to explore the dot syntax in more detail.</p>"},{"location":"pyspark/avoid-dots-periods-column-names/#nested-schemas","title":"Nested schemas","text":"<p>Create a DataFrame with a nested schema:</p> <pre><code>schema = StructType([\n    StructField(\"person.name\", StringType(), True),\n    StructField(\"person\", StructType([\n        StructField(\"name\", StringType(), True),\n        StructField(\"age\", IntegerType(), True)]))\n])\ndata = [\n    (\"charles\", Row(\"chuck\", 42)),\n    (\"lawrence\", Row(\"larry\", 73))\n]\ndf = spark.createDataFrame(data, schema)\ndf.show()\n</code></pre> <pre><code>+-----------+-----------+\n|person.name|     person|\n+-----------+-----------+\n|    charles|[chuck, 42]|\n|   lawrence|[larry, 73]|\n+-----------+-----------+\n</code></pre> <p>In this example, <code>\"person.name\"</code> refers to the <code>name</code> field that's nested in the <code>person</code> column and <code>\"`person.name`\"</code> refers to the <code>person.name</code> column. Let's run a <code>select()</code> query to verify:</p> <pre><code>cols = [\"person\", \"person.name\", \"`person.name`\"]\ndf.select(cols).show()\n</code></pre> <pre><code>+-----------+-----+-----------+\n|     person| name|person.name|\n+-----------+-----+-----------+\n|[chuck, 42]|chuck|    charles|\n|[larry, 73]|larry|   lawrence|\n+-----------+-----+-----------+\n</code></pre> <p>This code is error prone because you might forget backticks and get the wrong result. It's best to eliminate dots from column names.</p>"},{"location":"pyspark/avoid-dots-periods-column-names/#replaces-dots-with-underscores-in-column-names","title":"Replaces dots with underscores in column names","text":"<p>Here's how to replace dots with underscores in DataFrame column names.</p> <pre><code>clean_df = df.toDF(*(c.replace('.', '_') for c in df.columns))\nclean_df.show()\n</code></pre> <pre><code>+-----------+-----------+\n|person_name|     person|\n+-----------+-----------+\n|    charles|[chuck, 42]|\n|   lawrence|[larry, 73]|\n+-----------+-----------+\n</code></pre> <p>You can access the <code>person_name</code> and <code>person.name</code> fields in <code>clean_df</code> without worrying about backticks.</p> <pre><code>clean_df.select(\"person_name\", \"person.name\", \"person.age\").show()\n</code></pre> <pre><code>+-----------+-----+---+\n|person_name| name|age|\n+-----------+-----+---+\n|    charles|chuck| 42|\n|   lawrence|larry| 73|\n+-----------+-----+---+\n</code></pre> <p>Renaming column names in PySpark is an important topic that's covered in this blog post. The design patterns covered in the blog post will make you a better PySpark programmer. Study it carefully.</p>"},{"location":"pyspark/avoid-dots-periods-column-names/#conclusion","title":"Conclusion","text":"<p>Dots in PySpark column names can cause headaches, especially if you have a complicated codebase and need to add backtick escapes in a lot of different places.</p> <p>It's easier to replace the dots in column names with underscores, or another character, so you don't need to worry about escaping.</p> <p>Avoid writing out column names with dots to disk. Don't make youre downstream users deal with backtick escaping either.</p> <p>If you'd like to continue learning about PySpark, check out this blog post on chaining custom transformations. Chaining transformations is essential for creating a clean PySpark codebase.</p>"},{"location":"pyspark/chaining-dataframe-transformations/","title":"Chaining Custom PySpark DataFrame Transformations","text":"<p>PySpark code should generally be organized as single purpose DataFrame transformations that can be chained together for production analyses (e.g. generating a datamart).</p> <p>This blog post demonstrates how to monkey patch the DataFrame object with a transform method, how to define custom DataFrame transformations, and how to chain the function calls.</p> <p>We'll also demonstrate how to run multiple custom transformations with function composition using the cytoolz library.</p> <p>If you're using the Scala API, read this blog post on chaining DataFrame transformations with Scala.</p>"},{"location":"pyspark/chaining-dataframe-transformations/#accessing-dataframe-transform-method","title":"Accessing DataFrame transform method","text":"<p>Spark 3 includes a native DataFrame transform method, so Spark 3 users can skip the rest of this section.</p> <p>Spark 2 users can monkey patch the DataFrame object with a transform method so we can chain DataFrame transformations.</p> <pre><code>from pyspark.sql.dataframe import DataFrame\n\n\ndef transform(self, f):\n    return f(self)\n\n\nDataFrame.transform = transform\n</code></pre> <p>This code snippet is from the quinn project.</p>"},{"location":"pyspark/chaining-dataframe-transformations/#chaining-dataframe-transformations-with-lambda","title":"Chaining DataFrame Transformations with\u00a0lambda","text":"<p>Let's define a couple of simple DataFrame transformations to test the transform method.</p> <pre><code>def with_greeting(df):\n    return df.withColumn(\"greeting\", lit(\"hi\"))\n\ndef with_something(df, something):\n    return df.withColumn(\"something\", lit(something))\n</code></pre> <p>Let's create a DataFrame and then run the <code>with_greeting</code> and <code>with_something</code> DataFrame transformations.</p> <pre><code>data = [(\"jose\", 1), (\"li\", 2), (\"liz\", 3)]\nsource_df = spark.createDataFrame(data, [\"name\", \"age\"])\n\nactual_df = (source_df\n    .transform(lambda df: with_greeting(df))\n    .transform(lambda df: with_something(df, \"crazy\")))\n</code></pre> <pre><code>print(actual_df.show())\n\n+----+---+--------+---------+\n|name|age|greeting|something|\n+----+---+--------+---------+\n|jose|  1|      hi|    crazy|\n|  li|  2|      hi|    crazy|\n| liz|  3|      hi|    crazy|\n+----+---+--------+---------+\n</code></pre> <p>The <code>lambda</code> is optional for custom DataFrame transformations that only take a single DataFrame argument so we can refactor <code>with_greeting</code> line as follows:</p> <pre><code>actual_df = (source_df\n    .transform(with_greeting)\n    .transform(lambda df: with_something(df, \"crazy\")))\n</code></pre> <p>Without the <code>DataFrame#transform</code> method, we would have needed to write code like this:</p> <pre><code>df1 = with_greeting(source_df)\nactual_df = with_something(df1, \"moo\")\n</code></pre> <p>The transform method improves our code by helping us avoid multiple order dependent variable assignments. Creating multiple variables gets especially ugly when 5+ transformations need to be run. You don't want df1, df2, df3, df4, and df5 \ud83d\ude21</p> <p>Let's define a DataFrame transformation with an alternative method signature to allow for easier chaining \ud83d\ude05</p>"},{"location":"pyspark/chaining-dataframe-transformations/#chaining-dataframe-transformations-with-functoolspartial","title":"Chaining DataFrame Transformations with functools.partial","text":"<p>Let's define a <code>with_jacket</code> DataFrame transformation that appends a <code>jacket</code> column to a DataFrame.</p> <pre><code>def with_jacket(word, df):\n    return df.withColumn(\"jacket\", lit(word))\n</code></pre> <p>We'll use the same <code>source_df</code> DataFrame and <code>with_greeting</code> method from before and chain the transformations with <code>functools.partial</code>.</p> <pre><code>from functools import partial\n\nactual_df = (source_df\n    .transform(with_greeting)\n    .transform(partial(with_jacket, \"warm\")))\n</code></pre> <pre><code>print(actual_df.show())\n\n+----+---+--------+------+\n|name|age|greeting|jacket|\n+----+---+--------+------+\n|jose|  1|      hi|  warm|\n|  li|  2|      hi|  warm|\n| liz|  3|      hi|  warm|\n+----+---+--------+------+\n</code></pre> <p><code>functools.partial</code> helps us get rid of the <code>lambda</code> functions, but we can do even better\u2026</p>"},{"location":"pyspark/chaining-dataframe-transformations/#defining-dataframe-transformations-as-nested-functions","title":"Defining DataFrame transformations as nested functions","text":"<p>DataFrame transformations that are defined with nested functions have the most elegant interface for chaining. Let's define a <code>with_funny</code> function that appends a <code>funny</code> column to a DataFrame.</p> <pre><code>def with_funny(something_funny):\n    return lambda df: (\n        df.withColumn(\"funny1\", F.lit(something_funny))\n    )\n</code></pre> <p>We'll use the same <code>source_df</code> DataFrame and <code>with_greeting</code> method from before.</p> <pre><code>actual_df = (source_df\n     .transform(with_greeting)\n     .transform(with_funny(\"haha\")))\n</code></pre> <pre><code>print(actual_df.show())\n\n+----+---+--------+-----+\n|name|age|greeting|funny|\n+----+---+--------+-----+\n|jose|  1|      hi| haha|\n|  li|  2|      hi| haha|\n| liz|  3|      hi| haha|\n+----+---+--------+-----+\n</code></pre> <p>This is much better! \ud83c\udf8a. Thanks for suggesting this implementation hoffrocket!</p> <p>We can also define a custom transformation with an inner function (the inner function underscore in this example).</p> <pre><code>def with_funny(word):\n    def _(df):\n        return df.withColumn(\"funny\", lit(word))\n    return _\n</code></pre> <p>The inner function is named <code>_</code>. If you're going to explicitly name the inner function, using an underscore is a good choice because it's easy to apply consistently throughout the codebase. This design pattern was suggested by the developer that added the <code>transform</code> method to the DataFrame API, see here.</p> <pre><code>def with_funny(word):\n    def _(df):\n        return df.withColumn(\"funny\", lit(word))\n    return _\n</code></pre> <p>The inner function is named <code>_</code>. Naming the inner function as underscore makes it easier to build a consistent codebase, as suggested by the developer that added the <code>transform</code> method to the DataFrame API, see here.</p>"},{"location":"pyspark/chaining-dataframe-transformations/#function-composition-with-cytoolz","title":"Function composition with\u00a0cytoolz","text":"<p>We can define custom DataFrame transformations with the <code>@curry</code> decorator and run them with function composition provided by <code>cytoolz</code>.</p> <pre><code>from cytoolz import curry\nfrom cytoolz.functoolz import compose\n\n@curry\ndef with_stuff1(arg1, arg2, df):\n    return df.withColumn(\"stuff1\", lit(f\"{arg1} {arg2}\"))\n\n@curry\ndef with_stuff2(arg, df):\n    return df.withColumn(\"stuff2\", lit(arg))\ndata = [(\"jose\", 1), (\"li\", 2), (\"liz\", 3)]\nsource_df = spark.createDataFrame(data, [\"name\", \"age\"])\n\npipeline = compose(\n    with_stuff1(\"nice\", \"person\"),\n    with_stuff2(\"yoyo\")\n)\nactual_df = pipeline(source_df)\n</code></pre> <pre><code>print(actual_df.show())\n\n+----+---+------+-----------+\n|name|age|stuff2|     stuff1|\n+----+---+------+-----------+\n|jose|  1|  yoyo|nice person|\n|  li|  2|  yoyo|nice person|\n| liz|  3|  yoyo|nice person|\n+----+---+------+-----------+\n</code></pre> <p>The <code>compose</code> function applies transformations from right to left (bottom to top). We can modify the function to apply the transformations from left to right (top to bottom):</p> <pre><code>pipeline = compose(*reversed([\n    with_stuff1(\"nice\", \"person\"),\n    with_stuff2(\"yoyo\")\n]))\nactual_df = pipeline(source_df)\n</code></pre> <pre><code>print(actual_df.show())\n\n+----+---+-----------+------+\n|name|age|     stuff1|stuff2|\n+----+---+-----------+------+\n|jose|  1|nice person|  yoyo|\n|  li|  2|nice person|  yoyo|\n| liz|  3|nice person|  yoyo|\n+----+---+-----------+------+\n</code></pre> <p>Custom transformations are often order dependent and running them from left to right may be required.</p> <p>Follow the best practices outlined in this post to make it easier to write code with dependencies with cytoolz.</p>"},{"location":"pyspark/chaining-dataframe-transformations/#custom-transformations-make-testing-easier","title":"Custom transformations make testing easier","text":"<p>Custom transformations encourage developers to write code that's easy to test. The code logic is broken up into a bunch of single purpose functions that are easy to understand.</p> <p>Read this blog post on testing PySpark code for examples of how to test custom transformations.</p>"},{"location":"pyspark/chaining-dataframe-transformations/#chaining-custom-transformations-with-the-scala-api","title":"Chaining custom transformations with the Scala\u00a0API","text":"<p>The Scala API defines a Dataset#transform method that makes it easy to chain custom transformations. The Scala programming lanaguage allows for multiple parameter lists, so you don't need to define nested functions.</p> <p>Chaining custom DataFrame transformations is easier with the Scala API, but still necessary when writing PySpark code!</p> <p>This blog post explains how to chain DataFrame transformations with the Scala API.</p>"},{"location":"pyspark/chaining-dataframe-transformations/#next-steps","title":"Next steps","text":"<p>You should organize your code as single purpose DataFrame transformations that are tested individually.</p> <p>Following dependency management and project organization best practices will make your life a lot easier as a PySpark developer. Your development time should be mixed between experimentation in notebooks and coding with software engineering best practices in GitHub repos - both are important.</p> <p>Use the transform method to chain your DataFrame transformations and run production analyses. Any DataFrame transformations that make assumptions about the underlying schema of a DataFrame should be validated with the quinn DataFrame validation helper methods.</p> <p>If you're writing PySpark code properly, you should be using the transform method quite frequently \ud83d\ude09</p>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/","title":"Converting a PySpark DataFrame Column to a Python List","text":"<p>There are several ways to convert a PySpark DataFrame column to a Python list, but some approaches are much slower / likely to error out with OutOfMemory exceptions than others!</p> <p>This blog post outlines the different approaches and explains the fastest method for large lists. It'll also explain best practices and the limitations of collecting data in lists.</p> <p>If you're collecting a small amount of data, the approach doesn't matter that much, but if you're collecting a lot of data or facing out of memory exceptions, it's important for you to read this post in detail.</p>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#benchmarking-summary","title":"Benchmarking summary","text":"<p>Suppose you have the following DataFrame:</p> <pre><code>+---+-----+\n|mvv|count|\n+---+-----+\n|  1|    5|\n|  2|    9|\n|  3|    3|\n|  4|    1|\n+---+-----+\n</code></pre> <p>Here's how to convert the <code>mvv</code> column to a Python list with <code>toPandas</code>.</p> <pre><code>list(df.select('mvv').toPandas()['mvv']) # =&gt; [1, 2, 3, 4]\n</code></pre> <p>This table summarizes the runtime for each approach in seconds for datasets with one thousand, one hundred thousand, and one hundred million rows.</p> One thousand Hundred thousand Hundred million toPandas 0.32 0.35 18.12 flatMap 0.55 0.5 57.5 map 0.39 0.77 60.6 list comprehension 0.85 5.18 toLocalIterator 1.44 1.34 <p>Here's a graphical representation of the benchmarking results:</p> <p></p> <p>The list comprehension approach failed and the toLocalIterator took more than 800 seconds to complete on the dataset with a hundred million rows, so those results are excluded.</p>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#code-for-each-approach","title":"Code for each approach","text":"<p>Here's the <code>toPandas</code> code:</p> <pre><code>list(df.select('mvv').toPandas()['mvv']) # =&gt; [1, 2, 3, 4]\n</code></pre> <p><code>toPandas</code> was significantly improved in Spark 2.3. Make sure you're using a modern version of Spark to take advantage of these huge performance gains.</p> <p>Here's the <code>flatMap</code> code:</p> <pre><code>df.select('mvv').rdd.flatMap(lambda x: x).collect()\n</code></pre> <p>Here's the <code>map</code> code:</p> <pre><code>df.select('mvv').rdd.map(lambda row : row[0]).collect()\n</code></pre> <p>Here's the <code>collect()</code> list comprehension code:</p> <pre><code>[row[0] for row in df.select('mvv').collect()]\n</code></pre> <p>Here's the <code>toLocalIterator</code> list comprehension code:</p> <pre><code>[r[0] for r in df.select('mvv').toLocalIterator()]\n</code></pre>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#benchmarking-details","title":"Benchmarking details","text":"<p>The benchmarking analysis was run on cluster with a driver node and 5 worker nodes. The ec2 instances used were i3.xlarge (30.5 GB of RAM and 4 cores each) using Spark 2.4.5. Each dataset was broken into 20 files that were stored in S3.</p>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#best-practices-when-creating-lists-from-dataframes","title":"Best practices when creating lists from DataFrames","text":"<p>You want to collect as little data to the driver node as possible. Collecting data transfers all the data from the worker nodes to the driver node which is slow and only works for small datasets.</p> <p>Spark is powerful because it lets you process data in parallel. If the driver node is the only node that's processing and the other nodes are sitting idle, then you aren't harnessing the power of the Spark engine.</p> <p>It's best to avoid collecting data to lists and figure out to solve problems in a parallel manner.</p> <p>Collecting data to a Python list and then iterating over the list will transfer all the work to the driver node while the worker nodes sit idle. This design pattern is a common bottleneck in PySpark analyses.</p> <p>If you must collect data to the driver node to construct a list, try to make the size of the data that's being collected smaller first:</p> <ul> <li>run a <code>select()</code> to only collect the columns you need</li> <li>run aggregations</li> <li>deduplicate with <code>distinct()</code></li> </ul> <p>Don't collect extra data to the driver node and iterate over the list to clean the data. Organize the data in the DataFrame, so you can collect the list with minimal work.</p>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#what-happens-if-you-collect-too-much-data","title":"What happens if you collect too much data","text":"<p>The driver node can only handle so much data. If you run <code>list(df.select('mvv').toPandas()['mvv'])</code> on a dataset that's too large you'll get this error message:</p> <pre><code>UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.fallback.enabled' does not have an effect on failures in the middle of computation.\n\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 9 tasks (4.2 GB) is bigger than spark.driver.maxResultSize (4.0 GB)\n</code></pre> <p>If you run <code>[row[0] for row in df.select('mvv').collect()]</code> on a dataset that's too large, you'll get this error message (on Databricks):</p> <pre><code>ConnectException error: This is often caused by an OOM error that causes the connection to the Python REPL to be closed. Check your query's memory usage.\n</code></pre> <p>The dreaded java.lang.OutOfMemoryError.</p> <p>There is only so much data that can be collected to a Python list. Spark will error out if you try to collect too much data.</p>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#how-to-collect-multiple-lists","title":"How to collect multiple lists","text":"<p>Suppose you'd like to collect two columns from a DataFrame to two separate lists.</p> <p>It's best to run the collect operation once and then split up the data into two lists. We want to avoid collecting data to the driver node whenever possible. Collecting once is better than collecting twice.</p> <p>Here's an example of collecting one and then splitting out into two lists:</p> <pre><code>df = spark.createDataFrame([(1, 5), (2, 9), (3, 3), (4, 1)], [\"mvv\", \"count\"])\ncollected = df.select('mvv', 'count').toPandas()\nmvv = list(collected['mvv'])\ncount = list(collected['count'])\n</code></pre>"},{"location":"pyspark/column-to-list-collect-tolocaliterator/#next-steps","title":"Next steps","text":"<p>Newbies often fire up Spark, read in a DataFrame, convert it to Pandas, and perform a \"regular Python analysis\" wondering why Spark is so slow! They might even resize the cluster and wonder why doubling the computing power doesn't help.</p> <p>Collecting data to a Python list is one example of this \"do everything on the driver node antipattern\".</p> <p>Sometimes it's nice to build a Python list but do it sparingly and always brainstorm better approaches. Keep data spread across the worker nodes, so you can run computations in parallel and use Spark to its true potential.</p>"},{"location":"pyspark/concat-array-union-except-intersect/","title":"Combining PySpark arrays with concat, union, except and intersect","text":"<p>This post shows the different ways to combine multiple PySpark arrays into a single array.</p> <p>These operations were difficult prior to Spark 2.4, but now there are built-in functions that make combining arrays easy.</p>"},{"location":"pyspark/concat-array-union-except-intersect/#concat","title":"concat","text":"<p><code>concat</code> joins two array columns into a single array.</p> <p>Creating a DataFrame with two array columns so we can demonstrate with an example.</p> <pre><code>df = spark.createDataFrame(\n    [([\"a\", \"a\", \"b\", \"c\"], [\"c\", \"d\"])], [\"arr1\", \"arr2\"]\n)\ndf.show()\n</code></pre> <pre><code>+------------+------+\n|        arr1|  arr2|\n+------------+------+\n|[a, a, b, c]|[c, d]|\n+------------+------+\n</code></pre> <p>Concatenate the two arrays with <code>concat</code>:</p> <pre><code>res = df.withColumn(\"arr_concat\", concat(col(\"arr1\"), col(\"arr2\")))\nres.show()\n</code></pre> <pre><code>+------------+------+------------------+\n|        arr1|  arr2|        arr_concat|\n+------------+------+------------------+\n|[a, a, b, c]|[c, d]|[a, a, b, c, c, d]|\n+------------+------+------------------+\n</code></pre> <p>Notice that <code>arr_concat</code> contains duplicate values.</p> <p>We can remove the duplicates with <code>array_distinct</code>:</p> <pre><code>df.withColumn(\n    \"arr_concat_distinct\", array_distinct(concat(col(\"arr1\"), col(\"arr2\")))\n).show()\n</code></pre> <pre><code>+------------+------+-------------------+\n|        arr1|  arr2|arr_concat_distinct|\n+------------+------+-------------------+\n|[a, a, b, c]|[c, d]|       [a, b, c, d]|\n+------------+------+-------------------+\n</code></pre> <p>Let's look at another way to return a distinct concatenation of two arrays that isn't as verbose.</p>"},{"location":"pyspark/concat-array-union-except-intersect/#array_union","title":"<code>array_union</code>","text":"<p><code>array_union</code> combines two arrays, without any duplicates.</p> <pre><code>res = df.withColumn(\"arr_union\", array_union(col(\"arr1\"), col(\"arr2\")))\nres.show()\n</code></pre> <pre><code>+------------+------+------------+\n|        arr1|  arr2|   arr_union|\n+------------+------+------------+\n|[a, a, b, c]|[c, d]|[a, b, c, d]|\n+------------+------+------------+\n</code></pre> <p>We can get the same result by nesting <code>concat</code> in <code>array_distinct</code>, but that's less efficient and unnecessarily verbose.</p>"},{"location":"pyspark/concat-array-union-except-intersect/#array_intersect","title":"<code>array_intersect</code>","text":"<p><code>array_intersect</code> returns the elements that are in both arrays.</p> <pre><code>res = df.withColumn(\"arr_intersect\", array_intersect(col(\"arr1\"), col(\"arr2\")))\nres.show()\n</code></pre> <pre><code>+------------+------+-------------+\n|        arr1|  arr2|arr_intersect|\n+------------+------+-------------+\n|[a, a, b, c]|[c, d]|          [c]|\n+------------+------+-------------+\n</code></pre> <p>In our example, <code>c</code> is the only element that's in both <code>arr1</code> and <code>arr2</code>.</p>"},{"location":"pyspark/concat-array-union-except-intersect/#array_except","title":"<code>array_except</code>","text":"<p><code>array_except</code> returns a distinct list of the elements that are in <code>arr1</code>, but not in <code>arr2</code>.</p> <pre><code>res = df.withColumn(\"arr_except\", array_except(col(\"arr1\"), col(\"arr2\")))\nres.show()\n</code></pre> <pre><code>+------------+------+----------+\n|        arr1|  arr2|arr_except|\n+------------+------+----------+\n|[a, a, b, c]|[c, d]|    [a, b]|\n+------------+------+----------+\n</code></pre> <p><code>a</code> and <code>b</code> are in <code>arr1</code> and not in <code>arr2</code>.</p>"},{"location":"pyspark/concat-array-union-except-intersect/#conclusion","title":"Conclusion","text":"<p>Several functions were added in PySpark 2.4 that make it significantly easier to work with array columns.</p> <p>Earlier versions of Spark required you to write UDFs to perform basic array functions which was tedious.</p> <p>This post doesn't cover all the important array functions. Make sure to also learn about the exists and forall functions and the transform / filter functions.</p> <p>You'll be a PySpark array master once you're comfortable with these functions.</p>"},{"location":"pyspark/constant-column-lit-typedlit/","title":"Adding constant columns with lit and typedLit to PySpark DataFrames","text":"<p>This post explains how to add constant columns to PySpark DataFrames with <code>lit</code> and <code>typedLit</code>.</p> <p>You'll see examples where these functions are useful and when these functions are invoked implicitly.</p> <p><code>lit</code> and <code>typedLit</code> are easy to learn and all PySpark programmers need to be comfortable using them.</p>"},{"location":"pyspark/constant-column-lit-typedlit/#simple-lit-example","title":"Simple lit example","text":"<p>Create a DataFrame with <code>num</code> and <code>letter</code> columns.</p> <pre><code>df = spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"num\", \"letter\"])\ndf.show()\n</code></pre> <pre><code>+---+------+\n|num|letter|\n+---+------+\n|  1|     a|\n|  2|     b|\n+---+------+\n</code></pre> <p>Add a <code>cool</code> column to the DataFrame with the constant value 23.</p> <pre><code>from pyspark.sql.functions import *\n\ndf.withColumn(\"cool\", lit(23)).show()\n</code></pre> <pre><code>+---+------+----+\n|num|letter|cool|\n+---+------+----+\n|  1|     a|  23|\n|  2|     b|  23|\n+---+------+----+\n</code></pre> <p>Let's try this code without using <code>lit</code>:</p> <pre><code>df.withColumn(\"cool\", 23).show()\n</code></pre> <p>That'll give you the following error:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\", line 2454, in withColumn\n    assert isinstance(col, Column), \"col should be Column\"\nAssertionError: col should be Column\n</code></pre> <p>The second argument to <code>withColumn</code> must be a Column object and cannot be an integer.</p>"},{"location":"pyspark/constant-column-lit-typedlit/#add-constant-value-to-column","title":"Add constant value to column","text":"<p>Let's add 5 to the <code>num</code> column:</p> <pre><code>df.withColumn(\"num_plus_5\", df.num + lit(5)).show()\n</code></pre> <pre><code>+---+------+----------+\n|num|letter|num_plus_5|\n+---+------+----------+\n|  1|     a|         6|\n|  2|     b|         7|\n+---+------+----------+\n</code></pre> <p><code>df.num</code> and <code>lit(5)</code> both return Column objects, as you can observe in the PySpark console.</p> <pre><code>&gt;&gt;&gt; df.num\nColumn&lt;'num'&gt;\n&gt;&gt;&gt; lit(5)\nColumn&lt;'5'&gt;\n</code></pre> <p>The <code>+</code> operator works when both operands are Column objects.</p> <p>The <code>+</code> operator will also work if one operand is a Column object and the other is an integer.</p> <pre><code>df.withColumn(\"num_plus_5\", df.num + 5).show()\n</code></pre> <pre><code>+---+------+----------+\n|num|letter|num_plus_5|\n+---+------+----------+\n|  1|     a|         6|\n|  2|     b|         7|\n+---+------+----------+\n</code></pre> <p>PySpark implicitly converts 5 (an integer) to a Column object and that's why this code works. Let's refresh our understanding of implicit conversions in Python.</p>"},{"location":"pyspark/constant-column-lit-typedlit/#python-type-conversions","title":"Python type conversions","text":"<p>Let's look at how integers and floating point numbers are added with Python to illustrate the implicit conversion behavior.</p> <p>An integer cannot be added with a floating point value without type conversion. Language designers either need to throw an error when users add ints and floats or convert the int to a float and then perform the addition. Python language designers made the decision to implicitly convert integers to floating point values in this situation.</p> <p>Here's an example that uses implicit conversion.</p> <pre><code>3 + 1.2 # 4.2\n</code></pre> <p>Programmers can also explicitly convert integers to floating point values, so no implicit conversions are needed.</p> <pre><code>float(3) + 1.2 # 4.2\n</code></pre> <p>Python doesn't always perform implicit type conversions. This code will error out for example:</p> <pre><code>\"hi\" + 3\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: can only concatenate str (not \"int\") to str\n</code></pre> <p>You need to make an explicit type conversion if you'd like to concatenate a string with an integer in Python.</p> <pre><code>\"hi\" + str(3) # 'hi3'\n</code></pre>"},{"location":"pyspark/constant-column-lit-typedlit/#pyspark-implicit-type-conversions","title":"PySpark implicit type conversions","text":"<p>Let's look at how PySpark implicitly converts integers to Columns with some console experimentation.</p> <pre><code># implicit conversion\n&gt;&gt;&gt; col(\"num\") + 5\nColumn&lt;'(num + 5)'&gt;\n\n# explicit conversion\n&gt;&gt;&gt; col(\"num\") + lit(5)\nColumn&lt;'(num + 5)'&gt;\n</code></pre> <p>It's best to use <code>lit</code> and perform explicit conversions, so the intentions of your code are clear. You should avoid relying on implicit conversion rules that may behave unexpectedly in certain situations.</p>"},{"location":"pyspark/constant-column-lit-typedlit/#array-constant-column","title":"Array constant column","text":"<p>The Scala API has a <code>typedLit</code> function to handle complex types like arrays, but there is no such method in the PySpark API, so hacks are required.</p> <p>Here's how to add a constant <code>[5, 8]</code> array column to the DataFrame.</p> <pre><code>df.withColumn(\"nums\", array(lit(5), lit(8))).show()\n</code></pre> <pre><code>+---+------+------+\n|num|letter|  nums|\n+---+------+------+\n|  1|     a|[5, 8]|\n|  2|     b|[5, 8]|\n+---+------+------+\n</code></pre> <p>This code does not work.</p> <pre><code>df.withColumn(\"nums\", lit([5, 8])).show()\n</code></pre> <p>It errors out as follows:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\", line 98, in lit\n    return col if isinstance(col, Column) else _invoke_function(\"lit\", col)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\", line 58, in _invoke_function\n    return Column(jf(*args))\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 111, in deco\n    return f(*a, **kw)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: java.lang.RuntimeException: Unsupported literal type class java.util.ArrayList [5, 8]\n    at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:90)\n    at org.apache.spark.sql.catalyst.expressions.Literal$.$anonfun$create$2(literals.scala:152)\n    at scala.util.Failure.getOrElse(Try.scala:222)\n    at org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:152)\n    at org.apache.spark.sql.functions$.typedLit(functions.scala:131)\n    at org.apache.spark.sql.functions$.lit(functions.scala:114)\n    at org.apache.spark.sql.functions.lit(functions.scala)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n</code></pre>"},{"location":"pyspark/constant-column-lit-typedlit/#next-steps","title":"Next steps","text":"<p>You've learned how to add constant columns to DataFrames in this post. You've also learned about type conversion in PySpark and how the <code>lit</code> function is used implicitly in certain situations.</p> <p>There are times when you can omit <code>lit</code> and rely on implicit type conversions, but it's better to write explicit PySpark code and invoke <code>lit</code> whenever it's needed.</p>"},{"location":"pyspark/dict-map-to-multiple-columns/","title":"Converting a PySpark Map / Dictionary to Multiple Columns","text":"<p>Python dictionaries are stored in PySpark map columns (the <code>pyspark.sql.types.MapType</code> class). This blog post explains how to convert a map into multiple columns.</p> <p>You'll want to break up a map to multiple columns for performance gains and when writing data to different types of data stores. It's typically best to avoid writing complex columns.</p>"},{"location":"pyspark/dict-map-to-multiple-columns/#creating-a-dataframe-with-a-maptype-column","title":"Creating a DataFrame with a MapType column","text":"<p>Let's create a DataFrame with a map column called <code>some_data</code>:</p> <pre><code>data = [(\"jose\", {\"a\": \"aaa\", \"b\": \"bbb\"}), (\"li\", {\"b\": \"some_letter\", \"z\": \"zed\"})]\ndf = spark.createDataFrame(data, [\"first_name\", \"some_data\"])\ndf.show(truncate=False)\n</code></pre> <pre><code>+----------+----------------------------+\n|first_name|some_data                   |\n+----------+----------------------------+\n|jose      |[a -&gt; aaa, b -&gt; bbb]        |\n|li        |[b -&gt; some_letter, z -&gt; zed]|\n+----------+----------------------------+\n</code></pre> <p>Use <code>df.printSchema</code> to verify the type of the <code>some_data</code> column:</p> <pre><code>root\n |-- first_name: string (nullable = true)\n |-- some_data: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n</code></pre> <p>You can see <code>some_data</code> is a MapType column with string keys and values.</p> <p>Add a <code>some_data_a</code> column that grabs the value associated with the key <code>a</code> in the <code>some_data</code> column. The <code>getItem</code> method helps when fetching values from PySpark maps.</p> <pre><code>df.withColumn(\"some_data_a\", F.col(\"some_data\").getItem(\"a\")).show(truncate=False)\n</code></pre> <pre><code>+----------+----------------------------+-----------+\n|first_name|some_data                   |some_data_a|\n+----------+----------------------------+-----------+\n|jose      |[a -&gt; aaa, b -&gt; bbb]        |aaa        |\n|li        |[b -&gt; some_letter, z -&gt; zed]|null       |\n+----------+----------------------------+-----------+\n</code></pre> <p>This syntax also works:</p> <pre><code>df.withColumn(\"some_data_a\", F.col(\"some_data\")[\"a\"]).show()\n</code></pre>"},{"location":"pyspark/dict-map-to-multiple-columns/#manually-expanding-the-dataframe","title":"Manually expanding the DataFrame","text":"<p>We can manually append the <code>some_data_a</code>, <code>some_data_b</code>, and <code>some_data_z</code> columns to our DataFrame as follows:</p> <pre><code>df\\\n    .withColumn(\"some_data_a\", F.col(\"some_data\").getItem(\"a\"))\\\n    .withColumn(\"some_data_b\", F.col(\"some_data\").getItem(\"b\"))\\\n    .withColumn(\"some_data_z\", F.col(\"some_data\").getItem(\"z\"))\\\n    .show(truncate=False)\n</code></pre> <pre><code>+----------+----------------------------+-----------+-----------+-----------+\n|first_name|some_data                   |some_data_a|some_data_b|some_data_z|\n+----------+----------------------------+-----------+-----------+-----------+\n|jose      |[a -&gt; aaa, b -&gt; bbb]        |aaa        |bbb        |null       |\n|li        |[b -&gt; some_letter, z -&gt; zed]|null       |some_letter|zed        |\n+----------+----------------------------+-----------+-----------+-----------+\n</code></pre> <p>We can refactor this code to be more concise and to generate a more efficient parsed logical plan.</p> <pre><code>cols = [F.col(\"first_name\")] + list(map(\n    lambda f: F.col(\"some_data\").getItem(f).alias(str(f)),\n    [\"a\", \"b\", \"z\"]))\ndf.select(cols).show()\n</code></pre> <pre><code>+----------+----+-----------+----+\n|first_name|   a|          b|   z|\n+----------+----+-----------+----+\n|      jose| aaa|        bbb|null|\n|        li|null|some_letter| zed|\n+----------+----+-----------+----+\n</code></pre> <p>Manually appending the columns is fine if you know all the distinct keys in the map. If you don't know all the distinct keys, you'll need a programatic solution, but be warned - this approach is slow!</p>"},{"location":"pyspark/dict-map-to-multiple-columns/#programatically-expanding-the-dataframe","title":"Programatically expanding the DataFrame","text":"<p>Here's the code to programatically expand the DataFrame (keep reading to see all the steps broken down individually):</p> <pre><code>keys_df = df.select(F.explode(F.map_keys(F.col(\"some_data\")))).distinct()\nkeys = list(map(lambda row: row[0], keys_df.collect()))\nkey_cols = list(map(lambda f: F.col(\"some_data\").getItem(f).alias(str(f)), keys))\nfinal_cols = [F.col(\"first_name\")] + key_cols\ndf.select(final_cols).show()\n</code></pre> <pre><code>+----------+----+-----------+----+\n|first_name|   z|          b|   a|\n+----------+----+-----------+----+\n|      jose|null|        bbb| aaa|\n|        li| zed|some_letter|null|\n+----------+----+-----------+----+\n</code></pre> <p>Let's break down each step of this code.</p> <p>Step 1: Create a DataFrame with all the unique keys</p> <pre><code>keys_df = df.select(F.explode(F.map_keys(F.col(\"some_data\")))).distinct()\nkeys_df.show()\n</code></pre> <pre><code>+---+\n|col|\n+---+\n|  z|\n|  b|\n|  a|\n+---+\n</code></pre> <p>Step 2: Convert the DataFrame to a list with all the unique keys</p> <pre><code>keys = list(map(lambda row: row[0], keys_df.collect()))\nprint(keys) # =&gt; ['z', 'b', 'a']\n</code></pre> <p>The <code>collect()</code> method gathers all the data on the driver node, which can be slow. We call <code>distinct()</code> to limit the data that's being collected on the driver node. Spark is a big data engine that's optimized for running computations in parallel on multiple nodes in a cluster. Collecting data on a single node and leaving the worker nodes idle should be avoided whenever possible. We're only using <code>collect()</code> here cause it's the only option.</p> <p>Step 3: Create an array of column objects for the map items</p> <pre><code>key_cols = list(map(lambda f: F.col(\"some_data\").getItem(f).alias(str(f)), keys))\nprint(key_cols)\n# =&gt; [Column&lt;b'some_data[z] AS `z`'&gt;, Column&lt;b'some_data[b] AS `b`'&gt;, Column&lt;b'some_data[a] AS `a`'&gt;]\n</code></pre> <p>Step 4: Add any additional columns before calculating the final result</p> <pre><code>final_cols = [F.col(\"first_name\")] + key_cols\nprint(final_cols)\n# =&gt; [Column&lt;b'first_name'&gt;, Column&lt;b'some_data[z] AS `z`'&gt;, Column&lt;b'some_data[b] AS `b`'&gt;, Column&lt;b'some_data[a] AS `a`'&gt;]\n</code></pre> <p>Step 5: Run a <code>select()</code> to get the final result</p> <pre><code>df.select(final_cols).show()\n</code></pre> <pre><code>+----------+----+-----------+----+\n|first_name|   z|          b|   a|\n+----------+----+-----------+----+\n|      jose|null|        bbb| aaa|\n|        li| zed|some_letter|null|\n+----------+----+-----------+----+\n</code></pre> <p>Step 2 is the potential bottleneck. If there aren't too many unique keys it shouldn't be too slow.</p> <p>Steps 3 and 4 should run very quickly. Running a single select operation in Step 5 is also quick.</p>"},{"location":"pyspark/dict-map-to-multiple-columns/#examining-logical-plans","title":"Examining logical plans","text":"<p>Use the <code>explain()</code> function to print the logical plans and see if the parsed logical plan needs a lot of optimizations:</p> <pre><code>df.select(final_cols).explain(True)\n</code></pre> <pre><code>== Parsed Logical Plan ==\n'Project [unresolvedalias('first_name, None), 'some_data[z] AS z#28, 'some_data[b] AS b#29, 'some_data[a] AS a#30]\n+- LogicalRDD [first_name#0, some_data#1], false\n\n== Analyzed Logical Plan ==\nfirst_name: string, z: string, b: string, a: string\nProject [first_name#0, some_data#1[z] AS z#28, some_data#1[b] AS b#29, some_data#1[a] AS a#30]\n+- LogicalRDD [first_name#0, some_data#1], false\n\n== Optimized Logical Plan ==\nProject [first_name#0, some_data#1[z] AS z#28, some_data#1[b] AS b#29, some_data#1[a] AS a#30]\n+- LogicalRDD [first_name#0, some_data#1], false\n\n== Physical Plan ==\n*(1) Project [first_name#0, some_data#1[z] AS z#28, some_data#1[b] AS b#29, some_data#1[a] AS a#30]\n</code></pre> <p>As you can see the parsed logical plan is quite similar to the optimized logical plan. Catalyst does not need to perform a lot of optimizations, so our code is efficient.</p>"},{"location":"pyspark/dict-map-to-multiple-columns/#next-steps","title":"Next steps","text":"<p>Breaking out a MapType column into multiple columns is fast if you know all the distinct map key values, but potentially slow if you need to figure them all out dynamically.</p> <p>You would want to avoid calculating the unique map keys whenever possible. Consider storing the distinct values in a data store and updating it incrementally if you have production workflows that depend on the distinct keys.</p> <p>If breaking out your map into separate columns is slow, consider segmenting your job into two steps:</p> <ul> <li>Step 1: Break the map column into separate columns and write it out to disk</li> <li>Step 2: Read the new dataset with separate columns and perform the rest of your analysis</li> </ul> <p>Complex column types are important for a lot of Spark analyses. In general favor StructType columns over MapType columns because they're easier to work with.</p>"},{"location":"pyspark/exists-forall-any-all-array/","title":"exists and forall PySpark array functions","text":"<p>This blog post demonstrates how to find if any element in a PySpark array meets a condition with <code>exists</code> or if all elements in an array meet a condition with <code>forall</code>.</p> <p><code>exists</code> is similar to the Python <code>any</code> function. <code>forall</code> is similar to the Python <code>all</code> function.</p>"},{"location":"pyspark/exists-forall-any-all-array/#exists","title":"exists","text":"<p>This section demonstrates how <code>any</code> is used to determine if one or more elements in an array meets a certain predicate condition and then shows how the PySpark <code>exists</code> method behaves in a similar manner.</p> <p>Create a regular Python array and use <code>any</code> to see if it contains the letter <code>b</code>.</p> <pre><code>arr = [\"a\", \"b\", \"c\"]\nany(e == \"b\" for e in arr) # True\n</code></pre> <p>We can also wrap <code>any</code> in a function that's takes array and anonymous function arguments. This is similar to what we'll see in PySpark.</p> <pre><code>def any_lambda(iterable, function):\n  return any(function(i) for i in iterable)\n\nequals_b = lambda e: e == \"b\"\n\nany_lambda(arr, equals_b) # True\n</code></pre> <p>We've seen how <code>any</code> works with vanilla Python. Let's see how <code>exists</code> works similarly with a PySpark array column.</p> <p>Create a DataFrame with an array column.</p> <pre><code>df = spark.createDataFrame(\n    [([\"a\", \"b\", \"c\"],), ([\"x\", \"y\", \"z\"],)], [\"some_arr\"]\n)\ndf.show()\n</code></pre> <pre><code>+---------+\n| some_arr|\n+---------+\n|[a, b, c]|\n|[x, y, z]|\n+---------+\n</code></pre> <p>Append a column that returns <code>True</code> if the array contains the letter <code>b</code> and <code>False</code> otherwise.</p> <pre><code>equals_b = lambda e: e == \"b\"\nres = df.withColumn(\"has_b\", exists(col(\"some_arr\"), equals_b))\nres.show()\n</code></pre> <pre><code>+---------+-----+\n| some_arr|has_b|\n+---------+-----+\n|[a, b, c]| true|\n|[x, y, z]|false|\n+---------+-----+\n</code></pre> <p>The <code>exists</code> function takes an array column as the first argument and an anonymous function as the second argument.</p>"},{"location":"pyspark/exists-forall-any-all-array/#forall","title":"forall","text":"<p><code>all</code> is used to determine if every element in an array meets a certain predicate condition.</p> <p>Create an array of numbers and use <code>all</code> to see if every number is even.</p> <pre><code>nums = [1, 2, 3]\nall(e % 2 == 0 for e in nums) # False\n</code></pre> <p>You can also wrap <code>all</code> in a function that's easily invoked with an array and an anonymous function.</p> <pre><code>def all_lambda(iterable, function):\n  return all(function(i) for i in iterable)\n\nis_even = lambda e: e % 2 == 0\n\nevens = [2, 4, 8]\nall_lambda(evens, is_even) # True\n</code></pre> <p><code>forall</code> in PySpark behaves like <code>all</code> in vanilla Python.</p> <p>Create a DataFrame with an array column.</p> <pre><code>df = spark.createDataFrame(\n    [([1, 2, 3],), ([2, 6, 12],)], [\"some_arr\"]\n)\ndf.show()\n</code></pre> <pre><code>+----------+\n|  some_arr|\n+----------+\n| [1, 2, 3]|\n|[2, 6, 12]|\n+----------+\n</code></pre> <p>Append a column that returns <code>True</code> if the array only contains even numbers and <code>False</code> otherwise.</p> <pre><code>is_even = lambda e: e % 2 == 0\nres = df.withColumn(\"all_even\", forall(col(\"some_arr\"), is_even))\nres.show()\n</code></pre> <pre><code>+----------+--------+\n|  some_arr|all_even|\n+----------+--------+\n| [1, 2, 3]|   false|\n|[2, 6, 12]|    true|\n+----------+--------+\n</code></pre>"},{"location":"pyspark/exists-forall-any-all-array/#conclusion","title":"Conclusion","text":"<p><code>exists</code> and <code>forall</code> are flexible because they're invoked with a function argument. These functions are easily adaptable for lots of use cases.</p> <p>You'll often work with array columns and these functions will easily allow you to code complex logic.</p>"},{"location":"pyspark/filter-array/","title":"Filtering PySpark Arrays and DataFrame Array Columns","text":"<p>This post explains how to filter values from a PySpark array column.</p> <p>It also explains how to filter DataFrames with array columns (i.e. reduce the number of rows in a DataFrame).</p> <p>Filtering values from an ArrayType column and filtering DataFrame rows are completely different operations of course. The <code>pyspark.sql.DataFrame#filter</code> method and the <code>pyspark.sql.functions#filter</code> function share the same name, but have different functionality. One removes elements from an array and the other removes rows from a DataFrame.</p> <p>It's important to understand both. The rest of this post provides clear examples.</p>"},{"location":"pyspark/filter-array/#filter-array-column","title":"filter array column","text":"<p>Suppose you have the following DataFrame with a <code>some_arr</code> column that contains numbers.</p> <pre><code>df = spark.createDataFrame(\n    [([1, 2, 3, 5, 7],), ([2, 4, 9],)], [\"some_arr\"]\n)\ndf.show()\n</code></pre> <pre><code>+---------------+\n|       some_arr|\n+---------------+\n|[1, 2, 3, 5, 7]|\n|      [2, 4, 9]|\n+---------------+\n</code></pre> <p>Use <code>filter</code> to append an <code>arr_evens</code> column that only contains the even numbers from <code>some_arr</code>:</p> <pre><code>from pyspark.sql.functions import *\n\nis_even = lambda x: x % 2 == 0\nres = df.withColumn(\"arr_evens\", filter(col(\"some_arr\"), is_even))\nres.show()\n</code></pre> <pre><code>+---------------+---------+\n|       some_arr|arr_evens|\n+---------------+---------+\n|[1, 2, 3, 5, 7]|      [2]|\n|      [2, 4, 9]|   [2, 4]|\n+---------------+---------+\n</code></pre> <p>The vanilla <code>filter</code> method in Python works similarly:</p> <pre><code>list(filter(is_even, [2, 4, 9])) # [2, 4]\n</code></pre> <p>The Spark filter function takes <code>is_even</code> as the second argument and the Python filter function takes <code>is_even</code> as the first argument. It's never easy ;)</p> <p>Now let's turn our attention to filtering entire rows.</p>"},{"location":"pyspark/filter-array/#filter-rows-if-array-column-contains-a-value","title":"filter rows if array column contains a value","text":"<p>Suppose you have the following DataFrame.</p> <pre><code>df = spark.createDataFrame(\n    [([\"one\", \"two\", \"three\"],), ([\"four\", \"five\"],), ([\"one\", \"nine\"],)], [\"some_arr\"]\n)\ndf.show()\n</code></pre> <pre><code>+-----------------+\n|         some_arr|\n+-----------------+\n|[one, two, three]|\n|     [four, five]|\n|      [one, nine]|\n+-----------------+\n</code></pre> <p>Here's how to filter out all the rows that don't contain the string <code>one</code>:</p> <pre><code>res = df.filter(array_contains(col(\"some_arr\"), \"one\"))\nres.show()\n</code></pre> <pre><code>+-----------------+\n|         some_arr|\n+-----------------+\n|[one, two, three]|\n|      [one, nine]|\n+-----------------+\n</code></pre> <p><code>array_contains</code> makes for clean code.</p> <p><code>where()</code> is an alias for filter so <code>df.where(array_contains(col(\"some_arr\"), \"one\"))</code> will return the same result.</p>"},{"location":"pyspark/filter-array/#filter-on-if-at-least-one-element-in-an-array-meets-a-condition","title":"filter on if at least one element in an array meets a condition","text":"<p>Create a DataFrame with some words:</p> <pre><code>df = spark.createDataFrame(\n    [([\"apple\", \"pear\"],), ([\"plan\", \"pipe\"],), ([\"cat\", \"ant\"],)], [\"some_words\"]\n)\ndf.show()\n</code></pre> <pre><code>+-------------+\n|   some_words|\n+-------------+\n|[apple, pear]|\n| [plan, pipe]|\n|   [cat, ant]|\n+-------------+\n</code></pre> <p>Filter out all the rows that don't contain a word that starts with the letter <code>a</code>.</p> <pre><code>starts_with_a = lambda s: s.startswith(\"a\")\nres = df.filter(exists(col(\"some_words\"), starts_with_a))\nres.show()\n</code></pre> <pre><code>+-------------+\n|   some_words|\n+-------------+\n|[apple, pear]|\n|   [cat, ant]|\n+-------------+\n</code></pre> <p><code>exists</code> lets you model powerful filtering logic.</p> <p>See the PySpark exists and forall post for a detailed discussion of <code>exists</code> and the other method we'll talk about next, <code>forall</code>.</p>"},{"location":"pyspark/filter-array/#filter-if-all-elements-in-an-array-meet-a-condition","title":"filter if all elements in an array meet a condition","text":"<p>Create a DataFrame with some integers:</p> <pre><code>df = spark.createDataFrame(\n    [([1, 2, 3, 5, 7],), ([2, 4, 9],), ([2, 4, 6],)], [\"some_ints\"]\n)\ndf.show()\n</code></pre> <pre><code>+---------------+\n|      some_ints|\n+---------------+\n|[1, 2, 3, 5, 7]|\n|      [2, 4, 9]|\n|      [2, 4, 6]|\n+---------------+\n</code></pre> <p>Filter out all the rows that contain any odd numbers.</p> <pre><code>is_even = lambda x: x % 2 == 0\nres = df.filter(forall(col(\"some_ints\"), is_even))\nres.show()\n</code></pre> <pre><code>+---------+\n|some_ints|\n+---------+\n|[2, 4, 6]|\n+---------+\n</code></pre> <p><code>forall</code> is useful when filtering.</p>"},{"location":"pyspark/filter-array/#conclusion","title":"Conclusion","text":"<p>PySpark has a <code>pyspark.sql.DataFrame#filter</code> method and a separate <code>pyspark.sql.functions.filter</code> function. Both are important, but they're useful in completely different contexts.</p> <p>The <code>filter</code> function was added in Spark 3.1, whereas the <code>filter</code> method has been around since the early days of Spark (1.3). The filter method is especially powerful when used with multiple conditions or with <code>forall</code> / <code>exsists</code> (methods added in Spark 3.1).</p> <p>PySpark 3 has added a lot of developer friendly functions and makes big data processing with Python a delight.</p>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/","title":"Install PySpark, Delta Lake, and Jupyter Notebooks on Mac with conda","text":"<p>This blog post explains how to install PySpark, Delta Lake, and Jupyter Notebooks on a Mac. This setup will let you easily run Delta Lake computations on your local machine in a Jupyter notebook for experimentation or to unit test your business logic.</p> <p>In order to run PySpark, you need to install Java. This guide will teach you how to install Java with a package manager that lets you easily switch between Java versions.</p> <p>You also need to pay close attention when setting your PySpark and Delta Lake versions, as they must be compatible. This post will show you how to pin PySpark and Delta Lake versions when creating your environment, to ensure they are compatible.</p> <p>Creating a local PySpark / Delta Lake / Jupyter setup can be a bit tricky, but you'll find it easy by following the steps in this guide.</p>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/#install-java","title":"Install Java","text":"<p>You need to install Java to run Spark code. Feel free to skip this section if you've already installed Java.</p> <p>Installing Java can be difficult because there are different vendors and versions. SDKMAN, short for the Software Development Kit Manager, makes it easy to install, and switch between, different Java versions.</p> <p>See this blog post for a detailed description on how to work with SDKMAN.</p> <p>After installing SDKMAN, you can run <code>sdk list java</code> to list all the Java versions that are available for installation. Spark works well with the <code>zulu</code> Java vendor. You can run a command like <code>sdk install java 8.0.322-zulu</code> to install Java 8, a Java version that works well with different version of Spark. You may need to run a slightly different command as Java versions are updated frequently.</p> <p>Run <code>java -version</code> and you should see output like this if the installation was successful:</p> <pre><code>openjdk version \"1.8.0_322\"\nOpenJDK Runtime Environment (Zulu 8.60.0.21-CA-macosx) (build 1.8.0_322-b06)\nOpenJDK 64-Bit Server VM (Zulu 8.60.0.21-CA-macosx) (build 25.322-b06, mixed mode)\n</code></pre>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/#install-conda","title":"Install conda","text":"<p>The next step is to install Miniconda, so you can build a software environment with Delta Lake, Jupyter, and PySpark.</p> <p>After Miniconda is installed, you should be able to run the <code>conda info</code> command.</p> <p>Now you're ready to start creating a software environment with all the required dependencies.</p>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/#install-pyspark-delta-lake-via-conda","title":"Install PySpark &amp; Delta Lake via conda","text":"<p>We're going to create a conda software environment from a YAML file that'll allow us to specify the exact versions of PySpark and Delta Lake that are known to be compatible.</p> <p>You can see the compatible versions here. For example, you can use Delta Lake 1.2 with PySpark 3.2, but cannot use Delta Lake 0.3.0 with PySpark 3.2.</p> <p>Here's an example YAML file with the required dependencies.</p> <pre><code>name: mr-delta\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.9\n  - ipykernel\n  - nb_conda\n  - jupyterlab\n  - jupyterlab_code_formatter\n  - isort\n  - black\n  - pyspark=3.2.0\n  - pip\n  - pip:\n    - delta-spark==1.2.1\n</code></pre> <p>Notice how the Python, PySpark, and <code>delta-spark</code> dependencies are pinned to specific versions that are known to be compatible. You want to explicitly set dependencies that are compatible rather than relying on conda to properly resolve the dependency versions.</p> <p><code>delta-spark</code> is installed via pip because it's not uploaded to <code>conda-forge</code>.</p> <p>This conda environment file is also available in the delta-examples code repo. You can clone the repo, <code>cd</code> into the project directory, and run <code>conda create env -f envs/mr-delta.yml</code> to create the conda environment.</p> <p>When you run <code>conda env list</code>, you should see the \"mr-delta\" environment listed.</p> <p>Run <code>conda activate mr-delta</code> to activate the environment. Once the environment is activated, you're ready to open a Jupyter notebook.</p>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/#open-jupyter-notebooks","title":"Open Jupyter notebooks","text":"<p>Make sure you've changed into the delta-examples directory and have the mr-delta conda environment activated.</p> <p>Then run <code>jupyter lab</code> to open up this project in your browser via Jupyter.</p> <p>You should now be able to run all the commands in this notebook.</p> <p>Here's how the computations should look in your Jupyter notebook:</p> <p></p> <p>Take a look at the following code snippet and pay close attention on how you need to initialize the <code>SparkSession</code>:</p> <pre><code>import pyspark\nfrom delta import *\n\nbuilder = (\n    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n)\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n</code></pre> <p>You need to use the <code>configure_spark_with_delta_pip</code> function to properly initialize the <code>SparkSession</code> when working with Delta Lake.</p> <p>You'll normally be using Delta Lake with Spark, but sometimes it's convenient to work with Delta Lake outside of a Spark setting. Let's see how you can perform Delta Lake operations, even without Spark.</p>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/#using-delta-lake-without-spark","title":"Using Delta Lake without Spark?","text":"<p>delta-rs is a Rust implementation of Delta Lake that also exposes Python bindings. This library allows you to perform common operations on Delta Lakes, even when a Spark runtime environment is not installed.</p> <p>Delta has you covered, even if you don't have access to a Spark runtime for a certain chunk of code.</p>"},{"location":"pyspark/install-delta-lake-jupyter-conda-mac/#conclusion","title":"Conclusion","text":"<p>This post taught you how to create a conda environment with Jupyter, PySpark, and Delta Lake and how to run basic computations.</p> <p>Running computations locally is a great way to learn Delta. You can execute commands and observe how the Parquet files and transaction log are changed.</p> <p>The localhost setup described in this post is also great if you'd like to run Delta Lake unit tests before deploying code to production. Testing your PySpark code is always a good idea.</p>"},{"location":"pyspark/none-null/","title":"Navigating None and null in PySpark","text":"<p>This blog post shows you how to gracefully handle <code>null</code> in PySpark and how to avoid <code>null</code> input errors.</p> <p>Mismanaging the <code>null</code> case is a common source of errors and frustration in PySpark.</p> <p>Following the tactics outlined in this post will save you from a lot of pain and production bugs.</p>"},{"location":"pyspark/none-null/#create-dataframes-with-null-values","title":"Create DataFrames with null values","text":"<p>Let's start by creating a DataFrame with <code>null</code> values:</p> <pre><code>df = spark.createDataFrame([(1, None), (2, \"li\")], [\"num\", \"name\"])\ndf.show()\n</code></pre> <pre><code>+---+----+\n|num|name|\n+---+----+\n|  1|null|\n|  2|  li|\n+---+----+\n</code></pre> <p>You use <code>None</code> to create DataFrames with <code>null</code> values.</p> <p><code>null</code> is not a value in Python, so this code will not work:</p> <pre><code>df = spark.createDataFrame([(1, null), (2, \"li\")], [\"num\", \"name\"])\n</code></pre> <p>It throws the following error:</p> <pre><code>NameError: name 'null' is not defined\n</code></pre>"},{"location":"pyspark/none-null/#read-csvs-with-null-values","title":"Read CSVs with null values","text":"<p>Suppose you have the following data stored in the <code>some_people.csv</code> file:</p> <pre><code>first_name,age\nluisa,23\n\"\",45\nbill,\n</code></pre> <p>Read this file into a DataFrame and then show the contents to demonstrate which values are read into the DataFrame as <code>null</code>.</p> <pre><code>path = \"/Users/powers/data/some_people.csv\"\ndf = spark.read.option(\"header\", True).csv(path)\ndf.show()\n</code></pre> <pre><code>+----------+----+\n|first_name| age|\n+----------+----+\n|     luisa|  23|\n|      null|  45|\n|      bill|null|\n+----------+----+\n</code></pre> <p>The empty string in row 2 and the missing value in row 3 are both read into the PySpark DataFrame as <code>null</code> values.</p>"},{"location":"pyspark/none-null/#isnull","title":"isNull","text":"<p>Create a DataFrame with <code>num1</code> and <code>num2</code> columns.</p> <pre><code>df = spark.createDataFrame([(1, None), (2, 2), (None, None)], [\"num1\", \"num2\"])\ndf.show()\n</code></pre> <pre><code>+----+----+\n|num1|num2|\n+----+----+\n|   1|null|\n|   2|   2|\n|null|null|\n+----+----+\n</code></pre> <p>Append an <code>is_num2_null</code> column to the DataFrame:</p> <pre><code>df.withColumn(\"is_num2_null\", df.num2.isNull()).show()\n</code></pre> <pre><code>+----+----+------------+\n|num1|num2|is_num2_null|\n+----+----+------------+\n|   1|null|        true|\n|   2|   2|       false|\n|null|null|        true|\n+----+----+------------+\n</code></pre> <p>The <code>isNull</code> function returns <code>True</code> if the value is <code>null</code> and <code>False</code> otherwise.</p>"},{"location":"pyspark/none-null/#equality","title":"equality","text":"<p>Let's look at how the <code>==</code> equality operator handles comparisons with null values.</p> <pre><code>df.withColumn(\"num1==num2\", df.num1 == df.num2).show()\n</code></pre> <pre><code>+----+----+----------+\n|num1|num2|num1==num2|\n+----+----+----------+\n|   1|null|      null|\n|   2|   2|      true|\n|null|null|      null|\n+----+----+----------+\n</code></pre> <p>If either, or both, of the operands are <code>null</code>, then <code>==</code> returns <code>null</code>.</p> <p>Lots of times, you'll want this equality behavior:</p> <ul> <li>When one value is <code>null</code> and the other is not <code>null</code>, return <code>False</code></li> <li>When both values are <code>null</code>, return <code>True</code></li> </ul> <p>Here's one way to perform a null safe equality comparison:</p> <pre><code>df.withColumn(\n  \"num1_eq_num2\",\n  when(df.num1.isNull() &amp; df.num2.isNull(), True)\n    .when(df.num1.isNull() | df.num2.isNull(), False)\n    .otherwise(df.num1 == df.num2)\n).show()\n</code></pre> <pre><code>+----+----+------------+\n|num1|num2|num1_eq_num2|\n+----+----+------------+\n|   1|null|       false|\n|   2|   2|        true|\n|null|null|        true|\n+----+----+------------+\n</code></pre> <p>Let's look at a built-in function that lets you perform null safe equality comparisons with less typing.</p>"},{"location":"pyspark/none-null/#null-safe-equality","title":"null safe equality","text":"<p>We can perform the same null safe equality comparison with the built-in <code>eqNullSafe</code> function.</p> <pre><code>df.withColumn(\"num1_eqNullSafe_num2\", df.num1.eqNullSafe(df.num2)).show()\n</code></pre> <pre><code>+----+----+--------------------+\n|num1|num2|num1_eqNullSafe_num2|\n+----+----+--------------------+\n|   1|null|               false|\n|   2|   2|                true|\n|null|null|                true|\n+----+----+--------------------+\n</code></pre> <p><code>eqNullSafe</code> saves you from extra code complexity. This function is often used when joining DataFrames.</p>"},{"location":"pyspark/none-null/#user-defined-function-pitfalls","title":"User Defined Function pitfalls","text":"<p>This section shows a UDF that works on DataFrames without <code>null</code> values and fails for DataFrames with <code>null</code> values. It then shows how to refactor the UDF so it doesn't error out for <code>null</code> values.</p> <p>Start by creating a DataFrame that does not contain <code>null</code> values.</p> <pre><code>countries1 = spark.createDataFrame([(\"Brasil\", 1), (\"Mexico\", 2)], [\"country\", \"id\"])\ncountries1.show()\n</code></pre> <pre><code>+-------+---+\n|country| id|\n+-------+---+\n| Brasil|  1|\n| Mexico|  2|\n+-------+---+\n</code></pre> <p>Create a UDF that appends the string \"is fun!\".</p> <pre><code>from pyspark.sql.types import StringType\n\n@udf(returnType=StringType())\ndef bad_funify(s):\n     return s + \" is fun!\"\n</code></pre> <p>Run the UDF and observe that is works for DataFrames that don't contain any <code>null</code> values.</p> <pre><code>countries1.withColumn(\"fun_country\", bad_funify(\"country\")).show()\n</code></pre> <pre><code>+-------+---+--------------+\n|country| id|   fun_country|\n+-------+---+--------------+\n| Brasil|  1|Brasil is fun!|\n| Mexico|  2|Mexico is fun!|\n+-------+---+--------------+\n</code></pre> <p>Let's create another DataFrame and run the <code>bad_funify</code> function again.</p> <pre><code>countries2 = spark.createDataFrame([(\"Thailand\", 3), (None, 4)], [\"country\", \"id\"])\ncountries2.withColumn(\"fun_country\", bad_funify(\"country\")).show()\n</code></pre> <p>This code will error out cause the <code>bad_funify</code> function can't handle <code>null</code> values. Here's the stack trace:</p> <pre><code>org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in &lt;genexpr&gt;\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"&lt;stdin&gt;\", line 3, in bad_funify\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n    at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n    at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n21/06/19 19:58:35 WARN TaskSetManager: Lost task 2.0 in stage 45.0 (TID 110) (192.168.1.74 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in &lt;genexpr&gt;\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"&lt;stdin&gt;\", line 3, in bad_funify\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n    at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n    at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n    at org.apache.spark.scheduler.Task.run(Task.scala:131)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n\n21/06/19 19:58:35 ERROR TaskSetManager: Task 2 in stage 45.0 failed 1 times; aborting job\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\", line 484, in show\n    print(self._jdf.showString(n, 20, vertical))\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n    raise converted from None\npyspark.sql.utils.PythonException:\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in &lt;genexpr&gt;\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"&lt;stdin&gt;\", line 3, in bad_funify\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n</code></pre> <p>Let's write a <code>good_funify</code> function that won't error out.</p> <pre><code>@udf(returnType=StringType())\ndef good_funify(s):\n     return None if s == None else s + \" is fun!\"\n\ncountries2.withColumn(\"fun_country\", good_funify(\"country\")).show()\n</code></pre> <pre><code>+--------+---+----------------+\n| country| id|     fun_country|\n+--------+---+----------------+\n|Thailand|  3|Thailand is fun!|\n|    null|  4|            null|\n+--------+---+----------------+\n</code></pre> <p>Always make sure to handle the null case whenever you write a UDF. It's really annoying to write a function, build a wheel file, and attach it to a cluster, only to have it error out when run on a production dataset that contains <code>null</code> values.</p>"},{"location":"pyspark/none-null/#built-in-pyspark-functions-gracefully-handle-null","title":"Built-in PySpark functions gracefully handle null","text":"<p>All of the built-in PySpark functions gracefully handle the <code>null</code> input case by simply returning <code>null</code>. They don't error out. <code>null</code> values are common and writing PySpark code would be really tedious if erroring out was the default behavior.</p> <p>Let's write a <code>best_funify</code> function that uses the built-in PySpark functions, so we don't need to explicitly handle the <code>null</code> case ourselves.</p> <pre><code>def best_funify(col):\n     return concat(col, lit(\" is fun!\"))\n\ncountries2.withColumn(\"fun_country\", best_funify(countries2.country)).show()\n</code></pre> <pre><code>+--------+---+----------------+\n| country| id|     fun_country|\n+--------+---+----------------+\n|Thailand|  3|Thailand is fun!|\n|    null|  4|            null|\n+--------+---+----------------+\n</code></pre> <p>It's always best to use built-in PySpark functions whenever possible. They handle the <code>null</code> case and save you the hassle.</p> <p>There are other benefits of built-in PySpark functions, see the article on User Defined Functions for more information.</p>"},{"location":"pyspark/none-null/#nullability","title":"nullability","text":"<p>Each column in a DataFrame has a <code>nullable</code> property that can be set to <code>True</code> or <code>False</code>.</p> <p>If <code>nullable</code> is set to <code>False</code> then the column cannot contain <code>null</code> values.</p> <p>Here's how to create a DataFrame with one column that's nullable and another column that is not.</p> <pre><code>from pyspark.sql import Row\nfrom pyspark.sql.types import *\n\nrdd = spark.sparkContext.parallelize([\n    Row(name='Allie', age=2),\n    Row(name='Sara', age=33),\n    Row(name='Grace', age=31)])\n\nschema = schema = StructType([\n   StructField(\"name\", StringType(), True),\n   StructField(\"age\", IntegerType(), False)])\n\ndf = spark.createDataFrame(rdd, schema)\n\ndf.show()\n</code></pre> <pre><code>+-----+---+\n| name|age|\n+-----+---+\n|Allie|  2|\n| Sara| 33|\n|Grace| 31|\n+-----+---+\n</code></pre> <p>Use the <code>printSchema</code> function to check the <code>nullable</code> flag:</p> <pre><code>df.printSchema()\n</code></pre> <pre><code>root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n</code></pre> <p>In theory, you can write code that doesn't explicitly handle the <code>null</code> case when working with the <code>age</code> column because the nullable flag means it doesn't contain null values. In practice, the <code>nullable</code> flag is a weak guarantee and you should always write code that handles the null case (or rely on built-in PySpark functions to gracefully handle the null case for you).</p> <p>See the blog post on DataFrame schemas for more information about controlling the <code>nullable</code> property, including unexpected behavior in some cases.</p>"},{"location":"pyspark/none-null/#testing-the-null-case","title":"Testing the null case","text":"<p>You should always make sure your code works properly with null input in the test suite.</p> <p>Let's look at a helper function from the quinn library that converts all the whitespace in a string to single spaces.</p> <pre><code>def single_space(col):\n    return F.trim(F.regexp_replace(col, \" +\", \" \"))\n</code></pre> <p>Let's look at the test for this function.</p> <pre><code>def test_single_space(spark):\n    df = spark.create_df(\n        [\n            (\"  I like     fish  \", \"I like fish\"),\n            (\"    zombies\", \"zombies\"),\n            (\"simpsons   cat lady\", \"simpsons cat lady\"),\n            (None, None),\n        ],\n        [\n            (\"words\", StringType(), True),\n            (\"expected\", StringType(), True),\n        ],\n    )\n    actual_df = df.withColumn(\"words_single_spaced\", quinn.single_space(F.col(\"words\")))\n    chispa.assert_column_equality(actual_df, \"words_single_spaced\", \"expected\")\n</code></pre> <p>The <code>(None, None)</code> row verifies that the <code>single_space</code> function returns <code>null</code> when the input is <code>null</code>.</p> <p>The desired function output for <code>null</code> input (returning <code>null</code> or erroring out) should be documented in the test suite.</p>"},{"location":"pyspark/none-null/#next-steps","title":"Next steps","text":"<p>You've learned how to effectively manage <code>null</code> and prevent it from becoming a pain in your codebase.</p> <p><code>null</code> values are a common source of errors in PySpark applications, especially when you're writing User Defined Functions.</p> <p>Get in the habit of verifying that your code gracefully handles <code>null</code> input in your test suite to avoid production bugs.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/","title":"PySpark Dependency Management and Wheel Packaging with Poetry","text":"<p>This blog post explains how to create a PySpark project with Poetry, the best Python dependency management system. It'll also explain how to package PySpark projects as wheel files, so you can build libraries and easily access the code on Spark clusters.</p> <p>Poetry is beloved by the co-creator of Django and other bloggers. It's even part of the hypermodern Python stack.</p> <p>This post will demonstrate how Poetry is great for PySpark projects.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#creating-a-project","title":"Creating a project","text":"<p>Let's create a project called angelou after the poet Maya Angelou. Check out this repo for all the code in this blog.</p> <p>Create the project with <code>poetry new angelou</code>.</p> <p>This will create the <code>angelou</code> project with these contents:</p> <pre><code>angelou\n  angelou\n    __init__.py\n  pyproject.toml\n  README.rst\n  tests\n    __init__.py\n    test_angelou.py\n</code></pre> <p>Here are the contents of the <code>pyproject.toml</code> file:</p> <pre><code>[tool.poetry]\nname = \"angelou\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"MrPowers &lt;matthewkevinpowers@gmail.com&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^5.2\"\n\n[build-system]\nrequires = [\"poetry&gt;=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n</code></pre> <p>The <code>pyproject.toml</code> file specifies the Python version and the project dependencies.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#add-pyspark-to-project","title":"Add PySpark to project","text":"<p>Add PySpark to the project with the <code>poetry add pyspark</code> command.</p> <p>Here's the console output when the command is run:</p> <pre><code>Creating virtualenv angelou--6rG3Bgg-py3.7 in /Users/matthewpowers/Library/Caches/pypoetry/virtualenvs\nUsing version ^2.4.5 for pyspark\n\nUpdating dependencies\nResolving dependencies... (2.1s)\n\nWriting lock file\n\n\nPackage operations: 13 installs, 0 updates, 0 removals\n\n  - Installing zipp (3.1.0)\n  - Installing importlib-metadata (1.6.0)\n  - Installing pyparsing (2.4.7)\n  - Installing six (1.15.0)\n  - Installing attrs (19.3.0)\n  - Installing more-itertools (8.3.0)\n  - Installing packaging (20.4)\n  - Installing pluggy (0.13.1)\n  - Installing py (1.8.1)\n  - Installing py4j (0.10.7)\n  - Installing wcwidth (0.1.9)\n  - Installing pyspark (2.4.5)\n  - Installing pytest (5.4.2)\n</code></pre> <p>Poetry automatically created a virtual environment all the project dependencies are stored in the <code>/Users/matthewpowers/Library/Caches/pypoetry/virtualenvs/angelou--6rG3Bgg-py3.7/</code> directory.</p> <p>You might wonder why 13 dependencies were added to the project - didn't we just install PySpark? Why wasn't one dependency added?</p> <p>PySpark depends on other libraries like <code>py4j</code>, as you can see with this search. Poetry needs to add everything PySpark depends on to the project as well.</p> <p>pytest requires <code>py</code>, <code>importlib-metadata</code>, and <code>pluggy</code>, so those dependencies need to be added to our project as well.</p> <p>Poetry makes sure your virtual environment contains all your explicit project dependencies and all dependencies of your explicit dependencies.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#poetry-lock-file","title":"Poetry lock file","text":"<p>The <code>poetry add pyspark</code> command also creates a <code>poetry.lock</code> file as hinted by the \"Writing lock file\" console output when <code>poetry add pyspark</code> is run.</p> <p>Here's what the Poetry website says about the Lock file:</p> <p>For your library, you may commit the <code>poetry.lock</code> file if you want to. This can help your team to always test against the same dependency versions. However, this lock file will not have any effect on other projects that depend on it. It only has an effect on the main project. If you do not want to commit the lock file and you are using git, add it to the <code>.gitignore</code>.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#pyspark-dataframe-transformation","title":"PySpark DataFrame transformation","text":"<p>Let's create a PySpark DataFrame transformation that'll append a <code>greeting</code> column to a DataFrame.</p> <p>Create a <code>transformations.py</code> file and add this code:</p> <pre><code>import pyspark.sql.functions as F\n\ndef with_greeting(df):\n    return df.withColumn(\"greeting\", F.lit(\"hello!\"))\n</code></pre> <p>Let's verify that the <code>with_greeting</code> function appends a <code>greeting</code> column as expected.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#testing-the-dataframe-transformation","title":"Testing the DataFrame transformation","text":"<p>Here's how we'll test the <code>with_greeting</code> function:</p> <ul> <li>Create a DataFrame and run the <code>with_greeting</code> function (<code>actual_df</code>)</li> <li>Create another DataFrame with the anticipated results (<code>expected_df</code>)</li> <li>Compare the DataFrames and make sure the actual result is the same as what's expected</li> </ul> <p>We need to create a SparkSession to create the DataFrames that'll be used in the test.</p> <p>Create a <code>sparksession.py</code> file with these contents:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = (SparkSession.builder\n            .master(\"local\")\n            .appName(\"angelou\")\n            .getOrCreate())\n</code></pre> <p>Create a <code>test_transformations.py</code> file in the <code>tests/</code> directory and add this code:</p> <pre><code>import pytest\n\nimport angelou.sparksession as S\nimport angelou.transformations as T\n\nclass TestTransformations(object):\n\n    def test_with_greeting(self):\n        source_data = [\n            (\"jose\", 1),\n            (\"li\", 2)\n        ]\n        source_df = S.spark.createDataFrame(\n            source_data,\n            [\"name\", \"age\"]\n        )\n\n        actual_df = T.with_greeting(source_df)\n\n        expected_data = [\n            (\"jose\", 1, \"hello!\"),\n            (\"li\", 2, \"hello!\")\n        ]\n        expected_df = S.spark.createDataFrame(\n            expected_data,\n            [\"name\", \"age\", \"greeting\"]\n        )\n\n        assert(expected_df.collect() == actual_df.collect())\n</code></pre>"},{"location":"pyspark/poetry-dependency-management-wheel/#console-workflow","title":"Console workflow","text":"<p>Run <code>poetry shell</code> to spawn a shell within the virtual environment. Then run <code>python</code> to start a REPL.</p> <p>We can copy and paste code from the test file to run the <code>with_greeting</code> function in the shell.</p> <pre><code>&gt;&gt;&gt; import angelou.sparksession as S\n&gt;&gt;&gt; import angelou.transformations as T\n&gt;&gt;&gt; source_data = [\n...     (\"jose\", 1),\n...     (\"li\", 2)\n... ]\n&gt;&gt;&gt; source_df = S.spark.createDataFrame(\n...     source_data,\n...     [\"name\", \"age\"]\n... )\n\n&gt;&gt;&gt; source_df.show()\n+----+---+\n|name|age|\n+----+---+\n|jose|  1|\n|  li|  2|\n+----+---+\n\n&gt;&gt;&gt; actual_df = T.with_greeting(source_df)\n&gt;&gt;&gt; actual_df.show()\n+----+---+--------+\n|name|age|greeting|\n+----+---+--------+\n|jose|  1|  hello!|\n|  li|  2|  hello!|\n+----+---+--------+\n</code></pre> <p>The console REPL workflow can be useful when you're experimenting with code.</p> <p>It's generally best to keep your console workflow to a minimum and devote your development efforts to building a great test suite.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#adding-quinn-dependency","title":"Adding quinn dependency","text":"<p>quinn contains useful PySpark helper functions.</p> <p>Add quinn to the project with <code>poetry add quinn</code>.</p> <p>The <code>pyproject.toml</code> file will be updated as follows when <code>quinn</code> is added:</p> <pre><code>[tool.poetry.dependencies]\npython = \"^3.7\"\npyspark = \"^2.4.5\"\nquinn = \"^0.4.0\"\n</code></pre> <p>quinn is also added to the lock file in two places:</p> <pre><code>[[package]]\ncategory = \"main\"\ndescription = \"Pyspark helper methods to maximize developer efficiency.\"\nname = \"quinn\"\noptional = false\npython-versions = \"&gt;=2.7\"\nversion = \"0.4.0\"\n\nquinn = [\n    {file = \"quinn-0.4.0-py3-none-any.whl\", hash = \"sha256:4f2f1dd0086a4195ee1ec4420351001ee5687e8183f80bcbc93d4e724510d114\"}\n]\n</code></pre>"},{"location":"pyspark/poetry-dependency-management-wheel/#using-quinn","title":"Using quinn","text":"<p>Create a <code>with_clean_first_name</code> function that'll remove all the non-word characters in the <code>first_name</code> column of a DataFrame. The quinn <code>remove_non_word_characters</code> function will help.</p> <pre><code>import quinn\n\ndef with_clean_first_name(df):\n    return df.withColumn(\n        \"clean_first_name\",\n        quinn.remove_non_word_characters(F.col(\"first_name\"))\n    )\n</code></pre> <p>Let's create a test to verify that <code>with_clean_first_name</code> removes all the non-word characters in the <code>first_name</code> field of this DataFrame:</p> <pre><code>+----------+------+\n|first_name|letter|\n+----------+------+\n|    jo&amp;&amp;se|     a|\n|      ##li|     b|\n|   !!sam**|     c|\n+----------+------+\n</code></pre> <p>This test will use the <code>create_df</code> SparkSession extension defined in quinn that makes it easier to create DataFrames.</p> <pre><code>from pyspark.sql.types import *\nfrom quinn.extensions import *\n\nimport angelou.sparksession as S\nimport angelou.transformations as T\n\nclass TestTransformations(object):\n\n    def test_with_clean_first_name(self):\n        source_df = S.spark.create_df(\n            [(\"jo&amp;&amp;se\", \"a\"), (\"##li\", \"b\"), (\"!!sam**\", \"c\")],\n            [(\"first_name\", StringType(), True), (\"letter\", StringType(), True)]\n        )\n\n        actual_df = T.with_clean_first_name(source_df)\n\n        expected_df = S.spark.create_df(\n            [(\"jo&amp;&amp;se\", \"a\", \"jose\"), (\"##li\", \"b\", \"li\"), (\"!!sam**\", \"c\", \"sam\")],\n            [(\"first_name\", StringType(), True), (\"letter\", StringType(), True), (\"clean_first_name\", StringType(), True)]\n        )\n\n        assert(expected_df.collect() == actual_df.collect())\n</code></pre>"},{"location":"pyspark/poetry-dependency-management-wheel/#packaging-wheel-file","title":"Packaging wheel file","text":"<p>You can build a wheel file with the <code>poetry build</code> command.</p> <p>This will output the following files:</p> <pre><code>dist/\n  angelou-0.1.0-py3-none-any.whl\n  angelou-0.1.0.tar.gz\n</code></pre> <p>You can run <code>poetry publish</code> to publish the package to PyPi.</p>"},{"location":"pyspark/poetry-dependency-management-wheel/#conclusion","title":"Conclusion","text":"<p>Dependency management has been a pain point for Python developers for years and the debate on how to solve the issue goes on.</p> <p>Thankfully, Python tooling has come a long way and Poetry makes it easy to manage project dependencies.</p> <p>Poetry is great option for PySpark projects. It makes it easy to build public libraries that are uploaded to PyPi or to build private wheel files so you can run your private projects on Spark clusters.</p>"},{"location":"pyspark/random-values-arrays-columns/","title":"Fetching Random Values from PySpark Arrays / Columns","text":"<p>This post shows you how to fetch a random value from a PySpark array or from a set of columns. It'll also show you how to add a column to a DataFrame with a random value from a Python array and how to fetch n random values from a given column.</p>"},{"location":"pyspark/random-values-arrays-columns/#random-value-from-pyspark-array","title":"Random value from PySpark array","text":"<p>Suppose you have the following DataFrame:</p> <pre><code>+------------+\n|     letters|\n+------------+\n|   [a, b, c]|\n|[a, b, c, d]|\n|         [x]|\n|          []|\n+------------+\n</code></pre> <p>You can leverage the <code>array_choice()</code> function defined in quinn to append a <code>random_letter</code> column that fetches a random value from <code>letters</code>.</p> <pre><code>actual_df = df.withColumn(\n    \"random_letter\",\n    quinn.array_choice(F.col(\"letters\"))\n)\nactual_df.show()\n</code></pre> <pre><code>+------------+-------------+\n|     letters|random_letter|\n+------------+-------------+\n|   [a, b, c]|            c|\n|[a, b, c, d]|            c|\n|         [x]|            x|\n|          []|         null|\n+------------+-------------+\n</code></pre> <p>Here's how the <code>array_choice()</code> function is defined:</p> <pre><code>import pyspark.sql.functions as F\n\ndef array_choice(col):\n    index = (F.rand()*F.size(col)).cast(\"int\")\n    return col[index]\n</code></pre>"},{"location":"pyspark/random-values-arrays-columns/#random-value-from-columns","title":"Random value from columns","text":"<p>You can also use <code>array_choice</code> to fetch a random value from a list of columns. Suppose you have the following DataFrame:</p> <pre><code>+----+----+----+\n|num1|num2|num3|\n+----+----+----+\n|   1|   2|   3|\n|   4|   5|   6|\n|   7|   8|   9|\n|  10|null|null|\n|null|null|null|\n+----+----+----+\n</code></pre> <p>Here's the code to append a <code>random_number</code> column that selects a random value from <code>num1</code>, <code>num2</code>, or <code>num3</code>.</p> <pre><code>actual_df = df.withColumn(\n    \"random_number\",\n    quinn.array_choice(F.array(F.col(\"num1\"), F.col(\"num2\"), F.col(\"num3\")))\n)\nactual_df.show()\n</code></pre> <pre><code>+----+----+----+-------------+\n|num1|num2|num3|random_number|\n+----+----+----+-------------+\n|   1|   2|   3|            1|\n|   4|   5|   6|            4|\n|   7|   8|   9|            8|\n|  10|null|null|           10|\n|null|null|null|         null|\n+----+----+----+-------------+\n</code></pre> <p>The <code>array</code> function is used to convert the columns to an array, so the input is suitable for <code>array_choice</code>.</p>"},{"location":"pyspark/random-values-arrays-columns/#random-value-from-python-array","title":"Random value from Python array","text":"<p>Suppose you'd like to add a <code>random_animal</code> column to an existing DataFrame that randomly selects between cat, dog, and mouse.</p> <pre><code>df = spark.createDataFrame([('jose',), ('maria',), (None,)], ['first_name'])\ncols = list(map(lambda col_name: F.lit(col_name), ['cat', 'dog', 'mouse']))\nactual_df = df.withColumn(\n    'random_animal',\n    quinn.array_choice(F.array(*cols))\n)\nactual_df.show()\n</code></pre> <pre><code>+----------+-------------+\n|first_name|random_animal|\n+----------+-------------+\n|      jose|          cat|\n|     maria|        mouse|\n|      null|          dog|\n+----------+-------------+\n</code></pre> <p>This tactic is useful when you're creating fake datasets.</p> <p>Study this code closely and make sure you're comfortable with making a list of PySpark column objects (this line of code: <code>cols = list(map(lambda col_name: F.lit(col_name), ['cat', 'dog', 'mouse']))</code>). Manipulating lists of PySpark columns is useful when renaming multiple columns, when removing dots from column names and when changing column types. It's an important design pattern for PySpark programmers to master.</p>"},{"location":"pyspark/random-values-arrays-columns/#n-random-values-from-a-column","title":"N random values from a column","text":"<p>Suppose you'd like to get some random values from a PySpark column, as discussed here. Here's a sample DataFrame:</p> <pre><code>+---+\n| id|\n+---+\n|123|\n|245|\n| 12|\n|234|\n+---+\n</code></pre> <p>Here's how to fetch three random values from the <code>id</code> column:</p> <pre><code>df.rdd.takeSample(False, 3)\n</code></pre> <p>Here's how you get the result as an array of integers:</p> <pre><code>list(map(lambda row: row[0], df.rdd.takeSample(False, 3))) # =&gt; [123, 12, 245]\n</code></pre> <p>This code also works, but requires a full table sort which is expensive:</p> <pre><code>df.select('id').orderBy(F.rand()).limit(3)\n</code></pre> <p>Examine the physical plan to verify that a full table sort is performed:</p> <pre><code>df.select('id').orderBy(F.rand()).limit(3).explain()\n</code></pre> <p>Here's the physical plan that's outputted:</p> <pre><code>TakeOrderedAndProject(limit=3, orderBy=[_nondeterministic#38 ASC NULLS FIRST], output=[id#32L])\n+- *(1) Project [id#32L, rand(-4436287143488772163) AS _nondeterministic#38]\n</code></pre> <p>If your table is huge, then a full table sort will be slow.</p>"},{"location":"pyspark/random-values-arrays-columns/#next-steps","title":"Next steps","text":"<p>Feel free to copy / paste <code>array_choice</code> in your notebooks or depend on quinn to access this functionality.</p> <p>Notebooks that don't rely on open source / private code abstractions tend to be overly complex. Think about moving generic code like <code>array_choice</code> to codebases outside your notebook. Solving problems with PySpark is hard enough. Don't make it harder by bogging down your notebooks with additional complexity.</p> <p>Read the blog posts on creating a PySpark project with Poetry and testing PySpark code to learn more about PySpark best practices.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/","title":"Renaming Multiple PySpark DataFrame columns (withColumnRenamed, select, toDF)","text":"<p>This blog post explains how to rename one or all of the columns in a PySpark DataFrame.</p> <p>You'll often want to rename columns in a DataFrame. Here are some examples:</p> <ul> <li>remove all spaces from the DataFrame columns</li> <li>convert all the columns to snake_case</li> <li>replace the dots in column names with underscores</li> </ul> <p>Lots of approaches to this problem are not scalable if you want to rename a lot of columns. Other solutions call <code>withColumnRenamed</code> a lot which may cause performance issues or cause StackOverflowErrors.</p> <p>This blog post outlines solutions that are easy to use and create simple analysis plans, so the Catalyst optimizer doesn't need to do hard optimization work.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#renaming-a-single-column-using-withcolumnrenamed","title":"Renaming a single column using withColumnRenamed","text":"<p>Renaming a single column is easy with <code>withColumnRenamed</code>. Suppose you have the following DataFrame:</p> <pre><code>+----------+------------+\n|first_name|likes_soccer|\n+----------+------------+\n|      jose|        true|\n+----------+------------+\n</code></pre> <p>You can rename the <code>likes_soccer</code> column to <code>likes_football</code> with this code:</p> <pre><code>df.withColumnRenamed(\"likes_soccer\", \"likes_football\").show()\n</code></pre> <pre><code>+----------+--------------+\n|first_name|likes_football|\n+----------+--------------+\n|      jose|          true|\n+----------+--------------+\n</code></pre> <p><code>withColumnRenamed</code> can also be used to rename all the columns in a DataFrame, but that's not a performant approach. Let's look at how to rename multiple columns in a performant manner.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#renaming-multiple-columns","title":"Renaming multiple columns","text":"<p>The quinn library has a <code>with_columns_renamed</code> function that renames all the columns in a DataFrame.</p> <p>Suppose you have the following DataFrame:</p> <pre><code>+-------------+-----------+\n|i like cheese|yummy stuff|\n+-------------+-----------+\n|         jose|          a|\n|           li|          b|\n|          sam|          c|\n+-------------+-----------+\n</code></pre> <p>Here's how to replace all the whitespace in the column names with underscores:</p> <pre><code>import quinn\n\ndef spaces_to_underscores(s):\n    return s.replace(\" \", \"_\")\n\nactual_df = quinn.with_columns_renamed(spaces_to_underscores)(source_df)\n\nactual_df.show()\n</code></pre> <pre><code>+-------------+-----------+\n|i_like_cheese|yummy_stuff|\n+-------------+-----------+\n|         jose|          a|\n|           li|          b|\n|          sam|          c|\n+-------------+-----------+\n</code></pre> <p>This code generates an efficient parsed logical plan:</p> <pre><code>== Parsed Logical Plan ==\n'Project ['`i.like.cheese` AS i_like_cheese#12, '`yummy.stuff` AS yummy_stuff#13]\n+- LogicalRDD [i.like.cheese#8, yummy.stuff#9], false\n\n== Analyzed Logical Plan ==\ni_like_cheese: string, yummy_stuff: string\nProject [i.like.cheese#8 AS i_like_cheese#12, yummy.stuff#9 AS yummy_stuff#13]\n+- LogicalRDD [i.like.cheese#8, yummy.stuff#9], false\n\n== Optimized Logical Plan ==\nProject [i.like.cheese#8 AS i_like_cheese#12, yummy.stuff#9 AS yummy_stuff#13]\n+- LogicalRDD [i.like.cheese#8, yummy.stuff#9], false\n\n== Physical Plan ==\n*(1) Project [i.like.cheese#8 AS i_like_cheese#12, yummy.stuff#9 AS yummy_stuff#13]\n+- Scan ExistingRDD[i.like.cheese#8,yummy.stuff#9]\n</code></pre> <p>The parsed logical plan and the optimized logical plan are the same so the Spark Catalyst optimizer does not have to do any hard work.</p> <p><code>with_columns_renamed</code> takes two sets of arguments, so it can be chained with the DataFrame transform method. This code will give you the same result:</p> <pre><code>source_df.transform(quinn.with_columns_renamed(spaces_to_underscores))\n</code></pre> <p>The <code>transform</code> method is included in the PySpark 3 API. If you're using Spark 2, you need to monkey patch transform onto the DataFrame class, as described in this the blog post.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#with_columns_renamed-source-code","title":"with_columns_renamed source code","text":"<p>Here's the source code for the <code>with_columns_renamed</code> method:</p> <pre><code>def with_columns_renamed(fun):\n    def _(df):\n        cols = list(map(\n            lambda col_name: F.col(\"`{0}`\".format(col_name)).alias(fun(col_name)),\n            df.columns\n        ))\n        return df.select(*cols)\n    return _\n</code></pre> <p>The code creates a list of the new column names and runs a single select operation. As you've already seen, this code generates an efficient parsed logical plan.</p> <p>Notice that this code does not run <code>withColumnRenamed</code> multiple times, like other implementations suggest. Calling <code>withColumnRenamed</code> many times is a performance bottleneck.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#todf-for-renaming-columns","title":"toDF for renaming columns","text":"<p>Let's use <code>toDF</code> to replace the whitespace with underscores (same objective, different implementation).</p> <pre><code>+-------------+-----------+\n|i like cheese|yummy stuff|\n+-------------+-----------+\n|         jose|          a|\n|           li|          b|\n|          sam|          c|\n+-------------+-----------+\n</code></pre> <p>Here's how we can update the column names with <code>toDF</code>:</p> <pre><code>df.toDF(*(c.replace(' ', '_') for c in df.columns)).show()\n</code></pre> <pre><code>+-------------+-----------+\n|i_like_cheese|yummy_stuff|\n+-------------+-----------+\n|         jose|          a|\n|           li|          b|\n|          sam|          c|\n+-------------+-----------+\n</code></pre> <p>This approach generates an efficient parsed plan.</p> <pre><code>== Parsed Logical Plan ==\nProject [i.like.cheese#0 AS i_like_cheese#11, yummy.stuff#1 AS yummy_stuff#12]\n+- LogicalRDD [i.like.cheese#0, yummy.stuff#1], false\n\n== Analyzed Logical Plan ==\ni_like_cheese: string, yummy_stuff: string\nProject [i.like.cheese#0 AS i_like_cheese#11, yummy.stuff#1 AS yummy_stuff#12]\n+- LogicalRDD [i.like.cheese#0, yummy.stuff#1], false\n\n== Optimized Logical Plan ==\nProject [i.like.cheese#0 AS i_like_cheese#11, yummy.stuff#1 AS yummy_stuff#12]\n+- LogicalRDD [i.like.cheese#0, yummy.stuff#1], false\n\n== Physical Plan ==\n*(1) Project [i.like.cheese#0 AS i_like_cheese#11, yummy.stuff#1 AS yummy_stuff#12]\n</code></pre> <p>You could also wrap this code in a function and give it a method signature so it can be chained with the <code>transform</code> method.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#withcolumnrenamed-antipattern-when-renaming-multiple-columns","title":"withColumnRenamed antipattern when renaming multiple columns","text":"<p>You can call <code>withColumnRenamed</code> multiple times, but this isn't a good solution because it creates a complex parsed logical plan.</p> <p>Here the <code>withColumnRenamed</code> implementation:</p> <pre><code>def rename_cols(df):\n    for column in df.columns:\n        new_column = column.replace('.','_')\n        df = df.withColumnRenamed(column, new_column)\n    return df\n\nrename_cols(df).explain(True)\n</code></pre> <p>Here are the logical plans:</p> <pre><code>== Parsed Logical Plan ==\nProject [i_like_cheese#31, yummy.stuff#28 AS yummy_stuff#34]\n+- Project [i.like.cheese#27 AS i_like_cheese#31, yummy.stuff#28]\n   +- LogicalRDD [i.like.cheese#27, yummy.stuff#28], false\n\n== Analyzed Logical Plan ==\ni_like_cheese: string, yummy_stuff: string\nProject [i_like_cheese#31, yummy.stuff#28 AS yummy_stuff#34]\n+- Project [i.like.cheese#27 AS i_like_cheese#31, yummy.stuff#28]\n   +- LogicalRDD [i.like.cheese#27, yummy.stuff#28], false\n\n== Optimized Logical Plan ==\nProject [i.like.cheese#27 AS i_like_cheese#31, yummy.stuff#28 AS yummy_stuff#34]\n+- LogicalRDD [i.like.cheese#27, yummy.stuff#28], false\n\n== Physical Plan ==\n*(1) Project [i.like.cheese#27 AS i_like_cheese#31, yummy.stuff#28 AS yummy_stuff#34]\n</code></pre> <p>The parsed and analyzed logical plans are more complex than what we've seen before. In this case, the Spark Catalyst optimizer is smart enough to perform optimizations and generate the same optimized logical plan, but parsing logical plans takes time as described in this blog post. It's best to write code that's easy for Catalyst to optimize. When you can, write code that has simple parsed logical plans.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#renaming-some-columns","title":"Renaming some columns","text":"<p>Suppose you have the following DataFrame with column names that use British English. You'd like to convert these column names to American English (change chips to french_fries and petrol to gas). You don't want to rename or remove columns that aren't being remapped to American English - you only want to change certain column names.</p> <pre><code>+------+-----+------+\n| chips|   hi|petrol|\n+------+-----+------+\n|potato|hola!| disel|\n+------+-----+------+\n</code></pre> <p>The quinn <code>with_some_columns_renamed</code> function makes it easy to rename some columns.</p> <pre><code>import quinn\n\nmapping = {\"chips\": \"french_fries\", \"petrol\": \"gas\"}\n\ndef british_to_american(s):\n    return mapping[s]\n\ndef change_col_name(s):\n    return s in mapping\n\nactual_df = quinn.with_some_columns_renamed(british_to_american, change_col_name)(source_df)\n\nactual_df.show()\n</code></pre> <pre><code>+------------+-----+-----+\n|french_fries|   hi|  gas|\n+------------+-----+-----+\n|      potato|hola!|disel|\n+------------+-----+-----+\n</code></pre> <p>This code generates an efficient parsed logical plan:</p> <pre><code>== Parsed Logical Plan ==\n'Project ['chips AS french_fries#32, unresolvedalias('hi, None), 'petrol AS gas#33]\n+- LogicalRDD [chips#16, hi#17, petrol#18], false\n\n== Analyzed Logical Plan ==\nfrench_fries: string, hi: string, gas: string\nProject [chips#16 AS french_fries#32, hi#17, petrol#18 AS gas#33]\n+- LogicalRDD [chips#16, hi#17, petrol#18], false\n\n== Optimized Logical Plan ==\nProject [chips#16 AS french_fries#32, hi#17, petrol#18 AS gas#33]\n+- LogicalRDD [chips#16, hi#17, petrol#18], false\n\n== Physical Plan ==\n*(1) Project [chips#16 AS french_fries#32, hi#17, petrol#18 AS gas#33]\n</code></pre> <p>The <code>with_some_columns_renamed</code> function takes two arguments:</p> <ul> <li>The first argument is a function specifies how the strings should be modified</li> <li>The second argument is a function that returns True if the string should be modified and False otherwise</li> </ul>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#replacing-dots-with-underscores-in-column-names","title":"Replacing dots with underscores in column names","text":"<p>You should always replace dots with underscores in PySpark column names, as explained in this post.</p> <p>You can apply the methodologies you've learned in this blog post to easily replace dots with underscores.</p>"},{"location":"pyspark/rename-multiple-columns-todf-withcolumnrenamed/#next-steps","title":"Next steps","text":"<p>It's important to write code that renames columns efficiently in Spark. You'll often start an analysis by read from a datasource and renaming the columns. If you use an inefficient renaming implementation, you're parsed logical plan will start out complex and will only get more complicated as you layer on more DataFrame transformations.</p> <p>Complicated parsed logical plans are difficult for the Catalyst optimizer to optimize.</p> <p>Make sure to read this blog post on chaining DataFrame transformations, so you're comfortable renaming columns with a function that's passed to the DataFrame#transform method. Writing elegant PySpark code will help you keep your notebooks clean and easy to read.</p>"},{"location":"pyspark/schema-structtype-structfield/","title":"Defining PySpark Schemas with StructType and StructField","text":"<p>This post explains how to define PySpark schemas and when this design pattern is useful.</p> <p>It'll also explain when defining schemas seems wise, but can actually be safely avoided.</p> <p>Schemas are often defined when validating DataFrames, reading in data from CSV files, or when manually constructing DataFrames in your test suite. You'll use all of the information covered in this post frequently when writing PySpark code.</p>"},{"location":"pyspark/schema-structtype-structfield/#access-dataframe-schema","title":"Access DataFrame schema","text":"<p>Let's create a PySpark DataFrame and then access the schema.</p> <pre><code>df = spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"num\", \"letter\"])\ndf.show()\n</code></pre> <pre><code>+---+------+\n|num|letter|\n+---+------+\n|  1|     a|\n|  2|     b|\n+---+------+\n</code></pre> <p>Use the <code>printSchema()</code> method to print a human readable version of the schema.</p> <pre><code>df.printSchema()\n\nroot\n |-- num: long (nullable = true)\n |-- letter: string (nullable = true)\n</code></pre> <p>The <code>num</code> column is long type and the <code>letter</code> column is string type. We created this DataFrame with the <code>createDataFrame</code> method and did not explicitly specify the types of each column. Spark infers the types based on the row values when you don't explicitly provides types.</p> <p>Use the <code>schema</code> attribute to fetch the actual schema object associated with a DataFrame.</p> <pre><code>df.schema\n\nStructType(List(StructField(num,LongType,true),StructField(letter,StringType,true)))\n</code></pre> <p>The entire schema is stored in a <code>StructType</code>. The details for each column in the schema is stored in <code>StructField</code> objects. Each <code>StructField</code> contains the column name, type, and nullable property.</p>"},{"location":"pyspark/schema-structtype-structfield/#define-basic-schema","title":"Define basic schema","text":"<p>Let's create another DataFrame, but specify the schema ourselves rather than relying on schema inference.</p> <pre><code>from pyspark.sql import Row\nfrom pyspark.sql.types import *\n\nrdd = spark.sparkContext.parallelize([\n    Row(name='Allie', age=2),\n    Row(name='Sara', age=33),\n    Row(name='Grace', age=31)])\n\nschema = schema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), False)])\n\ndf = spark.createDataFrame(rdd, schema)\n\ndf.show()\n</code></pre> <pre><code>+-----+---+\n| name|age|\n+-----+---+\n|Allie|  2|\n| Sara| 33|\n|Grace| 31|\n+-----+---+\n</code></pre> <p>This example uses the same <code>createDataFrame</code> method as earlier, but invokes it with a RDD and a <code>StructType</code> (a full schema object).</p> <p>Use the <code>printSchema()</code> method to verify that the DataFrame has the exact schema we specified.</p> <pre><code>df.printSchema()\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n</code></pre> <p>We can see that the column names, types, and nullable properties are exactly what we specified. Production grade code and test suites often require this fine grained precision.</p> <p>This post on creating PySpark DataFrames discusses another tactic for precisely creating schemas without so much typing.</p>"},{"location":"pyspark/schema-structtype-structfield/#define-schema-with-arraytype","title":"Define schema with ArrayType","text":"<p>PySpark DataFrames support array columns. An array can hold different objects, the type of which much be specified when defining the schema.</p> <p>Let's create a DataFrame with a column that holds an array of integers.</p> <pre><code>rdd = spark.sparkContext.parallelize([\n    Row(letter=\"a\", nums=[1, 2, 3]),\n    Row(letter=\"b\", nums=[4, 5, 6])])\n\nschema = schema = StructType([\n    StructField(\"letter\", StringType(), True),\n    StructField(\"nums\", ArrayType(IntegerType(), False))\n])\n\ndf = spark.createDataFrame(rdd, schema)\n\ndf.show()\n</code></pre> <pre><code>+------+---------+\n|letter|     nums|\n+------+---------+\n|     a|[1, 2, 3]|\n|     b|[4, 5, 6]|\n+------+---------+\n</code></pre> <p>Print the schema to view the ArrayType column.</p> <pre><code>df.printSchema()\n\nroot\n |-- letter: string (nullable = true)\n |-- nums: array (nullable = true)\n |    |-- element: integer (containsNull = false)\n</code></pre> <p>Array columns are useful for a variety of PySpark analyses.</p>"},{"location":"pyspark/schema-structtype-structfield/#nested-schemas","title":"Nested schemas","text":"<p>Schemas can also be nested. Let's build a DataFrame with a <code>StructType</code> within a <code>StructType</code>.</p> <pre><code>rdd = spark.sparkContext.parallelize([\n    Row(first_name=\"reggie\", stats=Row(home_runs=563, batting_average=0.262)),\n    Row(first_name=\"barry\", stats=Row(home_runs=762, batting_average=0.298))])\n\nschema = schema = StructType([\n    StructField(\"letter\", StringType(), True),\n    StructField(\n        \"stats\",\n        StructType([\n            StructField(\"home_runs\", IntegerType(), False),\n            StructField(\"batting_average\", DoubleType(), False),\n        ]),\n        True\n    ),\n])\n\ndf = spark.createDataFrame(rdd, schema)\n\ndf.show()\n</code></pre> <pre><code>+------+------------+\n|letter|       stats|\n+------+------------+\n|reggie|{563, 0.262}|\n| barry|{762, 0.298}|\n+------+------------+\n</code></pre> <p>Let's print the nested schema:</p> <pre><code>df.printSchema()\n\nroot\n |-- letter: string (nullable = true)\n |-- stats: struct (nullable = true)\n |    |-- home_runs: integer (nullable = false)\n |    |-- batting_average: double (nullable = false)\n</code></pre> <p>Nested schemas allow for a powerful way to organize data, but they also introduction additional complexities. It's generally easier to work with flat schemas, but nested (and deeply nested schemas) also allow for elegant solutions to certain problems.</p>"},{"location":"pyspark/schema-structtype-structfield/#reading-csv-files","title":"Reading CSV files","text":"<p>When reading a CSV file, you can either rely on schema inference or specify the schema yourself.</p> <p>For data exploration, schema inference is usually fine. You don't have to be overly concerned about types and nullable properties when you're just getting to know a dataset.</p> <p>For production applications, it's best to explicitly define the schema and avoid inference. You don't want to rely on fragile inference rules that may get updated and cause unanticipated changes in your code.</p> <p>Parquet files contain the schema information in the file footer, so you get the best of both worlds. You don't have to rely on schema inference and don't have to tediously define the schema yourself. This is one of many reasons why Parquet files are almost always better than CSV files in data analyses.</p>"},{"location":"pyspark/schema-structtype-structfield/#validations","title":"Validations","text":"<p>Suppose you're working with a data vendor that gives you an updated CSV file on a weekly basis that you need to ingest into your systems.</p> <p>The first step of your ingestion pipeline should be to validate that the schema of the file is what you expect. You don't want to ingest a file, and potentially corrupt a data lake, because the data vendor made some changes to the input file.</p> <p>The quinn data validation helper methods can assist you in validating schemas. You'll of course need to specify the expected schema, using the tactics outlined in this post, to invoke the schema validation checks.</p>"},{"location":"pyspark/schema-structtype-structfield/#test-suites","title":"Test suites","text":"<p>PySpark code is often tested by comparing two DataFrames or comparing two columns within a DataFrame. Creating DataFrames requires building schemas, using the tactics outlined in this post. See this post for more information on Testing PySpark Applications.</p>"},{"location":"pyspark/schema-structtype-structfield/#next-steps","title":"Next steps","text":"<p>PySpark exposes elegant schema specification APIs that help you create DataFrames, build reliable tests, and construct robust data pipelines.</p> <p>You'll be building PySpark schemas frequently so you might as well just memorize the syntax.</p>"},{"location":"pyspark/select-add-columns-withcolumn/","title":"select and add columns in PySpark","text":"<p>This post shows you how to select a subset of the columns in a DataFrame with <code>select</code>. It also shows how <code>select</code> can be used to add and rename columns. Most PySpark users don't know how to truly harness the power of <code>select</code>.</p> <p>This post also shows how to add a column with <code>withColumn</code>. Newbie PySpark developers often run <code>withColumn</code> multiple times to add multiple columns because there isn't a <code>withColumns</code> method. We will see why chaining multiple <code>withColumn</code> calls is an anti-pattern and how to avoid this pattern with <code>select</code>.</p> <p>This post starts with basic use cases and then advances to the lesser-known, powerful applications of these methods.</p>"},{"location":"pyspark/select-add-columns-withcolumn/#select-basic-use-case","title":"select basic use case","text":"<p>Create a DataFrame with three columns.</p> <pre><code>df = spark.createDataFrame(\n    [(\"jose\", 1, \"mexico\"), (\"li\", 2, \"china\"), (\"sandy\", 3, \"usa\")],\n    [\"name\", \"age\", \"country\"],\n)\ndf.show()\n</code></pre> <pre><code>+-----+---+-------+\n| name|age|country|\n+-----+---+-------+\n| jose|  1| mexico|\n|   li|  2|  china|\n|sandy|  3|    usa|\n+-----+---+-------+\n</code></pre> <p>Select the <code>age</code> and <code>name</code> columns:</p> <pre><code>df.select(\"age\", \"name\").show()\n</code></pre> <pre><code>+---+-----+\n|age| name|\n+---+-----+\n|  1| jose|\n|  2|   li|\n|  3|sandy|\n+---+-----+\n</code></pre> <p>The <code>select</code> method takes column names as arguments.</p> <p>If you try to select a column that doesn't exist in the DataFrame, your code will error out. Here's the error you'll see if you run <code>df.select(\"age\", \"name\", \"whatever\")</code>.</p> <pre><code>def deco(*a, **kw):\n    try:\n        return f(*a, **kw)\n    except py4j.protocol.Py4JJavaError as e:\n        converted = convert_exception(e.java_exception)\n        if not isinstance(converted, UnknownException):\n            # Hide where the exception came from that shows a non-Pythonic\n            # JVM exception message.\n           raise converted from None\n           pyspark.sql.utils.AnalysisException: cannot resolve '`whatever`' given input columns: [age, country, name];\n           'Project [age#77L, name#76, 'whatever]\n           +- LogicalRDD [name#76, age#77L, country#78], false\n</code></pre> <p>Get used to parsing PySpark stack traces!</p> <p>The <code>select</code> method can also take an array of column names as the argument.</p> <pre><code>df.select([\"country\", \"name\"]).show()\n</code></pre> <pre><code>+-------+-----+\n|country| name|\n+-------+-----+\n| mexico| jose|\n|  china|   li|\n|    usa|sandy|\n+-------+-----+\n</code></pre> <p>You can also select based on an array of column objects:</p> <pre><code>df.select([col(\"age\")]).show()\n</code></pre> <pre><code>+---+\n|age|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n</code></pre> <p>Keep reading to see how selecting on an array of column object allows for advanced use cases, like renaming columns.</p>"},{"location":"pyspark/select-add-columns-withcolumn/#withcolumn-basic-use-case","title":"withColumn basic use case","text":"<p><code>withColumn</code> adds a column to a DataFrame.</p> <p>Create a DataFrame with two columns:</p> <pre><code>df = spark.createDataFrame(\n    [(\"jose\", 1), (\"li\", 2), (\"luisa\", 3)], [\"name\", \"age\"]\n)\ndf.show()\n</code></pre> <pre><code>+-----+---+\n| name|age|\n+-----+---+\n| jose|  1|\n|   li|  2|\n|luisa|  3|\n+-----+---+\n</code></pre> <p>Append a <code>greeting</code> column to the DataFrame with the string <code>hello</code>:</p> <pre><code>df.withColumn(\"greeting\", lit(\"hello\")).show()\n</code></pre> <pre><code>+-----+---+--------+\n| name|age|greeting|\n+-----+---+--------+\n| jose|  1|   hello|\n|   li|  2|   hello|\n|luisa|  3|   hello|\n+-----+---+--------+\n</code></pre> <p>Now let's use <code>withColumn</code> to append an <code>upper_name</code> column that uppercases the <code>name</code> column.</p> <pre><code>df.withColumn(\"upper_name\", upper(col(\"name\"))).show()\n</code></pre> <pre><code>+-----+---+----------+\n| name|age|upper_name|\n+-----+---+----------+\n| jose|  1|      JOSE|\n|   li|  2|        LI|\n|luisa|  3|     LUISA|\n+-----+---+----------+\n</code></pre> <p><code>withColumn</code> is often used to append columns based on the values of other columns.</p>"},{"location":"pyspark/select-add-columns-withcolumn/#add-multiple-columns-withcolumns","title":"Add multiple columns (withColumns)","text":"<p>There isn't a <code>withColumns</code> method, so most PySpark newbies call <code>withColumn</code> multiple times when they need to add multiple columns to a DataFrame.</p> <p>Create a simple DataFrame:</p> <pre><code>df = spark.createDataFrame(\n    [(\"cali\", \"colombia\"), (\"london\", \"uk\")],\n    [\"city\", \"country\"],\n)\ndf.show()\n</code></pre> <pre><code>+------+--------+\n|  city| country|\n+------+--------+\n|  cali|colombia|\n|london|      uk|\n+------+--------+\n</code></pre> <p>Here's how to append two columns with constant values to the DataFrame using <code>select</code>:</p> <pre><code>actual = df.select([\"*\", lit(\"val1\").alias(\"col1\"), lit(\"val2\").alias(\"col2\")])\nactual.show()\n</code></pre> <pre><code>+------+--------+----+----+\n|  city| country|col1|col2|\n+------+--------+----+----+\n|  cali|colombia|val1|val2|\n|london|      uk|val1|val2|\n+------+--------+----+----+\n</code></pre> <p>The <code>*</code> selects all of the existing DataFrame columns and the other columns are appended. This design pattern is how <code>select</code> can append columns to a DataFrame, just like <code>withColumn</code>.</p> <p>The code is a bit verbose, but it's better than the following code that calls <code>withColumn</code> multiple times:</p> <pre><code>df.withColumn(\"col1\", lit(\"val1\")).withColumn(\"col2\", lit(\"val2\"))\n</code></pre> <p>There is a hidden cost of withColumn and calling it multiple times should be avoided.</p> <p>The Spark contributors are considering adding withColumns to the API, which would be the best option. That'd give the community a clean and performant way to add multiple columns.</p>"},{"location":"pyspark/select-add-columns-withcolumn/#snake-case-all-columns","title":"Snake case all columns","text":"<p>Create a DataFrame with annoyingly named columns:</p> <pre><code>annoying = spark.createDataFrame(\n    [(3, \"mystery\"), (23, \"happy\")],\n    [\"COOL NUMBER\", \"RELATED EMOTION\"],\n)\nannoying.show()\n</code></pre> <pre><code>+-----------+---------------+\n|COOL NUMBER|RELATED EMOTION|\n+-----------+---------------+\n|          3|        mystery|\n|         23|          happy|\n+-----------+---------------+\n</code></pre> <p>Gross.</p> <p>Write some code that'll convert all the column names to snake_case:</p> <pre><code>def to_snake_case(s):\n    return s.lower().replace(\" \", \"_\")\n\ncols = [col(s).alias(to_snake_case(s)) for s in annoying.columns]\nannoying.select(cols).show()\n</code></pre> <pre><code>+-----------+---------------+\n|cool_number|related_emotion|\n+-----------+---------------+\n|          3|        mystery|\n|         23|          happy|\n+-----------+---------------+\n</code></pre> <p>Some DataFrames have hundreds or thousands of columns, so it's important to know how to rename all the columns programatically with a loop, followed by a <code>select</code>.</p>"},{"location":"pyspark/select-add-columns-withcolumn/#remove-dots-from-all-column-names","title":"Remove dots from all column names","text":"<p>Create a DataFrame with dots in the column names:</p> <pre><code>annoying = spark.createDataFrame(\n    [(3, \"mystery\"), (23, \"happy\")],\n    [\"cool.number\", \"related.emotion\"],\n)\nannoying.show()\n</code></pre> <pre><code>+-----------+---------------+\n|cool.number|related.emotion|\n+-----------+---------------+\n|          3|        mystery|\n|         23|          happy|\n+-----------+---------------+\n</code></pre> <p>Remove the dots from the column names and replace them with underscores.</p> <pre><code>cols = [col(\"`\" + s + \"`\").alias(s.replace(\".\", \"_\")) for s in annoying.columns]\nannoying.select(cols).show()\n</code></pre> <pre><code>+-----------+---------------+\n|cool_number|related_emotion|\n+-----------+---------------+\n|          3|        mystery|\n|         23|          happy|\n+-----------+---------------+\n</code></pre> <p>Notice that this code hacks in backticks around the column name or else it'll error out (simply calling <code>col(s)</code> will cause an error in this case). These backticks are needed whenever the column name contains periods. Super annoying.</p> <p>You should never have dots in your column names as discussed in this post. Dots in column names cause weird bugs. Always get rid of dots in column names whenever you see them.</p>"},{"location":"pyspark/select-add-columns-withcolumn/#conclusion","title":"Conclusion","text":"<p>The <code>select</code> method can be used to grab a subset of columns, rename columns, or append columns. It's a powerful method that has a variety of applications.</p> <p><code>withColumn</code> is useful for adding a single column. It shouldn't be chained when adding multiple columns (fine to chain a few times, but shouldn't be chained hundreds of times). You now know how to append multiple columns with <code>select</code>, so you can avoid chaining <code>withColumn</code> calls.</p> <p>Hopefully <code>withColumns</code> is added to the PySpark codebase so it's even easier to add multiple columns.</p>"},{"location":"pyspark/sparksession-getorcreate-getactivesession/","title":"Creating and reusing the SparkSession with PySpark","text":"<p>This post explains how to create a SparkSession with <code>getOrCreate</code> and how to reuse the SparkSession with <code>getActiveSession</code>.</p> <p>You need a SparkSession to read data stored in files, when manually creating DataFrames, and to run arbitrary SQL queries.</p> <p>The SparkSession should be instantiated once and then reused throughout your application. Most applications should not create multiple sessions or shut down an existing session.</p> <p>When you're running Spark workflows locally, you're responsible for instantiating the SparkSession yourself. Spark runtime providers build the SparkSession for you and you should reuse it. You need to write code that properly manages the SparkSession for both local and production workflows.</p> <p>This post shows you how to build a resilient codebase that properly manages the SparkSession in the development, test, and production environments.</p>"},{"location":"pyspark/sparksession-getorcreate-getactivesession/#getorcreate","title":"getOrCreate","text":"<p>Here's an example of how to create a SparkSession with the builder:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = (SparkSession.builder\n  .master(\"local\")\n  .appName(\"chispa\")\n  .getOrCreate())\n</code></pre> <p><code>getOrCreate</code> will either create the SparkSession if one does not already exist or reuse an existing SparkSession.</p> <p>Let's look at a code snippet from the chispa test suite that uses this SparkSession.</p> <pre><code>import pytest\n\nfrom spark import *\nfrom chispa import *\n\ndef test_assert_df_equality():\n    data1 = [(1, \"jose\"), (2, \"li\")]\n    df1 = spark.createDataFrame(data1, [\"num\", \"name\"])\n    data2 = [(2, \"li\"), (1, \"jose\")]\n    df2 = spark.createDataFrame(data2, [\"num\", \"name\"])\n    assert_df_equality(df1, df2, ignore_row_order=True)\n</code></pre> <p><code>from spark import *</code> gives us access to the <code>spark</code> variable that contains the SparkSession used to create the DataFrames in this test.</p> <p>Reusing the same SparkSession throughout your test suite is important for your test suite performance. Shutting down and recreating SparkSessions is expensive and causes test suites to run painfully slowly.</p>"},{"location":"pyspark/sparksession-getorcreate-getactivesession/#getactivesession","title":"getActiveSession","text":"<p>Some functions can assume a SparkSession exists and should error out if the SparkSession does not exist.</p> <p>You should only be using <code>getOrCreate</code> in functions that should actually be creating a SparkSession. <code>getActiveSession</code> is more appropriate for functions that should only reuse an existing SparkSession.</p> <p>The <code>show_output_to_df</code> function in quinn is a good example of a function that uses <code>getActiveSession</code>. This function converts the string that's outputted from DataFrame#show back into a DataFrame object. It's useful when you only have the show output in a Stackoverflow question and want to quickly recreate a DataFrame.</p> <p>Let's take a look at the function in action:</p> <pre><code>import quinn\n\ns = \"\"\"+----+---+-----------+------+\n|name|age|     stuff1|stuff2|\n+----+---+-----------+------+\n|jose|  1|nice person|  yoyo|\n|  li|  2|nice person|  yoyo|\n| liz|  3|nice person|  yoyo|\n+----+---+-----------+------+\"\"\"\n\ndf = quinn.show_output_to_df(s)\n</code></pre> <p><code>show_output_to_df</code> uses a SparkSession under the hood to create the DataFrame, but does not force the user to pass the SparkSession as a function argument because that'd be tedious.</p> <p>Let's look at the function implementation:</p> <pre><code>def show_output_to_df(show_output):\n    l = show_output.split(\"\\n\")\n    ugly_column_names = l[1]\n    pretty_column_names = [i.strip() for i in ugly_column_names[1:-1].split(\"|\")]\n    pretty_data = []\n    ugly_data = l[3:-1]\n    for row in ugly_data:\n        r = [i.strip() for i in row[1:-1].split(\"|\")]\n        pretty_data.append(tuple(r))\n    return SparkSession.getActiveSession().createDataFrame(pretty_data, pretty_column_names)\n</code></pre> <p><code>show_output_to_df</code> takes a String as an argument and returns a DataFrame. It's a great example of a helper function that hides complexity and makes Spark easier to manage.</p>"},{"location":"pyspark/sparksession-getorcreate-getactivesession/#sparksession-from-dataframe","title":"SparkSession from DataFrame","text":"<p>You can also grab the SparkSession that's associated with a DataFrame.</p> <pre><code>data1 = [(1, \"jose\"), (2, \"li\")]\ndf1 = spark.createDataFrame(data1, [\"num\", \"name\"])\ndf1.sql_ctx.sparkSession\n</code></pre> <p>The SparkSession that's associated with <code>df1</code> is the same as the active SparkSession and can also be accessed as follows:</p> <pre><code>from pyspark.sql import SparkSession\n\nSparkSession.getActiveSession()\n</code></pre> <p>If you have a DataFrame, you can use it to access the SparkSession, but it's best to just grab the SparkSession with <code>getActiveSession()</code>.</p> <p>Let's shut down the active SparkSession to demonstrate the <code>getActiveSession()</code> returns <code>None</code> when no session exists.</p> <pre><code>spark.stop()\n\nSparkSession.getActiveSession() # None\n</code></pre> <p>Here's the error you'll get if you try to create a DataFrame now that the SparkSession was stopped.</p> <pre><code>spark.createDataFrame(data1, [\"num\", \"name\"])\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\", line 675, in createDataFrame\n    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\", line 700, in _create_dataframe\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\", line 526, in _createFromLocal\n    return self._sc.parallelize(data), schema\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\", line 530, in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\n  File \"/Users/powers/spark/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\", line 442, in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\nAttributeError: 'NoneType' object has no attribute 'sc'\n</code></pre>"},{"location":"pyspark/sparksession-getorcreate-getactivesession/#next-steps","title":"Next steps","text":"<p>You've learned how to effectively manage the SparkSession in your PySpark applications.</p> <p>You can create a SparkSession that's reused throughout your test suite and leverage SparkSessions created by third party Spark runtimes.</p>"},{"location":"pyspark/testing-pytest-chispa/","title":"Testing PySpark Code","text":"<p>This blog post explains how to test PySpark code with the chispa helper library.</p> <p>Writing fast PySpark tests that provide your codebase with adequate coverage is surprisingly easy when you follow some simple design patters. chispa outputs readable error messages to facilitate your development workflow.</p> <p>A robust test suite makes it easy for you to add new features and refactor your codebase. It also provides other developers with \"living code documentation\" - they can see the inputs and outputs of your functions.</p>"},{"location":"pyspark/testing-pytest-chispa/#example-project","title":"Example project","text":"<p>The pysparktestingexample project was created with Poetry, the best package manager for PySpark projects. All the code covered in this post is in the pysparktestingexample repo.</p> <p>Here are the commands that were run to setup the project:</p> <ul> <li><code>poetry new pysparktestingexample</code>: creates a new project and automatically includes pytest</li> <li><code>poetry add pyspark</code>: adds PySpark to the project</li> <li><code>poetry add chispa --dev</code>: adds chispa as a development dependency</li> </ul> <p>chispa is only needed in the test suite and that's why it's added as a development dependency.</p> <p>Your <code>pypoetry.toml</code> file will look like this after running the commands.</p> <pre><code>[tool.poetry]\nname = \"pysparktestingexample\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"MrPowers &lt;matthewkevinpowers@gmail.com&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\npyspark = \"^2.4.6\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^5.2\"\nchispa = \"^0.3.0\"\n\n[build-system]\nrequires = [\"poetry&gt;=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n</code></pre> <p>Poetry sets up a virtual environment with the PySpark, pytest, and chispa code that's needed for this example application.</p>"},{"location":"pyspark/testing-pytest-chispa/#sparksession","title":"SparkSession","text":"<p>Let's start by setting up the SparkSession in a pytest fixture, so it's easily accessible by all our tests.</p> <p>You'll use the SparkSession frequently in your test suite to build DataFrames.</p> <p>Create a <code>tests/conftest.py</code> file and add this code:</p> <pre><code>import pytest\nfrom pyspark.sql import SparkSession\n\n@pytest.fixture(scope='session')\ndef spark():\n    return SparkSession.builder \\\n      .master(\"local\") \\\n      .appName(\"chispa\") \\\n      .getOrCreate()\n</code></pre> <p>Let's use this fixture to create a test that compares the equality of two columns.</p>"},{"location":"pyspark/testing-pytest-chispa/#column-equality","title":"Column equality","text":"<p>Create a <code>functions.py</code> file and add a <code>remove_non_word_characters</code> function that'll remove all the non-word characters from a string.</p> <pre><code>import pyspark.sql.functions as F\n\ndef remove_non_word_characters(col):\n    return F.regexp_replace(col, \"[^\\\\w\\\\s]+\", \"\")\n</code></pre> <p>Let's write a test that makes sure this function removes all the non-word characters in strings.</p> <p>We'll put this code in a <code>tests/test_functions.py</code> file.</p> <p>Create a DataFrame with a column that contains non-word characters, run the <code>remove_non_word_characters</code> function, and check that all the non-word characters are removed with the chispa <code>assert_column_equality</code> method.</p> <pre><code>import pytest\n\nfrom pysparktestingexample.functions import remove_non_word_characters\nfrom chispa import *\nimport pyspark.sql.functions as F\n\ndef test_remove_non_word_characters(spark):\n    data = [\n        (\"jo&amp;&amp;se\", \"jose\"),\n        (\"**li**\", \"li\"),\n        (\"#::luisa\", \"luisa\"),\n        (None, None)\n    ]\n    df = spark.createDataFrame(data, [\"name\", \"expected_name\"])\\\n        .withColumn(\"clean_name\", remove_non_word_characters(F.col(\"name\")))\n    assert_column_equality(df, \"clean_name\", \"expected_name\")\n</code></pre> <p>The DataFrame is initally created with the \"input\" and \"expected\" values. <code>withColumn</code> appends the \"actual\" value that's returned from running the function that's being tested.</p> <p>Tests generally compare \"actual\" values with \"expected\" values. In this example, <code>clean_name</code> (actual value) is compared with <code>expected_name</code> (expected value).</p> <p>Notice that one row has an input value equal to <code>None</code>. Always test the <code>None</code> case and make sure the code does not error out.</p> <p>Notice that <code>def test_remove_non_word_characters(spark)</code> includes a reference to the \"spark\" fixture we created in the <code>conftest.py</code> file. Remember that this fixture is what provides you with access to the SparkSession in your tests so you can create DataFrames.</p>"},{"location":"pyspark/testing-pytest-chispa/#failing-column-equality-test","title":"Failing column equality test","text":"<p>Let's write another test that'll error out and inspect the test output to see how to debug the issue.</p> <p>Here's the failing test.</p> <pre><code>def test_remove_non_word_characters_nice_error(spark):\n    data = [\n        (\"matt7\", \"matt\"),\n        (\"bill&amp;\", \"bill\"),\n        (\"isabela*\", \"isabela\"),\n        (None, None)\n    ]\n    df = spark.createDataFrame(data, [\"name\", \"expected_name\"])\\\n        .withColumn(\"clean_name\", remove_non_word_characters(F.col(\"name\")))\n    assert_column_equality(df, \"clean_name\", \"expected_name\")\n</code></pre> <p>This'll return a nicely formatted error message:</p> <p></p> <p>We can see the <code>matt7</code> / <code>matt</code> row of data is what's causing the error because it's colored red. The other rows are colored blue because they're equal.</p> <p>Descriptive error messages are an advantage of the chispa library. Other libraries don't output error messages that allow developers to easily identify mismatched rows.</p>"},{"location":"pyspark/testing-pytest-chispa/#dataframe-equality","title":"DataFrame equality","text":"<p>Create a <code>transformations.py</code> file and add a <code>sort_columns</code> method that sorts the columns of a DataFrame in ascending or descending alphabetical order.</p> <pre><code>def sort_columns(df, sort_order):\n    sorted_col_names = None\n    if sort_order == \"asc\":\n        sorted_col_names = sorted(df.columns)\n    elif sort_order == \"desc\":\n        sorted_col_names = sorted(df.columns, reverse=True)\n    else:\n        raise ValueError(\"['asc', 'desc'] are the only valid sort orders and you entered a sort order of '{sort_order}'\".format(\n            sort_order=sort_order\n        ))\n    return df.select(*sorted_col_names)\n</code></pre> <p>Write a test that creates a DataFrame, reorders the columns with the <code>sort_columns</code> method, and confirms that the expected column order is the same as what's actually returned by the function. This test will compare the equality of two entire DataFrames. It'll be different than the previous test that compared the equality of two columns in a single DataFrame.</p> <p>Here's the test that'll be added to the <code>tests/test_transformations.py</code> file.</p> <pre><code>from pysparktestingexample.transformations import sort_columns\nfrom chispa.dataframe_comparer import assert_df_equality\nimport pyspark.sql.functions as F\n\ndef test_sort_columns_asc(spark):\n    source_data = [\n        (\"jose\", \"oak\", \"switch\"),\n        (\"li\", \"redwood\", \"xbox\"),\n        (\"luisa\", \"maple\", \"ps4\"),\n    ]\n    source_df = spark.createDataFrame(source_data, [\"name\", \"tree\", \"gaming_system\"])\n\n    actual_df = sort_columns(source_df, \"asc\")\n\n    expected_data = [\n        (\"switch\", \"jose\", \"oak\"),\n        (\"xbox\", \"li\", \"redwood\"),\n        (\"ps4\", \"luisa\", \"maple\"),\n    ]\n    expected_df = spark.createDataFrame(expected_data, [\"gaming_system\", \"name\", \"tree\"])\n\n    assert_df_equality(actual_df, expected_df)\n</code></pre> <p>This test is run with the <code>assert_df_equality</code> function defined in <code>chispa.dataframe_comparer</code>. The <code>assert_column_equality</code> method isn't appropriate for this test because we're comparing the order of multiple columns and the schema matters. Use the <code>assert_column_equality</code> method whenever possible and only revert to <code>assert_df_equality</code> when necessary.</p> <p>Let's write another test to verify that the <code>sort_columns</code> method can also rearrange the columns in descending order.</p> <pre><code>def test_sort_columns_desc(spark):\n    source_data = [\n        (\"jose\", \"oak\", \"switch\"),\n        (\"li\", \"redwood\", \"xbox\"),\n        (\"luisa\", \"maple\", \"ps4\"),\n    ]\n    source_df = spark.createDataFrame(source_data, [\"name\", \"tree\", \"gaming_system\"])\n\n    actual_df = sort_columns(source_df, \"desc\")\n\n    expected_data = [\n        (\"oak\", \"jose\", \"switch\"),\n        (\"redwood\", \"li\", \"xbox\"),\n        (\"maple\", \"luisa\", \"ps4\"),\n    ]\n    expected_df = spark.createDataFrame(expected_data, [\"tree\", \"name\", \"gaming_system\"])\n\n    assert_df_equality(actual_df, expected_df)\n</code></pre> <p>In a real codebase, you'd also want to write a third test to verify that <code>sort_columns</code> throws an error when the second argument is an invalid value (<code>asc</code> and <code>desc</code> are the only valid values for the second argument). We'll skip that test for now, but it's important for your test suite to verify your code throws descriptive error messages.</p>"},{"location":"pyspark/testing-pytest-chispa/#failing-dataframe-equality-test","title":"Failing DataFrame equality test","text":"<p>Let's write a DataFrame comparison test that'll return an error.</p> <p>Create a <code>modify_column_names</code> function in the <code>transformations.py</code> file that'll update all the column names in a DataFrame.</p> <pre><code>def modify_column_names(df, fun):\n    for col_name in df.columns:\n        df = df.withColumnRenamed(col_name, fun(col_name))\n    return df\n</code></pre> <p>Now create a <code>string_helpers.py</code> file with a <code>dots_to_underscores</code> method that converts the dots in a string to underscores.</p> <pre><code>def dots_to_underscores(s):\n    return s.replace(\".\", \"_\", 1)\n</code></pre> <p>Write a test to verify that <code>modify_column_names</code> converts all the dots are converted to underscores.</p> <pre><code>import pysparktestingexample.transformations as T\nimport pysparktestingexample.string_helpers as SH\n\ndef test_modify_column_names_error(spark):\n    source_data = [\n        (\"jose\", 8),\n        (\"li\", 23),\n        (\"luisa\", 48),\n    ]\n    source_df = spark.createDataFrame(source_data, [\"first.name\", \"person.favorite.number\"])\n\n    actual_df = T.modify_column_names(source_df, SH.dots_to_underscores)\n\n    expected_data = [\n        (\"jose\", 8),\n        (\"li\", 23),\n        (\"luisa\", 48),\n    ]\n    expected_df = spark.createDataFrame(expected_data, [\"first_name\", \"person_favorite_number\"])\n\n    assert_df_equality(actual_df, expected_df)\n</code></pre> <p>This'll return a nicely formatted error message:</p> <p></p> <p>Our code has a bug. The <code>person.favorite.number</code> column is converted to <code>person_favorite.number</code> and we want it to be converted to <code>person_favorite_number</code>. Your test suite will help you avoid releasing buggy code like this in production ;)</p>"},{"location":"pyspark/testing-pytest-chispa/#approximate-column-equality","title":"Approximate column equality","text":"<p>We can check if columns are approximately equal, which is especially useful for floating number comparisons.</p> <p>Create a <code>divide_by_three</code> function in <code>functions.py</code> that divides a number by three.</p> <pre><code>def divide_by_three(col):\n    return col / 3\n</code></pre> <p>Here's a test that uses the <code>assert_approx_column_equality</code> function to compare the equality of two floating point columns.</p> <pre><code>def test_divide_by_three(spark):\n    data = [\n        (1, 0.33),\n        (2, 0.66),\n        (3, 1.0),\n        (None, None)\n    ]\n    df = spark.createDataFrame(data, [\"num\", \"expected\"])\\\n        .withColumn(\"num_divided_by_three\", divide_by_three(F.col(\"num\")))\n    assert_approx_column_equality(df, \"num_divided_by_three\", \"expected\", 0.01)\n</code></pre> <p>The precision is set to 0.01 in this example. 0.33333333 and 0.33 are considered approximately equal because the absolute value of the difference between the two numbers is less than the specified precision.</p> <p>Let's add another test that's failing and inspect the error message.</p> <p>Here's a test that'll error out because of a row that's not approximately equal.</p> <pre><code>def test_divide_by_three_error(spark):\n    data = [\n        (5, 1.66),\n        (6, 2.0),\n        (7, 4.33),\n        (None, None)\n    ]\n    df = spark.createDataFrame(data, [\"num\", \"expected\"])\\\n        .withColumn(\"num_divided_by_three\", divide_by_three(F.col(\"num\")))\n    assert_approx_column_equality(df, \"num_divided_by_three\", \"expected\", 0.01)\n</code></pre> <p></p> <p>The error message makes it clear that for one row of data, we're expecting <code>num_divided_by_three</code> to equal 4.33, but it's actually 2.3333333333333335. Those numbers aren't approximately equal when the precision factor is 0.01.</p>"},{"location":"pyspark/testing-pytest-chispa/#approximate-dataframe-equality","title":"Approximate DataFrame equality","text":"<p>Let's create two DataFrames and confirm they're approximately equal.</p> <pre><code>def test_approx_df_equality_same():\n    data1 = [\n        (1.1, \"a\"),\n        (2.2, \"b\"),\n        (3.3, \"c\"),\n        (None, None)\n    ]\n    df1 = spark.createDataFrame(data1, [\"num\", \"letter\"])\n\n    data2 = [\n        (1.05, \"a\"),\n        (2.13, \"b\"),\n        (3.3, \"c\"),\n        (None, None)\n    ]\n    df2 = spark.createDataFrame(data2, [\"num\", \"letter\"])\n\n    assert_approx_df_equality(df1, df2, 0.1)\n</code></pre> <p>The <code>assert_approx_df_equality</code> method is smart and will only perform approximate equality operations for floating point numbers in DataFrames. It'll perform regular equality for strings and other types.</p> <p>Let's perform an approximate equality comparison for two DataFrames that are not equal.</p> <pre><code>def test_approx_df_equality_different():\n    data1 = [\n        (1.1, \"a\"),\n        (2.2, \"b\"),\n        (3.3, \"c\"),\n        (None, None)\n    ]\n    df1 = spark.createDataFrame(data1, [\"num\", \"letter\"])\n\n    data2 = [\n        (1.1, \"a\"),\n        (5.0, \"b\"),\n        (3.3, \"z\"),\n        (None, None)\n    ]\n    df2 = spark.createDataFrame(data2, [\"num\", \"letter\"])\n\n    assert_approx_df_equality(df1, df2, 0.1)\n</code></pre> <p>Here's the pretty error message that's outputted:</p> <p></p>"},{"location":"pyspark/testing-pytest-chispa/#schema-mismatch-messages","title":"Schema mismatch messages","text":"<p>DataFrame equality messages perform schema comparisons before analyzing the actual content of the DataFrames. DataFrames that don't have the same schemas should error out as fast as possible.</p> <p>Let's compare a DataFrame that has a string column an integer column with a DataFrame that has two integer columns to observe the schema mismatch message.</p> <pre><code>def test_schema_mismatch_message():\n    data1 = [\n        (1, \"a\"),\n        (2, \"b\"),\n        (3, \"c\"),\n        (None, None)\n    ]\n    df1 = spark.createDataFrame(data1, [\"num\", \"letter\"])\n\n    data2 = [\n        (1, 6),\n        (2, 7),\n        (3, 8),\n        (None, None)\n    ]\n    df2 = spark.createDataFrame(data2, [\"num\", \"num2\"])\n\n    assert_df_equality(df1, df2)\n</code></pre> <p>Here's the error message:</p> <p></p>"},{"location":"pyspark/testing-pytest-chispa/#benefits-of-testing","title":"Benefits of testing","text":"<p>A well written test suite makes your code easier to refactor. You be assured that newly added features don't break existing logic. Data tends to be messy and there are often lots of edge cases. Your test suite will make sure all the different types of dirty data are handled properly.</p> <p>The test suite also documents code functionality. Suppose you're working with a column that has 5 different types of dirty data that's cleaned by a function. You may have a function that cleans this field. Your test suite will provide representative examples of the different types of dirty data and how they're standardized. Someone new to the project can read the tests and understand the different types of dirty data that needs to be accounted for without even querying the data in production.</p> <p>Documenting dirty data attributes in a wiki or in comments is dangerous because the code can change and developers might forget to update the documentation. A test suite serves as \"living code documentation\". Developers should always keep the test suite passing. Whenever you push code to the master branch, you should have a continuous integration server that runs the test suite. If a test fails, fixing it should be a top priority. A test suite, living code documentaion, should never get outdated like traditional documentation.</p> <p>Functions with side effects or that perform multiple operations are hard to test. It's easier to break up code into single purpose modular functions, so they're easier to test. Testing encourages developers to write higher quality code.</p>"},{"location":"pyspark/testing-pytest-chispa/#next-steps","title":"Next steps","text":"<p>If you can, transition your project to Poetry, as described in this blog post. It'll make it easier to add development dependencies like pytest and chispa to your project.</p> <p>You'll want to leverage dependency injection and mocking to build a great test suite. You'll also want to wire up your project with continuous integration and continuous deployment.</p> <p>You'll find your code a lot easier to reason with when it's nice and tested ;)</p>"},{"location":"pyspark/udf-dict-broadcast/","title":"PySpark UDFs with Dictionary Arguments","text":"<p>Passing a dictionary argument to a PySpark UDF is a powerful programming technique that'll enable you to implement some complicated algorithms that scale.</p> <p>Broadcasting values and writing UDFs can be tricky. UDFs only accept arguments that are column objects and dictionaries aren't column objects. This blog post shows you the nested function work-around that's necessary for passing a dictionary to a UDF. It'll also show you how to broadcast a dictionary and why broadcasting is important in a cluster environment.</p> <p>Several approaches that do not work and the accompanying error messages are also presented, so you can learn more about how Spark works.</p>"},{"location":"pyspark/udf-dict-broadcast/#you-cant-pass-a-dictionary-as-a-udf-argument","title":"You can't pass a dictionary as a UDF argument","text":"<p>Lets create a <code>state_abbreviation</code> UDF that takes a string and a dictionary mapping as arguments:</p> <pre><code>@F.udf(returnType=StringType())\ndef state_abbreviation(s, mapping):\n    if s is not None:\n        return mapping[s]\n</code></pre> <p>Create a sample DataFrame, attempt to run the <code>state_abbreviation</code> UDF and confirm that the code errors out because UDFs can't take dictionary arguments.</p> <pre><code>import pyspark.sql.functions as F\n\ndf = spark.createDataFrame([\n    ['Alabama',],\n    ['Texas',],\n    ['Antioquia',]\n]).toDF('state')\n\nmapping = {'Alabama': 'AL', 'Texas': 'TX'}\n\ndf.withColumn('state_abbreviation', state_abbreviation(F.col('state'), mapping)).show()\n</code></pre> <p>Here's the error message: <code>TypeError: Invalid argument, not a string or column: {'Alabama': 'AL', 'Texas': 'TX'} of type &lt;class 'dict'&gt;. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.</code>.</p> <p>The <code>create_map</code> function sounds like a promising solution in our case, but that function doesn't help.</p> <p>Let's see if the <code>lit</code> function can help.</p> <pre><code>df.withColumn('state_abbreviation', state_abbreviation(F.col('state'), lit(mapping))).show()\n</code></pre> <p>This doesn't work either and errors out with this message: <code>py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.functions.lit: java.lang.RuntimeException: Unsupported literal type class java.util.HashMap {Texas=TX, Alabama=AL}</code>.</p> <p>The <code>lit()</code> function doesn't work with dictionaries.</p> <p>Let's try broadcasting the dictionary with the <code>pyspark.sql.functions.broadcast()</code> method and see if that helps.</p> <pre><code>df.withColumn('state_abbreviation', state_abbreviation(F.col('state'), F.broadcast(mapping))).show()\n</code></pre> <p>Broadcasting in this manner doesn't help and yields this error message: <code>AttributeError: 'dict' object has no attribute '_jdf'</code>.</p> <p>Broadcasting with <code>spark.sparkContext.broadcast()</code> will also error out. You need to approach the problem differently.</p>"},{"location":"pyspark/udf-dict-broadcast/#simple-solution","title":"Simple solution","text":"<p>Create a <code>working_fun</code> UDF that uses a nested function to avoid passing the dictionary as an argument to the UDF.</p> <pre><code>def working_fun(mapping):\n    def f(x):\n        return mapping.get(x)\n    return F.udf(f)\n</code></pre> <p>Create a sample DataFrame, run the <code>working_fun</code> UDF, and verify the output is accurate.</p> <pre><code>df = spark.createDataFrame([\n    ['Alabama',],\n    ['Texas',],\n    ['Antioquia',]\n]).toDF('state')\n\nmapping = {'Alabama': 'AL', 'Texas': 'TX'}\n\ndf.withColumn('state_abbreviation', working_fun(mapping)(F.col('state'))).show()\n</code></pre> <pre><code>+---------+------------------+\n|    state|state_abbreviation|\n+---------+------------------+\n|  Alabama|                AL|\n|    Texas|                TX|\n|Antioquia|              null|\n+---------+------------------+\n</code></pre> <p>This approach works if the dictionary is defined in the codebase (if the dictionary is defined in a Python project that's packaged in a wheel file and attached to a cluster for example). This code will not work in a cluster environment if the dictionary hasn't been spread to all the nodes in the cluster. It's better to explicitly broadcast the dictionary to make sure it'll work when run on a cluster.</p>"},{"location":"pyspark/udf-dict-broadcast/#broadcast-solution","title":"Broadcast solution","text":"<p>Let's refactor <code>working_fun</code> by broadcasting the dictionary to all the nodes in the cluster.</p> <pre><code>def working_fun(mapping_broadcasted):\n    def f(x):\n        return mapping_broadcasted.value.get(x)\n    return F.udf(f)\n\ndf = spark.createDataFrame([\n    ['Alabama',],\n    ['Texas',],\n    ['Antioquia',]\n]).toDF('state')\n\nmapping = {'Alabama': 'AL', 'Texas': 'TX'}\nb = spark.sparkContext.broadcast(mapping)\n\ndf.withColumn('state_abbreviation', working_fun(b)(F.col('state'))).show()\n</code></pre> <pre><code>+---------+------------------+\n|    state|state_abbreviation|\n+---------+------------------+\n|  Alabama|                AL|\n|    Texas|                TX|\n|Antioquia|              null|\n+---------+------------------+\n</code></pre> <p>Take note that you need to use <code>value</code> to access the dictionary in <code>mapping_broadcasted.value.get(x)</code>. If you try to run <code>mapping_broadcasted.get(x)</code>, you'll get this error message: <code>AttributeError: 'Broadcast' object has no attribute 'get'</code>. You'll see that error message whenever your trying to access a variable that's been broadcasted and forget to call <code>value</code>.</p> <p>Explicitly broadcasting is the best and most reliable way to approach this problem. The dictionary should be explicitly broadcasted, even if it is defined in your code.</p>"},{"location":"pyspark/udf-dict-broadcast/#creating-dictionaries-to-be-broadcasted","title":"Creating dictionaries to be broadcasted","text":"<p>You'll typically read a dataset from a file, convert it to a dictionary, broadcast the dictionary, and then access the broadcasted variable in your code.</p> <p>Here's an example code snippet that reads data from a file, converts it to a dictionary, and creates a broadcast variable.</p> <pre><code>df = spark\\\n    .read\\\n    .option('header', True)\\\n    .csv(word_prob_path)\nword_prob = {x['word']: x['word_prob'] for x in df.select('word', 'word_prob').collect()}\nword_prob_b = spark.sparkContext.broadcast(word_prob)\n</code></pre> <p>The quinn library makes this even easier.</p> <pre><code>import quinn\n\nword_prob = quinn.two_columns_to_dictionary(df, 'word', 'word_prob')\nword_prob_b = spark.sparkContext.broadcast(word_prob)\n</code></pre>"},{"location":"pyspark/udf-dict-broadcast/#broadcast-limitations","title":"Broadcast limitations","text":"<p>The broadcast size limit was 2GB and was increased to 8GB as of Spark 2.4, see here. Big dictionaries can be broadcasted, but you'll need to investigate alternate solutions if that dataset you need to broadcast is truly massive.</p>"},{"location":"pyspark/udf-dict-broadcast/#example-application","title":"Example application","text":"<p>wordninja is a good example of an application that can be easily ported to PySpark with the design pattern outlined in this blog post.</p> <p>The code depends on an list of 126,000 words defined in this file. The words need to be converted into a dictionary with a key that corresponds to the work and a probability value for the model.</p> <p>126,000 words sounds like a lot, but it's well below the Spark broadcast limits. You can broadcast a dictionary with millions of key/value pairs.</p> <p>You can use the design patterns outlined in this blog to run the wordninja algorithm on billions of strings. It's amazing how PySpark lets you scale algorithms!</p>"},{"location":"pyspark/udf-dict-broadcast/#conclusion","title":"Conclusion","text":"<p>Broadcasting dictionaries is a powerful design pattern and oftentimes the key link when porting Python algorithms to PySpark so they can be run at a massive scale.</p> <p>Your UDF should be packaged in a library that follows dependency management best practices and tested in your test suite. Spark code is complex and following software engineering best practices is essential to build code that's readable and easy to maintain.</p>"},{"location":"pyspark/union-unionbyname-merge-dataframes/","title":"Combining PySpark DataFrames with union and unionByName","text":"<p>Multiple PySpark DataFrames can be combined into a single DataFrame with <code>union</code> and <code>unionByName</code>.</p> <p><code>union</code> works when the columns of both DataFrames being joined are in the same order. It can give surprisingly wrong results when the schemas aren't the same, so watch out!</p> <p><code>unionByName</code> works when both DataFrames have the same columns, but in a different order. An optional parameter was also added in Spark 3.1 to allow unioning slightly different schemas.</p> <p>This post explains how to use both methods and gives details on how the operations function under the hood.</p>"},{"location":"pyspark/union-unionbyname-merge-dataframes/#union","title":"union","text":"<p>Suppose you have the following <code>americans</code> DataFrame:</p> <pre><code>+----------+---+\n|first_name|age|\n+----------+---+\n|       bob| 42|\n|      lisa| 59|\n+----------+---+\n</code></pre> <p>And the following <code>colombians</code> DataFrame:</p> <pre><code>+----------+---+\n|first_name|age|\n+----------+---+\n|     maria| 20|\n|    camilo| 31|\n+----------+---+\n</code></pre> <p>Here's how to union the two DataFrames.</p> <pre><code>res = americans.union(colombians)\nres.show()\n</code></pre> <pre><code>+----------+---+\n|first_name|age|\n+----------+---+\n|       bob| 42|\n|      lisa| 59|\n|     maria| 20|\n|    camilo| 31|\n+----------+---+\n</code></pre> <p>Here's the full code snippet in case you'd like to run this code on your local machine.</p> <pre><code>americans = spark.createDataFrame(\n    [(\"bob\", 42), (\"lisa\", 59)], [\"first_name\", \"age\"]\n)\ncolombians = spark.createDataFrame(\n    [(\"maria\", 20), (\"camilo\", 31)], [\"first_name\", \"age\"]\n)\nres = americans.union(colombians)\nres.show()\n</code></pre> <p>Suppose you have a <code>brasilians</code> DataFrame with <code>age</code> and <code>first_name</code> columns - the same columns as before but in reverse order.</p> <pre><code>+---+----------+\n|age|first_name|\n+---+----------+\n| 33|     tiago|\n| 36|     lilly|\n+---+----------+\n</code></pre> <p>If we union <code>americans</code> and <code>brasilians</code> with <code>americans.union(brasilans)</code>, we will get an incorrect result.</p> <pre><code>+----------+-----+\n|first_name|  age|\n+----------+-----+\n|       bob|   42|\n|      lisa|   59|\n|        33|tiago|\n|        36|lilly|\n+----------+-----+\n</code></pre> <p>Oh my, this is really bad.</p> <p>Here's <code>americans.printSchema()</code>:</p> <pre><code>root\n |-- first_name: string (nullable = true)\n |-- age: long (nullable = true)\n</code></pre> <p>Here's <code>brasilians.printSchema()</code>:</p> <pre><code>root\n |-- age: long (nullable = true)\n |-- first_name: string (nullable = true)\n</code></pre> <p>PySpark is unioning different types - that's definitely not what you want.</p> <p>Let's look at a solution that gives the correct result when the columns are in a different order.</p>"},{"location":"pyspark/union-unionbyname-merge-dataframes/#unionbyname","title":"unionByName","text":"<p><code>unionByName</code> joins by column names, not by the order of the columns, so it can properly combine two DataFrames with columns in different orders.</p> <p>Let's try combining <code>americans</code> and <code>brasilians</code> with <code>unionByName</code>.</p> <pre><code>res = americans.unionByName(brasilans)\nres.show()\n</code></pre> <pre><code>+----------+---+\n|first_name|age|\n+----------+---+\n|       bob| 42|\n|      lisa| 59|\n|     tiago| 33|\n|     lilly| 36|\n+----------+---+\n</code></pre> <p><code>unionByName</code> gives a correct result here, unlike the wrong answer we got with <code>union</code>.</p> <p>Let's create an <code>indians</code> DataFrame with <code>age</code>, <code>first_name</code>, and <code>hobby</code> columns:</p> <pre><code>indians = spark.createDataFrame(\n    [(55, \"arjun\", \"cricket\"), (5, \"ira\", \"playing\")], [\"age\", \"first_name\", \"hobby\"]\n)\nindians.show()\n</code></pre> <pre><code>+---+----------+-------+\n|age|first_name|  hobby|\n+---+----------+-------+\n| 55|     arjun|cricket|\n|  5|       ira|playing|\n+---+----------+-------+\n</code></pre> <p>Now union <code>americans</code> with <code>indians</code>:</p> <pre><code>res = americans.unionByName(indians)\nres.show()\n</code></pre> <p>This'll error out with the following message.</p> <pre><code>def deco(*a, **kw):\n    try:\n        return f(*a, **kw)\n    except py4j.protocol.Py4JJavaError as e:\n        converted = convert_exception(e.java_exception)\n        if not isinstance(converted, UnknownException):\n            # Hide where the exception came from that shows a non-Pythonic\n            # JVM exception message.\n           raise converted from None\n           pyspark.sql.utils.AnalysisException: Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns;\n           'Union false, false\n           :- LogicalRDD [first_name#219, age#220L], false\n           +- Project [first_name#224, age#223L, hobby#225]\n              +- LogicalRDD [age#223L, first_name#224, hobby#225], false\n</code></pre> <p>In PySpark 3.1.0, an optional <code>allowMissingColumns</code> argument was added, which allows DataFrames with different schemas to be unioned.</p> <pre><code>americans.unionByName(indians, allowMissingColumns = True).show()\n</code></pre> <pre><code>+----------+---+-------+\n|first_name|age|  hobby|\n+----------+---+-------+\n|       bob| 42|   null|\n|      lisa| 59|   null|\n|     arjun| 55|cricket|\n|       ira|  5|playing|\n+----------+---+-------+\n</code></pre> <p>Here's the ugly code you needed to write before <code>allowMissingColumns</code> was added:</p> <pre><code>for c in missing_col_names: americans = americans.withColumn(c, lit(None))\nres = americans.union(indians.select(\"first_name\", \"age\", \"hobby\"))\nres.show()\n</code></pre> <pre><code>+----------+---+-------+\n|first_name|age|  hobby|\n+----------+---+-------+\n|       bob| 42|   null|\n|      lisa| 59|   null|\n|     arjun| 55|cricket|\n|       ira|  5|playing|\n+----------+---+-------+\n</code></pre> <p>The <code>allowMissingColumns</code> option lets you focus on your application logic instead of getting caught up with the Python code.</p> <p>Thanks to Vegetable_Hamster732 for sharing this solution.</p>"},{"location":"pyspark/union-unionbyname-merge-dataframes/#unionall","title":"unionAll","text":"<p><code>unionAll</code> is an alias for <code>union</code> and should be avoided.</p> <p><code>unionAll</code> was used in older versions of PySpark and now <code>union</code> is preferred.</p>"},{"location":"pyspark/union-unionbyname-merge-dataframes/#conclusion","title":"Conclusion","text":"<p><code>union</code> is generally sufficient, but use <code>unionByName</code> if you want to be safe and avoid the weird bug we pointed out in this post.</p> <p>The <code>allowMissingColumns</code> argument makes <code>unionByName</code> easier to use when the schemas don't line up exactly. In earlier versions of PySpark, it was annoying to manually add <code>null</code> columns before running <code>union</code> to account for DataFrames with slightly different schemas.</p> <p>The PySpark maintainers are doing a great job incrementally improving the API to make it more developer friendly. The changes are backwards compatible, so we get new features without breaking changes. Great job Spark core team!</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/","title":"Building DAGs / Directed Acyclic Graphs with Python","text":"<p>Directed Acyclic Graphs (DAGs) are a critical data structure for data science / data engineering workflows. DAGs are used extensively by popular projects like Apache Airflow and Apache Spark.</p> <p>This blog post will teach you how to build a DAG in Python with the networkx library and run important graph algorithms.</p> <p>Once you're comfortable with DAGs and see how easy they are to work with, you'll find all sorts of analyses that are good candidates for DAGs. DAGs are just as important as data structures like dictionaries and lists for a lot of analyses.</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#simple-example","title":"Simple example","text":"<p>Consider the following DAG:</p> <p></p> <p>root, a, b, c, d, and e are referred to as nodes. The arrows that connect the nodes are called edges. A graph is a collection of nodes that are connected by edges. A directed acyclic graph is a special type of graph with properties that'll be explained in this post.</p> <p>Here's how we can construct our sample graph with the networkx library.</p> <pre><code>import networkx as nx\n\ngraph = nx.DiGraph()\ngraph.add_edges_from([(\"root\", \"a\"), (\"a\", \"b\"), (\"a\", \"e\"), (\"b\", \"c\"), (\"b\", \"d\"), (\"d\", \"e\")])\n</code></pre> <p><code>DiGraph</code> is short for \"directed graph\".</p> <p>The directed graph is modeled as a list of tuples that connect the nodes. Remember that these connections are referred to as \"edges\" in graph nomenclature. Take another look at the graph image and observe how all the arguments to <code>add_edges_from</code> match up with the arrows in the graph.</p> <p>networkx is smart enough to infer the nodes from a collection of edges.</p> <pre><code>graph.nodes() # =&gt; NodeView(('root', 'a', 'b', 'e', 'c', 'd'))\n</code></pre> <p>Algorithms let you perform powerful analyses on graphs. This blog post focuses on how to use the built-in networkx algorithms.</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#shortest-path","title":"Shortest path","text":"<p>The shortest path between two nodes in a graph is the quickest way to travel from the start node to the end node.</p> <p>Let's use the shortest path algorithm to calculate the quickest way to get from root to e.</p> <pre><code>nx.shortest_path(graph, 'root', 'e') # =&gt; ['root', 'a', 'e']\n</code></pre> <p>You could also go from root =&gt; a =&gt; b =&gt; d =&gt; e to get from root to e, but that'd be longer.</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#longest-path","title":"Longest path","text":"<p>The <code>dag_longest_path</code> method returns the longest path in a DAG.</p> <pre><code>nx.dag_longest_path(graph) # =&gt; ['root', 'a', 'b', 'd', 'e']\n</code></pre>"},{"location":"python/dag-directed-acyclic-graph-networkx/#topological-sorting","title":"Topological sorting","text":"<p>Nodes in a DAG can be topologically sorted such that for every directed edge uv from node u to node v, u comes before v in the ordering.</p> <p>Our graph has nodes (a, b, c, etc.) and directed edges (ab, bc, bd, de, etc.). Here's a couple of requirements that our topological sort need to satisfy:</p> <ul> <li>for ab, a needs to come before b in the ordering</li> <li>for bc, b needs to come before c</li> <li>for bd, b needs to come before d</li> <li>for de, d needs to come before e</li> </ul> <p>Let's run the algorithm and see if all our requirements are met:</p> <pre><code>list(nx.topological_sort(graph)) # =&gt; ['root', 'a', 'b', 'd', 'e', 'c']\n</code></pre> <p>Observe that a comes before b, b comes before c, b comes before d, and d comes before e. The topological sort meets all the ordering requirements.</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#checking-validity","title":"Checking validity","text":"<p>We can check to make sure the graph is directed.</p> <pre><code>nx.is_directed(graph) # =&gt; True\n</code></pre> <p>We can also make sure it's a directed acyclic graph.</p> <pre><code>nx.is_directed_acyclic_graph(graph) # =&gt; True\n</code></pre>"},{"location":"python/dag-directed-acyclic-graph-networkx/#directed-graph-thats-not-acyclic","title":"Directed graph that's not acyclic","text":"<p>Let's make a graph that's directed, but not acyclic. A \"not acyclic graph\" is more commonly referred to as a \"cyclic graph\".</p> <p></p> <pre><code>graph = nx.DiGraph()\ngraph.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 1)])\nnx.is_directed(graph) # =&gt; True\nnx.is_directed_acyclic_graph(graph) # =&gt; False\n</code></pre> <p>An acyclic graph is when a node can't reach itself. This graph isn't acyclic because nodes can reach themselves (for example 3 can take this trip 3 =&gt; 4 =&gt; 1 =&gt; 2 =&gt; 3 and arrive back at itself.</p> <p>Directed graphs that aren't acyclic can't be topologically sorted.</p> <pre><code>list(nx.topological_sort(graph)) # throws this error - networkx.exception.NetworkXUnfeasible: Graph contains a cycle or graph changed during iteration\n</code></pre> <p>Let's revisit the topological sorting requirements and examine why cyclic directed graphs can't be topologically sorted. Our graph has nodes 1, 2, 3, 4 and directed edges 12, 23, 34, and 41. Here are the requirements for topological sorting:</p> <ul> <li>for 12, 1 needs to come before 2 in the ordering</li> <li>for 23, 2 needs to come before 3</li> <li>for 34, 3 needs to come before 4</li> <li>for 41, 4 needs to come before 1</li> </ul> <p>The first three requirements are easy to meet and can be satisfied with a 1, 2, 3 sorting. But the final requirement is impossible to meet. 4 needs to be before 1, but 4, 1, 2, 3 isn't possible because 3 needs to come before 4.</p> <p>Topologically sorting cyclic graphs is impossible.</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#graph-thats-neither-directed-nor-acyclic","title":"Graph that's neither directed nor acyclic","text":"<p>We've been using the <code>DiGraph</code> class to make graphs that are directed thus far. You can use the <code>Graph</code> class to make undirected graphs. All the edges in an undirected graph are bidirectional, so arrows aren't needed in visual representations of undirected graphs.</p> <p></p> <pre><code>graph = nx.Graph()\ngraph.add_edges_from([('x', 'y'), ('y', 'z')])\nnx.is_directed(graph) # =&gt; False\nnx.is_directed_acyclic_graph(graph) # =&gt; False\n</code></pre> <p>You need to use different algorithms when interacting with bidirectional graphs. Stick with DAGs while you're getting started ;)</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#multiple-roots","title":"Multiple roots","text":"<p>A DAG can have multiple root nodes.</p> <p></p> <pre><code>graph = nx.DiGraph()\ngraph.add_edges_from([('m', 'p'), ('n', 'p'), ('o', 'p'), ('p', 'q')])\nnx.is_directed(graph) # =&gt; True\nnx.is_directed_acyclic_graph(graph) # =&gt; True\nlist(nx.topological_sort(graph)) # =&gt; ['o', 'n', 'm', 'p', 'q']\n</code></pre> <p>A directed graph can have multiple valid topological sorts. m, n, o, p, q is another way to topologically sort this graph.</p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#graphing-a-dag","title":"Graphing a DAG","text":"<p>It's easy to visualized networkx graphs with matplotlib.</p> <p>Here's how we can visualize the first DAG from this blog post:</p> <pre><code>from matplotlib import pyplot as plt\n\ng1 = nx.DiGraph()\ng1.add_edges_from([(\"root\", \"a\"), (\"a\", \"b\"), (\"a\", \"e\"), (\"b\", \"c\"), (\"b\", \"d\"), (\"d\", \"e\")])\nplt.tight_layout()\nnx.draw_networkx(g1, arrows=True)\nplt.savefig(\"g1.png\", format=\"PNG\")\n# tell matplotlib you're done with the plot: https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot\nplt.clf()\n</code></pre> <p></p> <p>Here's how to visualize our directed, cyclic graph.</p> <pre><code>g2 = nx.DiGraph()\ng2.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 1)])\nplt.tight_layout()\nnx.draw_networkx(g2, arrows=True)\nplt.savefig(\"g2.png\", format=\"PNG\")\nplt.clf()\n</code></pre> <p></p>"},{"location":"python/dag-directed-acyclic-graph-networkx/#next-steps","title":"Next steps","text":"<p>Now that you're familiar with DAGs and can see how easy they are to create and manage with networkx, you can easily start incorporating this data structure in your projects.</p> <p>I recently created a project called unicron that models PySpark transformations in a DAG, to give users an elegant interface for running order dependent functions. You'll be able to make nice abstractions like these when you're comfortable with the DAG data structure.</p> <p>Check out this blog post on setting up a PySpark project with Poetry if you're interested in learning how to process massive datasets with PySpark and use networkx algorithms at scale.</p>"},{"location":"python/how-pyenv-works-shims/","title":"Deep dive into how pyenv actually works by leveraging the shim design pattern","text":"<p>pyenv lets you manage multiple versions of Python on your computer.</p> <p>This blog post focuses on how pyenv uses the shim design pattern to provide a wonderful user experience (it doesn't focus on installing and using the software like other posts).</p> <p>Once pyenv is installed you can easily switch between one project that uses Python 3.6, another project that uses Python 3.7, and a system-wide default Python, without any additional thought.</p> <p>Managing Python versions and packages can be a huge pain. It's important to understand how pyenv works to debug issues. Complexities are layered on top of different Python versions\u2026 good luck understanding virtual environments if you don't understand Python versioning.</p> <p>pyenv is a fork of rbenv, a project to manage Ruby versions. The \"shim design philosophy\" used by pyenv and rbenv has proven to be a winner for maintaining multiple versions of a programming language on a given machine. Learning the shim design philosophy will make you a better programmer and teach you a powerful design pattern you can use with your own programs.</p>"},{"location":"python/how-pyenv-works-shims/#pyenv-installation","title":"pyenv installation","text":"<p>You can install pyenv with Homebrew using the <code>brew install pyenv</code> command.</p> <p>Homebrew runs this formula. We'll cover how Homebrew works in another post.</p> <p>Type <code>which pyenv</code> to see that there is a pyenv executable in the <code>/usr/local/bin</code> directory.</p> <p>Enter <code>open /usr/local/bin</code> to view the <code>pyenv</code> executable in the directory.</p> <p></p> <p>The <code>pyenv</code> executable is run whenever we run commands like <code>pyenv versions</code> or <code>pyenv install --list</code>. Simply run <code>pyenv</code> without any arguments to see a listing of all the pyenv commands that can be run.</p> <p>Let's dig into where pyenv installs Python code in the filesystem.</p>"},{"location":"python/how-pyenv-works-shims/#python-installations","title":"Python installations","text":"<p>Run <code>pyenv versions</code> to see the Python versions you currently have installed on your machine.</p> <pre><code>$ pyenv versions\n  system\n* 3.7.5 (set by /Users/matthewpowers/.pyenv/version)\n</code></pre> <p>Run <code>pyenv install --list</code> to see the Python versions that can be installed.</p> <p>One of the Python versions that can be installed is <code>3.6.10</code> - let's install it with <code>pyenv install 3.6.10</code>. Here's the console output:</p> <pre><code>~ $ pyenv install 3.6.10\npython-build: use openssl@1.1 from homebrew\npython-build: use readline from homebrew\nDownloading Python-3.6.10.tar.xz...\n-&gt; https://www.python.org/ftp/python/3.6.10/Python-3.6.10.tar.xz\nInstalling Python-3.6.10...\npython-build: use readline from homebrew\npython-build: use zlib from xcode sdk\nInstalled Python-3.6.10 to /Users/matthewpowers/.pyenv/versions/3.6.10\n</code></pre> <p>Python 3.6.10 was downloaded to the <code>/Users/matthewpowers/.pyenv/versions/3.6.10</code> directory.</p>"},{"location":"python/how-pyenv-works-shims/#python-versions","title":"Python versions","text":"<p>Type <code>pyenv versions</code> to see all the Python versions available on your machine.</p> <pre><code>~ $ pyenv versions\n  system\n  3.6.10\n* 3.7.5 (set by /Users/matthewpowers/.pyenv/version)\n</code></pre> <p>My machine has the system Python version, 3.6.10, and 3.7.5.</p> <p>Python 3.7.5 is the \"current selected\" version of Python that pyenv will use (as indicated by the * next to 3.7.5 when <code>pyenv versions</code> is run).</p> <p>If you run <code>python</code> when <code>3.7.5</code> is the selected Python version, then a Python 3.7.5 shell will be started. Understanding how pyenv knows to launch a 3.7.5 shell when the <code>python</code> command is run is the central focus of this post.</p> <pre><code>~ $ python\nPython 3.7.5 (default, Apr  7 2020, 08:06:08)\n[Clang 10.0.1 (clang-1001.0.46.4)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n</code></pre> <p>The <code>/Users/matthewpowers/.pyenv/version</code> file only contains a single line of data.</p> <pre><code>~ $ cat /Users/matthewpowers/.pyenv/version\n3.7.5\n</code></pre>"},{"location":"python/how-pyenv-works-shims/#pyenv-global","title":"pyenv global","text":"<p>Let's view the current \"global\" python version with <code>pyenv global</code>:</p> <pre><code>~ $ pyenv global\n3.7.5\n</code></pre> <p>The global Python version is set in the <code>/Users/matthewpowers/.pyenv/version</code> file.</p> <p>Let's change the global version to <code>3.6.10</code> with <code>pyenv global 3.6.10</code>.</p> <p>We can run <code>pyenv versions</code> and see that pyenv is using 3.6.10 by default now.</p> <pre><code>~ $ pyenv versions\n  system\n* 3.6.10 (set by /Users/matthewpowers/.pyenv/version)\n  3.7.5\n</code></pre> <p>Let's inspect the contents of the <code>/Users/matthewpowers/.pyenv/version</code> file to decipher what the <code>pyenv global 3.6.10</code> command did under the hood.</p> <pre><code>~ $ cat /Users/matthewpowers/.pyenv/version\n3.6.10\n</code></pre> <p>Looks like <code>pyenv global 3.6.10</code> just clobbered the <code>/Users/matthewpowers/.pyenv/version</code> file with a new Python version number.</p> <p>Let's make a bad decision and clobber <code>/Users/matthewpowers/.pyenv/version</code> with <code>echo</code> and <code>&gt;</code> to see if we can change the global Python version ourselves (don't do this - we're just hacking to learn how this all works).</p> <pre><code>~ $ echo 3.7.5 &gt; /Users/matthewpowers/.pyenv/version\n~ $ pyenv global\n3.7.5\n</code></pre> <p>The pyenv global version is set in the <code>/Users/matthewpowers/.pyenv/version</code> file. This approach simple and intuitive. Let's see how we can modify Python versions for different projects.</p>"},{"location":"python/how-pyenv-works-shims/#pyenv-local","title":"pyenv local","text":"<p>Let's create a <code>~/Documents/project1/</code> folder and cd into the directory.</p> <pre><code>~ $ mkdir ~/Documents/project1\n~ $ cd ~/Documents/project1/\n</code></pre> <p>We can run <code>pyenv version</code> to see this project is using Python 3.7.5 (it's using the global Python version by default):</p> <pre><code>project1 $ pyenv version\n3.7.5 (set by /Users/matthewpowers/.pyenv/version)\n</code></pre> <p>Let's change the Python version to 3.6.10 for this project.</p> <pre><code>project1 $ pyenv local 3.6.10\nproject1 $ pyenv version\n3.6.10 (set by /Users/matthewpowers/Documents/project1/.python-version)\n</code></pre> <p>pyenv added a <code>.python-version</code> file to the project1 directory.</p> <pre><code>project1 $ ls -ahl\ntotal 8\ndrwxr-xr-x  3 matthewpowers  staff    96B Apr 11 10:59 .\ndrwx------+ 8 matthewpowers  staff   256B Apr 11 13:49 ..\n-rw-r--r-- 1 matthewpowers  staff     7B Apr 11 10:59 .python-version\n</code></pre> <p>Let's inspect the contents of <code>/Users/matthewpowers/Documents/project1/.python-version</code>:</p> <pre><code>project1 $ cat /Users/matthewpowers/Documents/project1/.python-version\n3.6.10\n</code></pre> <p>The <code>/Users/matthewpowers/.pyenv/version</code> file that sets the global Python version is unchanged.</p> <pre><code>project1 $ cat /Users/matthewpowers/.pyenv/version\n3.7.5\n</code></pre> <p>pyenv is somehow performing an analysis like \"yes, I can see you have a global Python version set, but the <code>python version</code> command is being run from the <code>project1</code> directory, and that has a <code>.python-version</code> file that takes precedence over the global Python version\".</p> <p>Let's create a <code>~/Documents/project2/</code> folder and set the local Python version to \"system\".</p> <pre><code>project1 $ mkdir ~/Documents/project2\nproject1 $ cd ~/Documents/project2\nproject2 $ pyenv local system\n</code></pre> <p>Let's verify that project2 is using the system Python version.</p> <pre><code>project2 $ pyenv version\nsystem (set by /Users/matthewpowers/Documents/project2/.python-version)\n</code></pre> <p>Let's run some commands to recap and demonstrate that the home directory is using a Python version of 3.7.5, project1 is using 3.6.10, and project2 is using the system Python.</p> <pre><code>$ cd ~\n~ $ pyenv version\n3.7.5 (set by /Users/matthewpowers/.pyenv/version)\n\n~ $ cd ~/Documents/project1/\nproject1 $ pyenv version\n3.6.10 (set by /Users/matthewpowers/Documents/project1/.python-version)\n\nproject1 $ cd ~/Documents/project2/\nproject2 $ pyenv version\nsystem (set by /Users/matthewpowers/Documents/project2/.python-version)\n</code></pre> <p>Let's keep digging and see how pyenv is switching these Python versions magically.</p>"},{"location":"python/how-pyenv-works-shims/#path-basics","title":"PATH basics","text":"<p>PATH is an environment variable that specifies an ordered list of folders where executables are saved.</p> <p>Run <code>echo $PATH</code> on your machine to see the path on your machine.</p> <pre><code>~ $ echo $PATH\n/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\n</code></pre> <p>The <code>PATH</code> is an ordered list of directories delimited by a colon. It's easier to visualize a bulleted list:</p> <ul> <li><code>/usr/local/bin</code></li> <li><code>/usr/bin</code></li> <li><code>/bin</code></li> <li><code>/usr/sbin</code></li> <li><code>/sbin</code></li> </ul> <p>When you enter a command in Terminal, your computer will look for an executable in <code>/usr/local/bin</code> first, then <code>/usr/bin</code>, then <code>/bin</code>, then <code>/bin/usr/sbin</code> and finally <code>/sbin</code>.</p> <p>When you run <code>echo hi</code>, your Terminal will start by looking for an executable named <code>echo</code> in the <code>/usr/local/bin</code> directory. There isn't a <code>echo</code> executable in <code>/usr/local/bin/</code> (run <code>open /usr/local/bin/</code> on your machine and visually inspect to verify). <code>echo</code> isn't in <code>/usr/bin</code> either. An <code>echo</code> executable is stored in <code>/bin</code>. The Terminal will use whatever executable it finds first.</p> <p>You can also find where executables are located with the <code>whereis</code> command:</p> <pre><code>~ $ whereis echo\n/bin/echo\n</code></pre>"},{"location":"python/how-pyenv-works-shims/#pyenv-changes-path","title":"pyenv changes PATH","text":"<p>pyenv adds this code to the <code>~/.bash_profile</code> which changes the <code>PATH</code> on your machine:</p> <pre><code>if command -v pyenv 1&gt;/dev/null 2&gt;&amp;1; then\n  eval \"$(pyenv init -)\"\nfi\n</code></pre> <p>Run <code>echo $PATH</code> to see the <code>PATH</code> is different now: <code>/Users/matthewpowers/.pyenv/shims:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin</code>.</p> <p>The <code>/Users/matthewpowers/.pyenv/shims</code> directory has been added before all the other directories in the <code>PATH</code>.</p> <p>All Terminal commands will go through <code>/Users/matthewpowers/.pyenv/shims</code> first now. This allows pyenv to \"intercept\" any relevant Python commands.</p> <ul> <li>The <code>$ python</code> command will go to <code>/Users/matthewpowers/.pyenv/shims/python</code>.</li> <li>The <code>$ pip</code> command will go to <code>/Users/matthewpowers/.pyenv/shims/pip</code>.</li> </ul> <p><code>/Users/matthewpowers/.pyenv/shims</code> doesn't contain an executable named <code>echo</code>, so the <code>echo hi there</code> command will still use the <code>echo</code> executable that's in <code>/bin</code>.</p>"},{"location":"python/how-pyenv-works-shims/#rehashing-design-pattern","title":"Rehashing design pattern","text":"<p>Once you have pyenv installed and run a simple command like <code>pip install pandas</code>, what exactly happens? How does pyenv execute this code?</p> <p>The pyenv README has a great high level description on shims and the rehashing pattern. From the README:</p> <p>Shims are lightweight executables that simply pass your command along to pyenv. So with pyenv installed, when you run, say, <code>pip</code>, your operating system will do the following:</p> <ul> <li>Search your PATH for an executable file named pip</li> <li>Find the pyenv shim named pip at the beginning of your PATH</li> <li>Run the shim named pip, which in turn passes the command along to pyenv</li> </ul> <p>Let's describe these steps in more detail:</p> <ul> <li>When you run <code>pip install pandas</code>, your Terminal will search your <code>PATH</code> for a <code>pip</code> executable. It'll find that executable in the <code>/Users/matthewpowers/.pyenv/shims</code> directory that is at the front of your path. pyenv is intercepting this command and handling it before it can be picked up by another executable lower in the <code>PATH</code> hierarchy. The <code>python</code> command will use the executable in <code>/usr/bin</code>, the dreaded system Python version, it it's not intercepted.</li> <li><code>/Users/matthewpowers/.pyenv/shims/pip</code> is an executable file that's referred to as a \"shim\". A shim intercepts a call and redirects the operation elsewhere.</li> <li>The shim redirects the command to pyenv. Let's take a look at the code with <code>cat /Users/matthewpowers/.pyenv/shims/pip</code>:</li> </ul> <pre><code>#!/usr/bin/env bash\nset -e\n[ -n \"$PYENV_DEBUG\" ] &amp;&amp; set -x\n\nprogram=\"${0##*/}\"\nif [[ \"$program\" = \"python\"* ]]; then\n  for arg; do\n    case \"$arg\" in\n    -c* | -- ) break ;;\n    */* )\n      if [ -f \"$arg\" ]; then\n        export PYENV_FILE_ARG=\"$arg\"\n        break\n      fi\n      ;;\n    esac\n  done\nfi\n\nexport PYENV_ROOT=\"/Users/matthewpowers/.pyenv\"\nexec \"/usr/local/Cellar/pyenv/1.2.18/libexec/pyenv\" exec \"$program\" \"$@\"\n</code></pre> <p>We can see that <code>exec \"/usr/local/Cellar/pyenv/1.2.18/libexec/pyenv\"</code> is \"passing the command along to pyenv\".</p>"},{"location":"python/how-pyenv-works-shims/#intercepting-the-python-command","title":"Intercepting the python command","text":"<p>The <code>python</code> command will be intercepted by <code>/Users/matthewpowers/.pyenv/shims/python</code>, as you can see by running <code>which python</code>:</p> <pre><code>$ which python\n/Users/matthewpowers/.pyenv/shims/python\n</code></pre> <p>The code in <code>/Users/matthewpowers/.pyenv/shims/python</code> is exactly the same as the code in <code>/Users/matthewpowers/.pyenv/shims/pip</code>.</p> <p>This is what <code>cat /Users/matthewpowers/.pyenv/shims/python</code> returns:</p> <pre><code>#!/usr/bin/env bash\nset -e\n[ -n \"$PYENV_DEBUG\" ] &amp;&amp; set -x\n\nprogram=\"${0##*/}\"\nif [[ \"$program\" = \"python\"* ]]; then\n  for arg; do\n    case \"$arg\" in\n    -c* | -- ) break ;;\n    */* )\n      if [ -f \"$arg\" ]; then\n        export PYENV_FILE_ARG=\"$arg\"\n        break\n      fi\n      ;;\n    esac\n  done\nfi\n\nexport PYENV_ROOT=\"/Users/matthewpowers/.pyenv\"\nexec \"/usr/local/Cellar/pyenv/1.2.18/libexec/pyenv\" exec \"$program\" \"$@\"\n</code></pre> <p>The shim passes the command to pyenv without doing any processing.</p>"},{"location":"python/how-pyenv-works-shims/#pyenv-executable","title":"pyenv executable","text":"<p>Let's look at the pyenv executable that's being passed commands from the shims.</p> <p>Run <code>cat /usr/local/Cellar/pyenv/1.2.18/libexec/pyenv</code> to inspect the contents of the <code>pyenv</code> executable. Prepare yourself for 150 lines of Bash code.</p> <pre><code>#!/usr/bin/env bash\nset -e\n\nif [ \"$1\" = \"--debug\" ]; then\n  export PYENV_DEBUG=1\n  shift\nfi\n\nif [ -n \"$PYENV_DEBUG\" ]; then\n  export PS4='+ [${BASH_SOURCE##*/}:${LINENO}] '\n  set -x\nfi\n\nabort() {\n  { if [ \"$#\" -eq 0 ]; then cat -\n    else echo \"pyenv: $*\"\n    fi\n  } &gt;&amp;2\n  exit 1\n}\n\nif enable -f \"${BASH_SOURCE%/*}\"/../libexec/pyenv-realpath.dylib realpath 2&gt;/dev/null; then\n  abs_dirname() {\n    local path\n    path=\"$(realpath \"$1\")\"\n    echo \"${path%/*}\"\n  }\nelse\n  [ -z \"$PYENV_NATIVE_EXT\" ] || abort \"failed to load \\`realpath' builtin\"\n\n  READLINK=$(type -p greadlink readlink | head -1)\n  [ -n \"$READLINK\" ] || abort \"cannot find readlink - are you missing GNU coreutils?\"\n\n  resolve_link() {\n    $READLINK \"$1\"\n  }\n\n  abs_dirname() {\n    local path=\"$1\"\n\n    # Use a subshell to avoid changing the current path\n    (\n    while [ -n \"$path\" ]; do\n      cd_path=\"${path%/*}\"\n      if [[ \"$cd_path\" != \"$path\" ]]; then\n        cd \"$cd_path\"\n      fi\n      name=\"${path##*/}\"\n      path=\"$(resolve_link \"$name\" || true)\"\n    done\n\n    echo \"$PWD\"\n    )\n  }\nfi\n\nif [ -z \"${PYENV_ROOT}\" ]; then\n  PYENV_ROOT=\"${HOME}/.pyenv\"\nelse\n  PYENV_ROOT=\"${PYENV_ROOT%/}\"\nfi\nexport PYENV_ROOT\n\n# Transfer PYENV_FILE_ARG (from shims) into PYENV_DIR.\nif [ -z \"${PYENV_DIR}\" ]; then\n  if [ -n \"${PYENV_FILE_ARG}\" ]; then\n    if [ -L \"${PYENV_FILE_ARG}\" ]; then\n      PYENV_DIR=\"$(abs_dirname \"${PYENV_FILE_ARG}\")\"\n    else\n      PYENV_DIR=\"${PYENV_FILE_ARG%/*}\"\n    fi\n    export PYENV_DIR\n    unset PYENV_FILE_ARG\n  fi\nelse\n  [[ $PYENV_DIR == /* ]] || PYENV_DIR=\"$PWD/$PYENV_DIR\"\n  cd \"$PYENV_DIR\" 2&gt;/dev/null || abort \"cannot change working directory to \\`$PYENV_DIR'\"\n  PYENV_DIR=\"$PWD\"\n  cd \"$OLDPWD\"\nfi\n\nif [ -z \"${PYENV_DIR}\" ]; then\n  PYENV_DIR=\"$PWD\"\nfi\n\nif [ ! -d \"$PYENV_DIR\" ] || [ ! -e \"$PYENV_DIR\" ]; then\n  abort \"cannot change working directory to \\`$PYENV_DIR'\"\nfi\n\nPYENV_DIR=$(cd \"$PYENV_DIR\" &amp;&amp; echo \"$PWD\")\nexport PYENV_DIR\n\n\nshopt -s nullglob\n\nbin_path=\"$(abs_dirname \"$0\")\"\nfor plugin_bin in \"${PYENV_ROOT}/plugins/\"*/bin; do\n  PATH=\"${plugin_bin}:${PATH}\"\ndone\nexport PATH=\"${bin_path}:${PATH}\"\n\nPYENV_HOOK_PATH=\"${PYENV_HOOK_PATH}:${PYENV_ROOT}/pyenv.d\"\nif [ \"${bin_path%/*}\" != \"$PYENV_ROOT\" ]; then\n  # Add pyenv's own `pyenv.d` unless pyenv was cloned to PYENV_ROOT\n  PYENV_HOOK_PATH=\"${PYENV_HOOK_PATH}:${bin_path%/*}/pyenv.d\"\nfi\nPYENV_HOOK_PATH=\"${PYENV_HOOK_PATH}:/usr/local/etc/pyenv.d:/etc/pyenv.d:/usr/lib/pyenv/hooks\"\nfor plugin_hook in \"${PYENV_ROOT}/plugins/\"*/etc/pyenv.d; do\n  PYENV_HOOK_PATH=\"${PYENV_HOOK_PATH}:${plugin_hook}\"\ndone\nPYENV_HOOK_PATH=\"${PYENV_HOOK_PATH#:}\"\nexport PYENV_HOOK_PATH\n\nshopt -u nullglob\n\n\ncommand=\"$1\"\ncase \"$command\" in\n\"\" )\n  { pyenv---version\n    pyenv-help\n  } | abort\n  ;;\n-v | --version )\n  exec pyenv---version\n  ;;\n-h | --help )\n  exec pyenv-help\n  ;;\n* )\n  command_path=\"$(command -v \"pyenv-$command\" || true)\"\n  if [ -z \"$command_path\" ]; then\n    if [ \"$command\" == \"shell\" ]; then\n      abort \"shell integration not enabled. Run \\`pyenv init' for instructions.\"\n    else\n      abort \"no such command \\`$command'\"\n    fi\n  fi\n\n  shift 1\n  if [ \"$1\" = --help ]; then\n    if [[ \"$command\" == \"sh-\"* ]]; then\n      echo \"pyenv help \\\"$command\\\"\"\n    else\n      exec pyenv-help \"$command\"\n    fi\n  else\n    exec \"$command_path\" \"$@\"\n  fi\n  ;;\n</code></pre> <p>Here is where this script is defined in the pyenv repo.</p>"},{"location":"python/how-pyenv-works-shims/#pip-install-for-different-python-versions","title":"pip install for different Python versions","text":"<p>Suppose you're using Python 3.7.5 and run <code>pip install pandas</code>.</p> <p>The Pandas code will be stored in <code>.pyenv/versions/3.7.5/lib/python3.7/site-packages</code>.</p> <p>If you switch to Python 3.6.10 with <code>pyenv shell 3.6.10</code>, you'll need to reinstall pandas with another <code>pip install pandas</code>. The Python 3.6.10 pandas will be stored in <code>/Users/matthewpowers/.pyenv/versions/3.6.10/lib/python3.6/site-packages</code>.</p> <p>pyenv doesn't share library versions across different Python versions.</p> <p>See pyenv-virtualenv for more details about managing virtual environments with pyenv.</p>"},{"location":"python/how-pyenv-works-shims/#pyenv-is-not-bootstrapped-by-python","title":"pyenv is not bootstrapped by Python","text":"<p>You might wonder why the pyenv codebase is almost entirely Shell and Roff code.</p> <p></p> <p>pyenv intentionally avoids Python. If pyenv was written in Python, then the system would need Python installed to run pyenv commands. pyenv can install Python on machines that don't have any version of Python installed (most machines come with an old Python version pre-installed).</p> <p>You don't want a program that installs Python to depend on Python.</p> <p>The system version of Python that comes pre-installed in machines causes a huge headache for programmers that don't understand the PATH, executable, etc. The shell scripting patterns you've learned in this post will save you from a lot of Python development hell.</p>"},{"location":"python/how-pyenv-works-shims/#conclusion","title":"Conclusion","text":"<p>We learned a lot about pyenv and have a good understanding of how it works.</p> <p>The shims are injected at the beginning of your PATH so pyenv can route commands to the right Python version. pyenv is using versions specified in text files to see what version of Python should be used to run the commands.</p> <p>pyenv offers an elegant user interface for managing different Python versions for end users. The codebase is amazingly clean.</p> <p>Programs like these make you sit back with awe - you're starstruck that other programmers can architect such beauty.</p> <p>Study their design patterns and try to copy them! Copy what you like.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/","title":"Amazing Python Data Workflow with Poetry, Pandas, and Jupyter","text":"<p>Poetry makes it easy to install Pandas and Jupyter to perform data analyses.</p> <p>Poetry is a robust dependency management system and makes it easy to make Python libraries accessible in Jupyter notebooks.</p> <p>The workflow outlined in this post makes projects that can easily be run on other machines. Your teammates can easily run <code>poetry install</code> to setup an identical Jupyter development environment on their computers.</p> <p>Python dependency management is hard, especially for projects with notebooks. You'll often find yourself in dependency hell when trying to setup someone else's repo with Jupyter notebooks. This workflow saves you from dependency hell.</p> <p>This post shows how to manage environments with Poetry, but you can also use conda of course.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#create-a-project","title":"Create a project","text":"<p>Install Poetry and run the <code>poetry new blake</code> command to create a project called <code>blake</code>. All the code covered in this post is in a GitHub repo, but it's best to run all the commands on your local machine, so you learn more.</p> <p>Change into the <code>blake</code> directory with <code>cd blake</code> and examine the file structure:</p> <pre><code>blake/\n  blake/\n    __init__.py\n  tests/\n    __init__.py\n    test_blake.py\n  pyproject.toml\n  README.rst\n</code></pre> <p>We'll investigate the contents of these files later in this post.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#install-pandas-and-jupyter","title":"Install Pandas and Jupyter","text":"<p>Run <code>poetry add pandas jupyter ipykernel</code> to install the dependencies that are required for running notebooks on your local machine.</p> <p>This command downloads a bunch of Python code in the <code>~/Library/Caches/pypoetry/virtualenvs/blake-Y_2IcspR-py3.7/</code> directory. This is referred to as the \"virtual environment\" of your project.</p> <p>If you cloned the blake repo, you could simply run <code>poetry install</code> to setup the virtual environment.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#notebook-workflow","title":"Notebook workflow","text":"<p>Run <code>poetry shell</code> in your Terminal to create a subshell within the virtual environment. This is the key step that lets you run a Jupyter notebook with all the right project dependencies.</p> <p></p> <p>Run <code>jupyter notebook</code> to open the project with Jupyter in your browser.</p> <p>Click New =&gt; Folder to create a folder called <code>notebooks/</code>.</p> <p></p> <p>Go to the <code>notebooks</code> folder and click New =&gt; Notebook: Python 3 to create a notebook.</p> <p></p> <p>Click Untitled at the top of the page that opens and rename the notebook to be <code>some_pandas_fun</code>:</p> <p></p> <p>Run <code>2 + 2</code> in the first cell to make sure the notebook can run a basic Python command.</p> <p>Then run this series of commands in the subsequent cells to create a Pandas DataFrame.</p> <pre><code>import pandas as pd\ndata = [['tom', 10], ['nick', 15], ['juli', 14]]\ndf = pd.DataFrame(data, columns = ['Name', 'Age'])\nprint(df)\n</code></pre>"},{"location":"python/jupyter-workflow-poetry-pandas/#accessing-application-code-in-notebook","title":"Accessing application code in notebook","text":"<p>The application code goes in the <code>blake/</code> directory. Create a <code>blake/super_important.py</code> file with a <code>some_message</code> function.</p> <pre><code>def some_message():\n    return \"I like dancing reggaeton\"\n</code></pre> <p>Create another Jupyter notebook, import the <code>some_message</code> function, and run the code to make sure it's accessible.</p> <p></p> <p>Access application code in notebook</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#testing-application-code","title":"Testing application code","text":"<p>Create a <code>tests/test_super_important.py</code> file that verifies the <code>some_message</code> function is working properly.</p> <pre><code>import pytest\n\nfrom blake.super_important import *\n\ndef test_some_message():\n    assert some_message() == \"I like dancing reggaeton\"\n</code></pre> <p>Run the test suite with the <code>poetry run pytest tests/</code> command.</p> <p></p> <p>As you can see, Poetry makes it easy to organize the application code, notebooks, and tests for a project.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#write-a-parquet-file","title":"Write a Parquet file","text":"<p>Let's create a CSV file and then write it out as a Parquet file from a notebook.</p> <p>Pandas requires PyArrow to write Parquet files so run <code>poetry add pyarrow</code> to include the dependency.</p> <p>Create the <code>data/coffee.csv</code> file with this data:</p> <pre><code>coffee_type,has_milk\nblack,false\nlatte,true\namericano,false\n</code></pre> <p>Create a <code>notebooks/csv_to_parquet.ipynb</code> file that'll convert the CSV to a Parquet file.</p> <p>Here's the code:</p> <pre><code>import pandas as pd\nimport os\n\ndf = pd.read_csv(os.environ['HOME'] + '/Documents/code/my_apps/blake/data/coffee.csv')\nout_dirname = os.environ['HOME'] + '/Documents/code/my_apps/blake/tmp'\nos.makedirs(out_dirname, exist_ok=True)\ndf.to_parquet(out_dirname + '/coffee.parquet')\n</code></pre> <p></p>"},{"location":"python/jupyter-workflow-poetry-pandas/#read-a-parquet-file","title":"Read a Parquet file","text":"<p>Create a <code>notebooks/csv_to_parquet.ipynb</code> file and read the Parquet file into a DataFrame.</p> <p>Then count how many different types of coffee contain milk in the dataset.</p> <pre><code>import pandas as pd\nimport os\n\ndf = pd.read_parquet(os.environ['HOME'] + '/Documents/code/my_apps/blake/tmp/coffee.parquet')\ncoffees_with_milk = df[df['has_milk'] == True]\ncoffees_with_milk.count()\n</code></pre> <p></p> <p>Parquet is a better file format than CSV for almost all data analyses. Use this design pattern to build Parquet files, so you can perform your analyses quicker.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#pyprojecttoml","title":"pyproject.toml","text":"<p>Here are the contents of the <code>pyproject.toml</code> file:</p> <pre><code>[tool.poetry]\nname = \"blake\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"MrPowers\"]\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\npandas = \"^1.1.1\"\njupyter = \"^1.0.0\"\nipykernel = \"^5.3.4\"\npyarrow = \"^1.0.1\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^5.2\"\n\n[build-system]\nrequires = [\"poetry&gt;=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n</code></pre> <p>The project dependencies clearly specify the versions that are required to run this project. The dev-dependencies specify additional dependencies that are required when running the test suite (i.e. when running <code>poetry run pytest tests/</code>).</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#poetrylock","title":"poetry.lock","text":"<p>Here's how Poetry builds the virtual environment when <code>poetry install</code> is run:</p> <ul> <li>If the <code>poetry.lock</code> file exists, use the exact dependency version specified in the lock file to build the virtual environment</li> <li>If the <code>poetry.lock</code> file doesn't exist, then use the <code>pypoetry.toml</code> file to resolve the dependencies, build a lock file, and setup the virtual environment</li> </ul> <p>You should check the lock file into source control so collaborators can build a virtual environment that's identical to what you're using.</p> <p>If you're ever having trouble with the virtual environment or lock file, feel free to simply delete them and recreate them with <code>poetry install</code>. Don't manually modify the lock file or virtual environment.</p>"},{"location":"python/jupyter-workflow-poetry-pandas/#conclusion","title":"Conclusion","text":"<p>Poetry allows for an amazing local workflow with Pandas and Jupyter.</p> <p>Poetry virtual environments are clean, easy to use, and save you from dependency hell.</p> <p>Poetry also makes it easy to work with Python cluster computing libraries like Dask and PySpark.</p> <p>The Poetry workflow outlined in this post is especially useful, because it easily extends to a team of multiple developers. You can follow the steps outlined in this guide, upload your code to GitHub, so your teammates can clone the repo and run <code>poetry install</code> to create an identical development environment on their machine.</p> <p>Poetry saves your teammates from suffering with Python dependency hell. Make your projects easy to use and you'll get more users and adoption!</p>"},{"location":"python/split-csv-write-chunk-pandas/","title":"Splitting Large CSV files with Python","text":"<p>This blog post demonstrates different approaches for splitting a large CSV file into smaller CSV files and outlines the costs / benefits of the different approaches.</p> <p>TL;DR</p> <ul> <li>It\u2019s faster to split a CSV file with a shell command / the Python filesystem API</li> <li>Pandas / Dask are more robust and flexible options</li> </ul> <p>Let\u2019s investigate the different approaches &amp; look at how long it takes to split a 2.9 GB CSV file with 11.8 million rows of data.</p>"},{"location":"python/split-csv-write-chunk-pandas/#split-with-shell","title":"Split with shell","text":"<p>You can split a CSV on your local filesystem with a shell command.</p> <pre><code>FILENAME=nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\n\nsplit -b 10000000 $FILENAME tmp/split_csv_shell/file\n</code></pre> <p>This only takes 4 seconds to run. Each file output is 10MB and has around 40,000 rows of data.</p> <p>This approach has a number of key downsides:</p> <ul> <li>It cannot be run on files stored in a cloud filesystem like S3</li> <li>It breaks if there are newlines in the CSV row (possible for quoted data)</li> <li>Does not handle the header row</li> </ul>"},{"location":"python/split-csv-write-chunk-pandas/#python-filesystem-apis","title":"Python filesystem APIs","text":"<p>You can also use the Python filesystem readers / writers to split a CSV file.</p> <pre><code>chunk_size = 40000\n\ndef write_chunk(part, lines):\n    with open('../tmp/split_csv_python/data_part_'+ str(part) +'.csv', 'w') as f_out:\n        f_out.write(header)\n        f_out.writelines(lines)\n\nwith open(\"../nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\", \"r\") as f:\n    count = 0\n    header = f.readline()\n    lines = []\n    for line in f:\n        count += 1\n        lines.append(line)\n        if count % chunk_size == 0:\n            write_chunk(count // chunk_size, lines)\n            lines = []\n    # write remainder\n    if len(lines) &gt; 0:\n        write_chunk((count // chunk_size) + 1, lines)\n</code></pre> <p>This takes 9.6 seconds to run and properly outputs the header row in each split CSV file, unlike the shell script approach.</p> <p>It\u2019d be easier to adapt this script to run on files stored in a cloud object store than the shell script as well.</p> <p>Let\u2019s look at some approaches that are a bit slower, but more flexible.</p>"},{"location":"python/split-csv-write-chunk-pandas/#pandas","title":"Pandas","text":"<p>Here\u2019s how to read in chunks of the CSV file into Pandas DataFrames and then write out each DataFrame.</p> <pre><code>source_path = \"../nyc-parking-tickets/Parking_Violations_Issued_-_Fiscal_Year_2015.csv\"\n\nfor i,chunk in enumerate(pd.read_csv(source_path, chunksize=40000, dtype=dtypes)):\n    chunk.to_csv('../tmp/split_csv_pandas/chunk{}.csv'.format(i), index=False)\n</code></pre> <p>This approach writes 296 files, each with around 40,000 rows of data. It takes 160 seconds to execute.</p> <p>The Pandas approach is more flexible than the Python filesystem approaches because it allows you to process the data before writing. You could easily update the script to add columns, filter rows, or write out the data to different file formats.</p> <p>Manipulating the output isn\u2019t possible with the shell approach and difficult / error-prone with the Python filesystem approach.</p>"},{"location":"python/split-csv-write-chunk-pandas/#dask","title":"Dask","text":"<p>Here\u2019s how to read the CSV file into a Dask DataFrame in 10 MB chunks and write out the data as 287 CSV files.</p> <pre><code>ddf = dd.read_csv(source_path, blocksize=10000000, dtype=dtypes)\n\nddf.to_csv(\"../tmp/split_csv_dask\")\n</code></pre> <p>The Dask script runs in 172 seconds.</p> <p>For this particular computation, the Dask runtime is roughly equal to the Pandas runtime. The Dask task graph that builds instructions for processing a data file is similar to the Pandas script, so it makes sense that they take the same time to execute.</p> <p>Dask allows for some intermediate data processing that wouldn\u2019t be possible with the Pandas script, like sorting the entire dataset. The Pandas script only reads in chunks of the data, so it couldn\u2019t be tweaked to perform shuffle operations on the entire dataset.</p>"},{"location":"python/split-csv-write-chunk-pandas/#comparing-approaches","title":"Comparing approaches","text":"<p>This graph shows the program execution runtime by approach.</p> <p></p> <p>If you need to quickly split a large CSV file, then stick with the Python filesystem API.</p> <p>Processing time generally isn\u2019t the most important factor when splitting a large CSV file. Production grade data analyses typically involve these steps:</p> <ul> <li>Validating data and throwing out junk rows</li> <li>Properly assigning types to each column</li> <li>Writing data to a good file format for data analysis, like Parquet</li> <li>Compressing the data</li> </ul> <p>The main objective when splitting a large CSV file is usually to make downstream analyses run faster and more reliably. Dask is the most flexible option for a production-grade solution.</p>"},{"location":"python/split-csv-write-chunk-pandas/#next-steps","title":"Next steps","text":"<p>Large CSV files are not good for data analyses because they can\u2019t be read in parallel. Multiple files can easily be read in parallel.</p> <p>CSV files in general are limited because they don\u2019t contain schema metadata, the header row requires extra processing logic, and the row based nature of the file doesn\u2019t allow for column pruning. The main advantage of CSV files is that they\u2019re human readable, but that doesn\u2019t matter if you\u2019re processing your data with a production-grade data processing engine, like Python or Dask.</p> <p>Splitting up a large CSV file into multiple Parquet files (or another good file format) is a great first step for a production-grade data processing pipeline. Dask takes longer than a script that uses the Python filesystem API, but makes it easier to build a robust script. The performance drag doesn\u2019t typically matter. You only need to split the CSV once.</p> <p>The more important performance consideration is figuring out how to split the file in a manner that\u2019ll make all your downstream analyses run significantly faster.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/","title":"Writing Parquet Files in Python with Pandas, PySpark, and Koalas","text":"<p>This blog post shows how to convert a CSV file to Parquet with Pandas, Spark, PyArrow and Dask.</p> <p>It discusses the pros and cons of each approach and explains how both approaches can happily coexist in the same ecosystem.</p> <p>Parquet is a columnar file format whereas CSV is row based. Columnar file formats are more efficient for most analytical queries. You can speed up a lot of your Panda DataFrame queries by converting your CSV files and working off of Parquet files.</p> <p>All the code used in this blog is in this GitHub repo.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#pandas-approach","title":"Pandas approach","text":"<p>Suppose you have the following <code>data/us_presidents.csv</code> file:</p> <pre><code>full_name,birth_year\nteddy roosevelt,1901\nabe lincoln,1809\n</code></pre> <p>You can easily read this file into a Pandas DataFrame and write it out as a Parquet file as described in this Stackoverflow answer.</p> <pre><code>import pandas as pd\n\ndef write_parquet_file():\n    df = pd.read_csv('data/us_presidents.csv')\n    df.to_parquet('tmp/us_presidents.parquet')\n\nwrite_parquet_file()\n</code></pre> <p>This code writes out the data to a <code>tmp/us_presidents.parquet</code> file.</p> <p>Let's read the Parquet data into a Pandas DataFrame and view the results.</p> <pre><code>df = pd.read_parquet('tmp/us_presidents.parquet')\nprint(df)\n\n         full_name  birth_year\n0  teddy roosevelt        1901\n1      abe lincoln        1809\n</code></pre> <p>Pandas provides a beautiful Parquet interface. Pandas leverages the PyArrow library to write Parquet files, but you can also write Parquet files directly from PyArrow.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#pyarrow","title":"PyArrow","text":"<p>PyArrow lets you read a CSV file into a table and write out a Parquet file, as described in this blog post. The code is simple to understand:</p> <pre><code>import pyarrow.csv as pv\nimport pyarrow.parquet as pq\n\ntable = pv.read_csv('./data/people/people1.csv')\npq.write_table(table, './tmp/pyarrow_out/people1.parquet')\n</code></pre> <p>PyArrow is worth learning because it provides access to file schema and other metadata stored in the Parquet footer. Studying PyArrow will teach you more about Parquet.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#dask","title":"Dask","text":"<p>Dask is a parallel computing framework that makes it easy to convert a lot of CSV files to Parquet files with a single operation as described in this post.</p> <p>Here's a code snippet, but you'll need to read the blog post to fully understand it:</p> <pre><code>import dask.dataframe as dd\n\ndf = dd.read_csv('./data/people/*.csv')\ndf.to_parquet('./tmp/people_parquet2', write_index=False)\n</code></pre> <p>Dask is similar to Spark and easier to use for folks with a Python background. Spark is still worth investigating, especially because it's so powerful for big data sets.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#pyspark","title":"PySpark","text":"<p>Let's read the CSV data to a PySpark DataFrame and write it out in the Parquet format.</p> <p>We'll start by creating a <code>SparkSession</code> that'll provide us access to the Spark CSV reader.</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n  .master(\"local\") \\\n  .appName(\"parquet_example\") \\\n  .getOrCreate()\n\ndf = spark.read.csv('data/us_presidents.csv', header = True)\ndf.repartition(1).write.mode('overwrite').parquet('tmp/pyspark_us_presidents')\n</code></pre> <p>We need to specify <code>header = True</code> when reading the CSV to indicate that the first row of data is column headers.</p> <p>Spark normally writes data to a directory with many files. The directory only contains one file in this example because we used <code>repartition(1)</code>. Spark can write out multiple files in parallel for big datasets and that's one of the reasons Spark is such a powerful big data engine.</p> <p>Let's look at the contents of the <code>tmp/pyspark_us_presidents</code> directory:</p> <pre><code>pyspark_us_presidents/\n  _SUCCESS\n  part-00000-81610cf2-dc76-481e-b302-47b59e06d9b6-c000.snappy.parquet\n</code></pre> <p>The <code>part-00000-81...snappy.parquet</code> file contains the data. Spark uses the Snappy compression algorithm for Parquet files by default.</p> <p>Let's read <code>tmp/pyspark_us_presidents</code> Parquet data into a DataFrame and print it out.</p> <pre><code>df = spark.read.parquet('tmp/pyspark_us_presidents')\ndf.show()\n\n+---------------+----------+\n|      full_name|birth_year|\n+---------------+----------+\n|teddy roosevelt|      1901|\n|    abe lincoln|      1809|\n+---------------+----------+\n</code></pre> <p>Setting up a PySpark project on your local machine is surprisingly easy, see this blog post for details.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#koalas","title":"Koalas","text":"<p>koalas lets you use the Pandas API with the Apache Spark execution engine under the hood.</p> <p>Let's read the CSV and write it out to a Parquet folder (notice how the code looks like Pandas):</p> <pre><code>import databricks.koalas as ks\n\ndf = ks.read_csv('data/us_presidents.csv')\ndf.to_parquet('tmp/koala_us_presidents')\n</code></pre> <p>Read the Parquet output and display the contents:</p> <pre><code>df = ks.read_parquet('tmp/koala_us_presidents')\nprint(df)\n\n         full_name  birth_year\n0  teddy roosevelt        1901\n1      abe lincoln        1809\n</code></pre> <p>Koalas outputs data to a directory, similar to Spark. Here's what the <code>tmp/koala_us_presidents</code> directory contains:</p> <pre><code>koala_us_presidents/\n  _SUCCESS\n  part-00000-1943a0a6-951f-4274-a914-141014e8e3df-c000.snappy.parquet\n</code></pre>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#pandas-and-spark-can-happily-coexist","title":"Pandas and Spark can happily coexist","text":"<p>Pandas is great for reading relatively small datasets and writing out a single Parquet file.</p> <p>Spark is great for reading and writing huge datasets and processing tons of files in parallel.</p> <p>Suppose your data lake currently contains 10 terabytes of data and you'd like to update it every 15 minutes. You get 100 MB of data every 15 minutes. Maybe you setup a lightweight Pandas job to incrementally update the lake every 15 minutes. You can do the big extracts and data analytics on the whole lake with Spark.</p>"},{"location":"python/writing-parquet-pandas-pyspark-koalas/#next-steps","title":"Next steps","text":"<p>The Delta Lake project makes Parquet data lakes a lot more powerful by adding a transaction log. This makes it easier to perform operations like backwards compatible compaction, etc.</p> <p>I am going to try to make an open source project that makes it easy to interact with Delta Lakes from Pandas. The Delta lake design philosophy should make it a lot easier for Pandas users to manage Parquet datasets. Stay tuned!</p>"},{"location":"scala/call-java-code-with-wrapper/","title":"Wrapping Java Code with Clean Scala Interfaces","text":"<p>This post explains how to wrap a Java library with a Scala interface.</p> <p>You can instantiate Java classes directly in Scala, but it's best to wrap the Java code, so you don't need to interface with it directly. Scala wrappers help hide the Java messiness.</p> <p>os-lib is a great example of a project that hides the underlying Java messiness of common filesystem operations. It's a game changing lib to work with. Hiding verbose interfaces can add tons of value.</p> <p>commonmark-java is a great library for converting Markdown strings into HTML. Let's wrap this library in a single Scala function that'll take a Markdown string and return a HTML string.</p>"},{"location":"scala/call-java-code-with-wrapper/#java-interface","title":"Java interface","text":"<p>Here's the basic commonmark-java usage, per the README:</p> <pre><code>import org.commonmark.node.*;\nimport org.commonmark.parser.Parser;\nimport org.commonmark.renderer.html.HtmlRenderer;\n\nParser parser = Parser.builder().build();\nNode document = parser.parse(\"This is *Sparta*\");\nHtmlRenderer renderer = HtmlRenderer.builder().build();\nrenderer.render(document);  // \"&lt;p&gt;This is &lt;em&gt;Sparta&lt;/em&gt;&lt;/p&gt;\\n\"\n</code></pre> <p>That's a lot of imports and object instantiations to convert a Markdown string to a HTML string.</p> <p>The <code>Parser.builder().build()</code> chunk isn't even a typo - that's really the interface that's being exposed to end users.</p> <p>Let's see if we can clean this up.</p>"},{"location":"scala/call-java-code-with-wrapper/#include-the-library","title":"Include the library","text":"<p>Start by including the commonmark-java project in your <code>build.sbt</code> file:</p> <pre><code>libraryDependencies += \"com.atlassian.commonmark\" % \"commonmark\" % \"0.16.1\"\n</code></pre> <p>Scala libraries are included with two percent signs between the <code>groupId</code> and the <code>artifactId</code>, for example <code>libraryDependencies += \"com.lihaoyi\" %% \"os-lib\" % \"0.7.1\"</code>. The <code>%%</code> syntax is because Scala projects are cross compiled for different versions of Scala (e.g. one release for Scala 2.12 and another release for Scala 2.13).</p> <p>Java projects are not cross compiled, so they don't need the double <code>%%</code>.</p>"},{"location":"scala/call-java-code-with-wrapper/#create-a-scala-wrapper","title":"Create a Scala wrapper","text":"<p>Let's create a single function that takes a Markdown string argument and returns a HTML string.</p> <pre><code>object CommonmarkWrapper {\n\n  def renderMd(text: String) = {\n    val parser = org.commonmark.parser.Parser.builder().build()\n    val document = parser.parse(text)\n    val renderer = org.commonmark.renderer.html.HtmlRenderer.builder().build()\n    renderer.render(document)\n  }\n\n}\n</code></pre> <p>See how we've added <code>val</code> to the Java code to build syntactically correct Scala code.</p> <p>Let's see how this library can be used:</p> <pre><code>CommonmarkWrapper.renderMd(\"This is *Sparta*\") // \"&lt;p&gt;This is &lt;em&gt;Sparta&lt;/em&gt;&lt;/p&gt;\\n\"\n</code></pre> <p>All of the Java messiness is now being hidden from end users. They can just call the <code>renderMd</code> method without worrying out instantiating a bunch of different objects.</p>"},{"location":"scala/call-java-code-with-wrapper/#scala-the-better-java","title":"Scala, the better Java","text":"<p>A lot of early Scala adopters were just looking for a \"better Java\". Something similar to Java, but less verbose.</p> <p>Scala is now losing part of the \"better Java\" market share to Kotlin, but the idea of providing elegant interfaces via Scala still remains.</p> <p>You don't need to suffer with verbose Java code in a Scala codebase. Wrap the Java messiness in a file that's hidden away from the rest of your Scala code.</p> <p>Succinct Scala makes for a readable codebase.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/","title":"Scala Filesystem Operations (paths, move, copy, list, delete)","text":"<p>Basic filesystem operations have traditionally been complex in Scala.</p> <p>A simple operation like copying a file is a one-liner in some languages like Ruby, but a multi-line / multi-import mess if you rely on Java libraries.</p> <p>Li thankfully created an os-lib project that makes Scala filesystem operations easy and intuitive. The library has a Ruby-like elegance.</p> <p>Scala developers no longer need to mess around with low level Java libraries for basic filesystem operations.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#moving-a-file","title":"Moving a file","text":"<p>Suppose you're working in an SBT project with a <code>people.csv</code> file in this directory structure:</p> <pre><code>src/\n  main/\n  test/\n    resources/\n      people.csv\n  tmp/\n</code></pre> <p>Here's how to copy <code>src/test/resources/people.csv</code> into the <code>tmp</code> directory with os-lib:</p> <pre><code>os.copy(\n  os.pwd/\"src\"/\"test\"/\"resources\"/\"people.csv\",\n  os.pwd/\"tmp\"/\"people_copy.csv\"\n)\n</code></pre> <p>Here's the top ranked Stackoverflow answer when you Google \"copy file Scala\".</p> <p>The top ranked answer uses <code>java.io</code>:</p> <pre><code>import java.io.{File,FileInputStream,FileOutputStream}\nval src = new File(args(0))\nval dest = new File(args(1))\nnew FileOutputStream(dest) getChannel() transferFrom(\n    new FileInputStream(src) getChannel, 0, Long.MaxValue )\n</code></pre> <p>There's another answer that uses <code>java.nio</code>:</p> <pre><code>import java.nio.file.StandardCopyOption.REPLACE_EXISTING\nimport java.nio.file.Files.copy\nimport java.nio.file.Paths.get\n\nimplicit def toPath (filename: String) = get(filename)\n\ncopy (from, to, REPLACE_EXISTING)\n</code></pre> <p>The Java libraries require a bunch of imports and low level library understanding. You usually just want to copy a file and don't care about low level details.</p> <p>The Java libraries also require some yak-shaving research. You'll need learn how to handle Java paths and investigate if the blocking / non-blocking APIs are better for your application.</p> <p>The path / filesystem operations are both packed into os-lib, so you don't need to mess around with different packages.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#copying-a-file-in-ruby","title":"Copying a file in Ruby","text":"<p>The <code>os-lib</code> solution is similar to how you can copy a file in Ruby:</p> <pre><code>require 'fileutils'\n\nFileUtils.cp(\n  Dir.pwd + \"/src/test/resources/people.csv\",\n  Dir.pwd + \"/tmp/people_copy.csv\"\n)\n</code></pre> <p>Ruby is know for having a great filesystem API and it's a great accomplishment for a Scala library to provide such an elegant user interface.</p> <p>Li has a history of porting good Python libraries to Scala. Want to build a wonderful Scala graph library? Email me and let's build NetworkX for Scala.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#no-imports-needed","title":"No imports needed","text":"<p>The <code>os-lib</code> API is exposed via the <code>os</code> object.</p> <p>You can add <code>libraryDependencies += \"com.lihaoyi\" %% \"os-lib\" % \"0.7.1\"</code> to your <code>build.sbt</code> files and the <code>os</code> object is automatically available.</p> <p>You don't need to add any additional imports when using the library, so it feels like it's a native Scala package.</p> <p>Libraries generally shouldn't be exposed in this manner, but Li can get away with it ;)</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#creating-a-folder-and-file","title":"Creating a folder and file","text":"<p>Let's create a <code>my_folder</code> directory:</p> <pre><code>os.makeDir(os.pwd/\"my_folder\")\n</code></pre> <p>Now let's add a <code>story.txt</code> file to the folder with some text.</p> <pre><code>os.write(os.pwd/\"my_folder\"/\"story.txt\", \"once upon a time\")\n</code></pre> <p>These commands feel Unix-like.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#deleting-a-file","title":"Deleting a file","text":"<p>Let's delete the <code>story.txt</code> file:</p> <pre><code>os.remove(os.pwd/\"my_folder\"/\"story.txt\")\n</code></pre> <p><code>os.remove</code> can also be used to delete empty folders:</p> <pre><code>os.remove(os.pwd/\"my_folder\")\n</code></pre>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#file-listing","title":"File listing","text":"<p>Let's create a folder with a few files and then list the folder contents.</p> <pre><code>os.makeDir(os.pwd/\"dogs\")\nos.write(os.pwd/\"dogs\"/\"dog1.txt\", \"ruff ruff\")\nos.write(os.pwd/\"dogs\"/\"dog2.txt\", \"bow wow\")\nos.write(os.pwd/\"dogs\"/\"dog3.txt\", \"rrrrr\")\n</code></pre> <p><code>os.list(os.pwd/\"dogs\")</code> returns a listing of the files as an ArraySeq:</p> <pre><code>ArraySeq(\n  /Users/powers/Documents/code/my_apps/scala-design/dogs/dog1.txt,\n  /Users/powers/Documents/code/my_apps/scala-design/dogs/dog2.txt,\n  /Users/powers/Documents/code/my_apps/scala-design/dogs/dog3.txt\n)\n</code></pre> <p>These file listing capabilities allow for idiomatic Scala file processing. You can recursively list a directory and find the largest nested file for example.</p> <p>You can't delete the <code>dogs</code> directory with <code>os.remove(os.pwd/\"dogs\")</code> because it contains files. You need to use <code>os.remove.all(os.pwd/\"dogs\")</code>.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#this-library-is-a-good-example-for-the-scala-community","title":"This library is a good example for the Scala community","text":"<p>This library provides several good examples for the Scala community:</p> <ul> <li>it provides a Ruby-like, clean public interface</li> <li>it's dependency free</li> <li>it's intuitive and you can answer your own questions by searching the README (you don't need to read source code or ask Stackoverflow)</li> <li>the API is stable</li> <li>import interface is clean (well, non-existent for this lib)</li> <li>it's cross compiled with Scala 2.12 and 2.13</li> </ul> <p>You can depend on os-lib without worrying that it'll become a maintenance burden.</p>"},{"location":"scala/filesystem-paths-move-copy-list-delete-folders/#next-steps","title":"Next steps","text":"<p>Li's Hands on Scala Programming book has an entire chapter dedicated to this library and covers it in much more detail. You should buy the book. It's a masterpiece.</p> <p>You don't need to study the library extensively to start using it. Learn the basic examples and then answer your specific questions by searching the README with Command + F. Libraries that document the public interface in the README are so easy to use.</p> <p>Include the os-lib library in your project and enjoy it. Study the library design patterns that Li uses to make wonderful contributions to the Scala community. Writing code with minimal dependencies and clear public interfaces will make you a better Scala programmer.</p>"},{"location":"scala/library-best-practices/","title":"Scala Library Best Practices","text":"<p>This post explains the best practices Scala libraries should follow.</p> <p>Here are the most important best practices to follow:</p> <ul> <li>Great README</li> <li>Clearly defined API</li> <li>Accessible API documentation</li> <li>Clean JAR file</li> <li>Continuous integration</li> <li>Automated deployment</li> <li>Versioning</li> </ul> <p>Some of these best practices also apply to applications, but this post focuses on libraries.</p> <p>If you learn how to make awesome libraries, you can build open source codebases that are used by people around the world and private repos that are beloved by your company.</p>"},{"location":"scala/library-best-practices/#ultimate-litmus-test-for-a-library","title":"Ultimate litmus test for a library","text":"<p>A user should be able to review the library README (and associated links) and use the library you've developed to add business value without any individual coaching or training. This litmus test applies to both open source and private libraries.</p> <p>Build a great library, add some marketing, and you'll be piling up GitHub stars like tj and lihaoyi.</p> <p>Let's examine characteristics of well designed Scala libraries in more detail.</p>"},{"location":"scala/library-best-practices/#great-open-source-library-example","title":"Great open source library example","text":"<p>utest is a great example of a library that's easily adoptable.</p> <p>The README gives a short overview of how to use the library, a detailed description of key functionality, and a great explanation on why uTest is better than other testing libraries.</p> <p>A Scala programmer can easily add utest to their <code>build.sbt</code> file as a dependency and start writing tests based on the README examples.</p> <p>You don't need to meet with Li to understand how to use his library. He's already given you everything you need.</p> <p>Private repos should meet the same standard of excellence. New hires should be able to consult your private README and start using your library, even if there is a ton of domain specific context.</p>"},{"location":"scala/library-best-practices/#great-readme","title":"Great README","text":"<p>The README should start out by explaining why the library will add value for the user.</p> <p>spark-testing-base is a great example of a library that jumps right in and immediately tells the user how it adds value.</p> <p>The README should also explain how to install your library, a short overview of the code, a detailed overview and how to contribute.</p> <p>READMEs are generally too short and don't make a sales pitch to attract users. Don't be shy. Tell users why your library will make their lives better.</p>"},{"location":"scala/library-best-practices/#clearly-defined-api","title":"Clearly defined API","text":"<p>Libraries should have a clearly defined public interface.</p> <p>The <code>private</code> keyword should be used to differentiate implementation details from end user functions.</p> <p>Great library developers obssess over building public interfaces that are clean and intuitive. This blog post on uJson shows the thought process of a Scala developer that's fighting to create the best Scala JSON library possible.</p> <p>You can preliminarily assess the quality of a library by searching for the <code>private</code> keyword. It's a red flag if a library isn't using <code>private</code> extensively.</p>"},{"location":"scala/library-best-practices/#accessible-api-documentation","title":"Accessible API documentation","text":"<p>The programatic API documentation should be easily accessible via a README link.</p> <p>The Spark API docs are a great example. Your API docs should look a lot like the Spark API docs.</p> <p>Custom documentation like scalafmt, mill, and sbt is a perfectly acceptable replacement for bigger projects.</p> <p>Empathize with your users. Give them materials that'll make it easy for them to adopt and quickly derive value from your library.</p> <p><code>sbt doc</code> will exclude all methods that are flagged as <code>private</code>. Developers that care about providing their users with clean documentation are also the developers that work hard to build beautiful public interfaces.</p>"},{"location":"scala/library-best-practices/#limit-dependencies","title":"Limit dependencies","text":"<p>Limiting dependencies is so important that some libraries use it as a selling point.</p> <p>The uPickle library is marketed as \"a simple, fast, dependency-free JSON &amp; Binary (MessagePack) serialization library for Scala\".</p> <p>Users can add uPickle to their project without worrying about a bunch of other dependencies being added.</p> <p>Users like dependency-free libraries, so they can avoid dependency hell.</p> <p>Vendoring dependencies is another way to avoid build file dependencies (another technique to bypass downstream dependency hell).</p> <p>The utest vendors Fansi to avoid having a dependency.</p> <p>Note that uTest uses an internal copy of the Fansi library, vendored at utest.ufansi, in order to avoid any compatibility problems with any of your other dependencies. You can use ufansi to construct the colored ufansi.Strs that these methods require, or you could just return colored java.lang.String objects containing ANSI escapes, created however you like, and they will be automatically parsed into the correct format.</p> <p>Fansi is a single file project, so they is no reason to add it as a dependency to a core library like utest.</p> <p>Fansi is also vendored in the spark-fast-tests project. If Fanzi wasn't vendored, an application that depends on utest, spark-fast-tests, and Fanzi directly could create a dependency hell situation for users. Strategic vendoring saves users from this headache.</p> <p>Library dependencies are sometimes unavoidable. For example, Spark depends on FasterXML libraries. You should only add a dependency to a library when these criteria are met:</p> <ol> <li>The dependency saves you from significantly reinventing the wheel. Don't forget about the NPM left-pad debacle a few years back where a developer deleted a package with 11 lines of code which ended up breaking a lot of websites. You shouldn't depend on a library that only provides a tiny bit of functionality.</li> <li>You've inspected the dependencies of the dependency you're considering adding. The NPM left pad incident broke the internet because React dependended on a library that depended on left-pad (React had a \"transitive dependency\"). React didn't depend on left-pad directly, but it shouldn't have had a transitive dependency on such a trivial library.</li> </ol> <p>Library dependency decisions can create dependency hell for downstream users.</p>"},{"location":"scala/library-best-practices/#clean-jar-file","title":"Clean JAR file","text":"<p>The JAR file you distribute to users should only include the files you want to distribute. Your code might depend on Spark, Scala, and Scalatest, but that doesn't necessarily mean that those files should be inlucded in your JAR file.</p> <p>Spark and Scala are typically marked as \"provided\" dependencies and Scalatest is a \"test\" dependency. You should flag your dependencies accordingly. sbt-assembly doesn't include provided or test depenencies in the JAR file by default.</p> <p>You should run <code>jar tvf</code> and inspect the contents of your JAR file to make sure your build file only includes the right files.</p> <p>You should also consider shading certain dependencies when building JAR files.</p>"},{"location":"scala/library-best-practices/#continuous-integration","title":"Continuous integration","text":"<p>You should setup you libraries to run the test suite whenever code is merged with master.</p> <p>Continuous integration services are free for open source project and worth the money for private repos.</p> <p>The build status badge should be displayed prominently in the README.</p> <p>Some library maintainers can get in a bad habit of breaking the build and letting the repo sit in a failed state for days or weeks. This is really bad. Whenever a build is broken, you should take immediate action to fix it.</p>"},{"location":"scala/library-best-practices/#automated-deployment","title":"Automated deployment","text":"<p>You should have a single command that's documented in the README to deploy your project. Here's an example of what a deploy script can perform for an open source project:</p> <ul> <li>Build the JAR file</li> <li>Create a GitHub release with the JAR file attached</li> <li>Upload the JAR to Maven</li> <li>Build the new documentation and upload it to GitHub pages</li> </ul> <p>Maintaining libraries can be difficult and tedious\u2026 especially when years have passed and the bug reports keep flowing in. Fully automated deploy processes make maintenance less painful.</p>"},{"location":"scala/library-best-practices/#versioning","title":"Versioning","text":"<p>Most libraries should stricly follow Semantic Versioning.</p> <p>Your users should feel comfortable upgrading to a new minor version without any backwards incompatible changes.</p> <p>Some Scala library developers seem to randomly apply Semantic Versioning. They bump the PATCH version for \"minor changes\" and bump the MINOR version for \"medium sized changes\". It's better to tell users that the project follows Semantic Versioning in the README and then follow the guidlines strictly.</p> <p>CalVer is another popular versioning scheme. Let me know if you know of any popular Scala projects that use CalVer.</p>"},{"location":"scala/library-best-practices/#good-scala-libraries-add-a-tremendous-amount-of-value","title":"Good Scala libraries add a tremendous amount of value","text":"<p>You can help people from all over the world by developing a good Scala library.</p> <p>spark-daria allowed me to collaborate with smart Spark developers in India, Armenia, Spain, China, and other countries. I was able to meet many of these collaborators at Spark Summits in San Francisco and Amsterdam.</p> <p>The Scala community is supportive and participating is rewarding. I highly recommend building some libraries and seeing for yourself!</p>"},{"location":"scala/library-best-practices/#what-other-best-practices-should-scala-projects-follow","title":"What other best practices should Scala projects follow?","text":"<p>Ping me in the comments or email me if you have other suggestions for library best practices.</p>"},{"location":"scala/maintenance-nightmare-upgrade/","title":"Scala is a Maintenance Nightmare","text":"<p>This post explains why Scala projects are difficult to maintain.</p> <p>Scala is a powerful programming language that can make certain small teams hyper-productive.</p> <p>Scala can also slow productivity by drowning teams in in code complexity or burning them in dependency hell.</p> <p>Scala is famous for crazy, complex code - everyone knows about that risk factor already.</p> <p>The rest of this post focuses on the maintenance burden, a less discussed Scala productivity drain.</p> <p>This post generated lots of comment on Hackernews and Reddit. The Scala subreddit made some fair criticisms of the article, but the main points stand.</p> <p>Li tweeted the post and empathizes with the difficulties of working in the Scala ecosystem.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#cross-compiling-scala-libs","title":"Cross compiling Scala libs","text":"<p>Scala libraries need to be cross compiled with different Scala versions. utest v0.7.7 publishes separate JAR files for Scala 2.11, Scala 2.12, and Scala 2.13 for example.</p> <p></p> <p>Scala 2.12 projects that depend on utest v0.7.7 need to grab the JAR file that's compiled with Scala 2.12. Scala 2.10 users can't use utest v0.7.7 because there isn't a JAR file.</p> <p>Minor versions are compatible in most languages. Python projects that are built with Python 3.6 are usable in Python 3.7 projects for example.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#scala-itself-is-unstable","title":"Scala itself is unstable","text":"<p>Scala doesn't use <code>major.minor.patch</code> versioning as described in semver. It uses the pvp <code>epoch.major.minor</code> versioning.</p> <p>So Scala 2.11 =&gt; Scala 2.12 is a major release. It's not a minor release!</p> <p>Scala major releases are not backwards compatible. Java goes to extreme lengths to maintain backwards compatibility, so Java code that was built with Java 8 can be run with Java 14.</p> <p>That's not the case with Scala. Scala code that was built with Scala 2.11 cannot be run with Scala 2.12.</p> <p>Most of the difficulty of maintaining Scala apps stems from the frequency of major releases.</p> <p>Scala 2.0 was released in March 2006 and Scala 2.13 was released in June 2019. That's 13 major releases in 13 years!</p> <p>Migrating to Scala 2.12 or Scala 2.13 can be hard. These major version bumps can be trivial for some projects and really difficult for others.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#some-libs-drop-scala-versions-too-early","title":"Some libs drop Scala versions too early","text":"<p>Some dependencies force you to use old Scala versions that other libraries might stop supporting.</p> <p>The Databricks platform didn't start supporting Scala 2.12 till Databricks Runtime 7 was released in June 2020. Compare this with the Scala release dates:</p> <ul> <li>Scala 2.11: April 2014</li> <li>Scala 2.12: November 2016</li> <li>Scala 2.13: June 2019</li> </ul> <p>There was a full year where much of the Scala community had switched to Scala 2.13 and the Spark community was still stuck on Scala 2.11. That's a big gap, especially when you think of Scala minor versions as being akin to major version differences in other languages.</p> <p>Many Scala projects dropped support for Scala 2.11 long before Spark users were able to upgrade to Scala 2.12.</p> <p>Spark devs frequently needed to search the Maven page for a project and look for the latest project for the Scala version they are using.</p> <p></p>"},{"location":"scala/maintenance-nightmare-upgrade/#abandoned-libs","title":"Abandoned libs","text":"<p>Open source libraries are often abandoned, especially in Scala.</p> <p>Open source maintainers get tired or shift to different technology stacks. Lots of folks rage quit Scala too (another unique factor of the Scala community).</p> <p>Generic libraries are usable long after they're abandoned in many languages.</p> <p>Scala open source libs aren't usable for long after they're abandoned. Take spark-google-spreadsheets for example.</p> <p>The project isn't maintained anymore and all the JAR files are for Scala 2.10 / Scala 2.11.</p> <p></p> <p>Suppose you have a Spark 2 / Scala 2.11 project that depends on spark-google-spreadsheets and would like to upgrade to Spark 3 / Scala 2.12. The spark-google-spreadsheet dependency will prevent you from doing the upgrade. You'll need to either fork the repo, upgrade it, and publish it yourself or vendor the code in your repo.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#difficult-to-publish-to-maven","title":"Difficult to publish to Maven","text":"<p>Publishing open source project to Maven is way more difficult than most language ecosystems.</p> <p>The sbt-ci-release project provides the best overview of the steps to publish a project to Maven.</p> <p>You need to open a JIRA ticket to get a namespace, create GPG keys, register keys in a keyserver, and add SBT plugins just to get a manual publishing process working. It's a lot more work than publishing to PyPI or RubyGems.</p> <p>Adopting an unmaintained project and getting it published to Maven yourself is challenging. To be fair, this is equally challenging for any JVM language.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#properly-publishing-libs","title":"Properly publishing libs","text":"<p>Knowing how to properly publish libraries is difficult as well. Deequ is a popular Spark library for unit testing data that's fallen into the trap of trying to publish a variety of JAR files for different combinations of Scala / Spark versions per release.</p> <p></p> <p>Build matrices are difficult to maintain, especially if you want different combinations of code for different cells in the build matrix.</p> <p>The Deequ matrix somehow slipped up and has Scala 2.11 dependencies associated with the Spark 3 JAR. Not trying to single out Deequ, just showing how well funded, popular projects can even get tripped up when dealing with Scala publishing complexity.</p> <p>The Delta Lake project use a maintainable release process that avoids the build matrix. The README includes this disclaimer: \"Starting from 0.7.0, Delta Lake is only available with Scala version 2.12\".</p> <p>Take a look at the Delta Lake JAR files in Maven:</p> <p></p> <p>This shifts the burden of selecting the right project version to the library user.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#sbt","title":"SBT","text":"<p>Most Scala projects are built with SBT.</p> <p>Li detailed the problems with SBT and created a new solution, but new projects are still being built with SBT.</p> <p>SBT is a highly active project with more than total 10,000 commits and new features are added frequently.</p> <p>Scala project maintainers need to track the SBT releases and frequently upgrade the SBT version in their projects. Most SBT releases are backwards compatible thankfully.</p> <p>The Scala community should be grateful for @eed3si9n's tireless effort on this project.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#sbt-plugins","title":"SBT plugins","text":"<p>The SBT plugin ecosystem isn't as well maintained. SBT plugins are versioned and added, separate from regular library dependencies, so SBT projects have two levels of dependency hell (regular dependencies and SBT plugins).</p> <p>SBT plugins can't be avoided entirely. You need to add them to perform basic operations like building fat JAR files (sbt-assembly) or publishing JAR files to Maven ([sbt-sonatype]https://github.com/xerial/sbt-sonatype() and sbt-pgp).</p> <p>tut is an example of a plugin that was deprecated and required maintenance action. At least they provided a migration guide.</p> <p>It's best to avoid SBT plugins like the plague (unless you like doing maintenance).</p>"},{"location":"scala/maintenance-nightmare-upgrade/#breaking-changes-scalatest","title":"Breaking changes (Scalatest)","text":"<p>Scalatest, the most popular Scala testing framework, broke existing import statements in the 3.2 release (previous version of this article incorrectly stated that the breakage started in the 3.1 release). Users accustomed to libraries that follow semantic versioning were surprised to see their code break when performing a minor version bump.</p> <p>The creator of Scalatest commented on this blog (highly recommend reading his comment) and said \"My understanding of the semantic versioning spec has been that it does allow deprecation removals in a minor release\".</p> <p>Semver states that \"Major version MUST be incremented if any backwards incompatible changes are introduced to the public API\". Deleting public facing APIs is a backwards incompatible change.</p> <p>Semver specifically warns against backwards incompatible changes like these:</p> <p>Incompatible changes should not be introduced lightly to software that has a lot of dependent code. The cost that must be incurred to upgrade can be significant. Having to bump major versions to release incompatible changes means you\u2019ll think through the impact of your changes, and evaluate the cost/benefit ratio involved.</p> <p>Scalatest should have made a major version bump if they felt strongly about this change (unless they don't follow semver).</p> <p>There is an autofix for this that's available via SBT, Mill, and Maven. I personally don't want to install a plugin to fix import statements in my code. In any case, this supports the argument that maintaining Scala is work.</p> <p>Spark and popular testing libraries like spark-testing-base depend on Scalatest core classes. spark-testing-base users won't be able to use the latest version of Scalatest.</p> <p>What should spark-testing-base do? They already have a two dimensional build matrix for different versions of Scala &amp; Spark. Should they make a three dimensional build matrix for all possible combinations of Scala / Spark / Scalatest? spark-testing-base already has 592 artifacts in Maven.</p> <p>There are no good solutions here. Stability and backwards compatibility is impossible when multiple levels of core language components are all making breaking changes.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#when-is-scala-a-suitable-language","title":"When is Scala a suitable language","text":"<p>This maintenance discussion might have you thinking \"why would anyone ever use Scala?\"</p> <p>Scala is really only appropriate for difficult problems, like building compilers, that benefit from powerful Scala programming features.</p> <p>As Reynolds mentioned, Scala is a good language for Spark because Catalyst and Tungsten rely heavily on pattern matching.</p> <p>Scala should be avoided for easier problems that don't require advance programming language features.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#building-relatively-maintainable-scala-apps","title":"Building relatively maintainable Scala apps","text":"<p>Even the most basic of Scala apps require maintenance. Upgrading minor Scala versions can cause breaking changes and SBT versions need to be bumped regularly.</p> <p>Scala projects with library dependencies are harder to maintain. Make sure you depend on libraries that are actively maintained and show a pattern of providing long term support for multiple Scala versions.</p> <p>Prioritize dependency free libraries over libs that'll pull transitive dependencies into your projects. Dependency hell is painful in Scala.</p> <p>Go to great lengths to avoid adding library dependencies to your projects. Take a look at the <code>build.sbt</code> file of one of my popular Scala libraries and see that all the dependencies are test or provided. I would rather write hundreds of lines of code than add a dependency, expecially to a library.</p> <p>Don't use advanced SBT features. Use the minimal set of features and try to avoid multi-project builds.</p> <p>Use the minimal set of SBT plugins. It's generally better to skip project features than to add a SBT plugin. I'd rather not have a microsite associated with an open source project than add a SBT plugin. I'd definitely rather not add a SBT plugin to rename Scalatest classes for major breaking changes that happened in a minor release.</p> <p>The Scalatest Semantic Versioning infraction motivated me to shift project to utest and munit. Grateful for Scalatest's contributions to the Scala community, but want to avoid the pain of multiple redundant ways of doing the same thing.</p> <p>Shifting off SBT and using Mill isn't as easy. Li's libs seem to be the only popular ones to actually be using Mill. The rest of the community is still on SBT.</p> <p>Most devs don't want to learn another build tool, so it'll be hard for Mill to get market share. I would build Mill projects, but think it'd hinder open source contributions cause other folks don't want to learn another build tool.</p>"},{"location":"scala/maintenance-nightmare-upgrade/#conclusion","title":"Conclusion","text":"<p>Scala is a powerful programming language that can make small teams highly productive, despite the maintenance overhead. All the drawbacks I've mentioned in this post can be overcome. Scala is an incredibly powerful tool.</p> <p>Scala can also bring out the weirdness in programmers and create codebases that are incredibly difficult to follow, independent of the maintenance cost. Some programmers are more interested in functional programming paradigms and category theory than the drudgery of generating business value for paying customers.</p> <p>The full Scala nightmare is the double whammy of difficult code that's hard to modify and a high maintenance cost. This double whammy is why Scala has a terrible reputation in many circles.</p> <p>Scala can be a super power or an incredible liability that sinks an organization. At one end of the spectrum, we have Databricks, a 28 billion company that was build on Scala. At the other end of the spectrum, we have an ever growing graveyard of abandoned Scala libraries.</p> <p>Only roll the dice and use Scala if your team is really good enough to outweigh the Scala maintenance burden costs.</p>"},{"location":"scala/read-write-json/","title":"Reading and writing JSON with Scala","text":"<p>This blog post explains how to read and write JSON with Scala using the uPickle / uJSON library.</p> <p>This library makes it easy to work with JSON files in Scala.</p> <p>The Scala JSON ecosystem is disjointed. Many popular Scala JSON libraries are hard to use.</p> <p>Luckily for us, we don't need to do an exaustive analysis of all the libraries and figure out what's best. Li has already demonstrated that the alternative JSON libraries have unintuitive the user interfaces.</p> <p>Thankfully, Li created a clean solution, so we don't need to suffer.</p>"},{"location":"scala/read-write-json/#adding-the-library","title":"Adding the library","text":"<p>Add this to your <code>build.sbt</code> file:</p> <pre><code>libraryDependencies += \"com.lihaoyi\" %% \"upickle\" % \"0.9.5\"\n</code></pre> <p>This dependency adds both the <code>upickle</code> and <code>ujson</code> packages to your project in one fell swoop.</p>"},{"location":"scala/read-write-json/#reading-json-file","title":"Reading JSON file","text":"<p>Suppose you have the following JSON file:</p> <pre><code>{\n  \"first_name\": \"Phil\",\n  \"last_name\": \"Hellmuth\",\n  \"birth_year\": 1964\n}\n</code></pre> <p>Here's how you can read this JSON data into a <code>LinkedHashMap</code>:</p> <pre><code>val jsonString = os.read(os.pwd/\"src\"/\"test\"/\"resources\"/\"phil.json\")\nval data = ujson.read(jsonString)\ndata.value // LinkedHashMap(\"first_name\" -&gt; Str(\"Phil\"), \"last_name\" -&gt; Str(\"Hellmuth\"), \"birth_year\" -&gt; Num(1964.0))\n</code></pre> <p>The os-lib library is used to construct the path and read the file, as detailed here.</p> <p>We can fetch the <code>first_name</code> value as follows:</p> <pre><code>data(\"first_name\") // ujson.Value = Str(\"Phil\")\ndata(\"first_name\").str // String = \"Phil\"\ndata(\"first_name\").value // Any = \"Phil\"\n</code></pre> <p>You need to fetch the value correctly to get the correct result type.</p>"},{"location":"scala/read-write-json/#writing-json-file","title":"Writing JSON file","text":"<p>Let's change the last name from the <code>phil.json</code> file to \"Poker Brat\" and then write the updated JSON to disk.</p> <pre><code>val jsonString = os.read(os.pwd/\"src\"/\"test\"/\"resources\"/\"phil.json\")\nval data = ujson.read(jsonString)\ndata(\"last_name\") = \"Poker Brat\"\nos.write(os.pwd/\"tmp\"/\"poker_brat.json\", data)\n</code></pre> <p>Here are the contents of the <code>tmp/poker_brat.json</code> file:</p> <pre><code>{\"first_name\":\"Phil\",\"last_name\":\"Poker Brat\",\"birth_year\":1964}\n</code></pre> <p>This example demonstrates that <code>ujson</code> objects are mutable.</p> <p>Some Scala JSON libraries try to stick with immutable data structures, but that forces inconvenient user interfaces that aren't as performant. uJSON made a concious design decision to use mutable data structures, so the user interface is intuitive.</p>"},{"location":"scala/read-write-json/#reading-json-with-an-array","title":"Reading JSON with an array","text":"<p>Suppose you have the following <code>colombia.json</code> file:</p> <pre><code>{\n  \"continent\": \"South America\",\n  \"cities\": [\"Medellin\", \"Cali\", \"Bogot\u00e1\"]\n}\n</code></pre> <p>You can read this JSON file as follows:</p> <pre><code>val jsonString = os.read(os.pwd/\"src\"/\"test\"/\"resources\"/\"colombia.json\")\nval data = ujson.read(jsonString)\n</code></pre> <p>You can interact with the cities array as follows:</p> <pre><code>data(\"cities\") // ujson.Value = Arr(ArrayBuffer(Str(\"Medellin\"), Str(\"Cali\"), Str(\"Bogot\\u00e1\")))\ndata(\"cities\").value // Any = ArrayBuffer(Str(\"Medellin\"), Str(\"Cali\"), Str(\"Bogot\\u00e1\"))\ndata(\"cities\").arr // collection.mutable.ArrayBuffer[ujson.Value] = ArrayBuffer(Str(\"Medellin\"), Str(\"Cali\"), Str(\"Bogot\\u00e1\"))\ndata(\"cities\").arr.map(_.str) // collection.mutable.ArrayBuffer[String] = ArrayBuffer(\"Medellin\", \"Cali\", \"Bogot\\u00e1\")\n</code></pre> <p>A mutable collection is returned, so you can easily modify the array.</p>"},{"location":"scala/read-write-json/#writing-json-with-an-array","title":"Writing JSON with an array","text":"<p>Let's read the <code>colombia.json</code> file, add a city to the array, and write it out as a separate JSON file.</p> <pre><code>val jsonString = os.read(os.pwd/\"src\"/\"test\"/\"resources\"/\"colombia.json\")\nval data = ujson.read(jsonString)\ndata(\"cities\").arr.append(\"Cartagena\")\nos.write(os.pwd/\"tmp\"/\"more_colombia.json\", data)\n</code></pre> <p>Here are the contents of the <code>more_colombia.json</code> file:</p> <pre><code>{\"continent\":\"South America\",\"cities\":[\"Medellin\",\"Cali\",\"Bogot\u00e1\",\"Cartagena\"]}\n</code></pre> <p>uJSON makes it easy to modify an array and write out a JSON file.</p>"},{"location":"scala/read-write-json/#creating-json-in-memory","title":"Creating JSON in memory","text":"<p>We've been constructing uJSON objects by reading files on disk. Let's built a JSON object in memory and then write it out to disk.</p> <pre><code>val brasil = ujson.Obj(\"population\" -&gt; \"210 million\")\nbrasil(\"cities\") = ujson.Arr(\"recife\", \"sao paolo\")\nos.write(os.pwd/\"tmp\"/\"brasil.json\", brasil)\n</code></pre> <p>Here are the contents of the <code>brasil.json</code> file:</p> <pre><code>{\"population\":\"210 million\",\"cities\":[\"recife\",\"sao paolo\"]}\n</code></pre>"},{"location":"scala/read-write-json/#json-ecosystem-messiness","title":"JSON ecosystem messiness","text":"<p>json4s aims to create a single Abstract Syntax Tree for Scala JSON libraries because \"there are at least 6 JSON libraries for Scala, not counting the Java JSON libraries\".</p> <p>There was a native <code>scala.util.parsing</code> package with JSON parsing capabilities, but it was removed from the standard library in Scala 2.11. You can access this package with a separate import, but <code>scala.util.parsing.json.JSON</code> is deprecated as of Scala 2.13. Needless to say, stay away from this package.</p> <p>It's fitting for new Scala JSON libraries like circe self identify as \"Yet another JSON library for Scala\".</p>"},{"location":"scala/read-write-json/#conclusion","title":"Conclusion","text":"<p>os-lib and ujson make it easy to read and write JSON files with Scala.</p> <p>It's painful to work with JSON and Scala without these libraries.</p> <p>Read this article or Hands on Scala Programming to learn more details about how ujson is implemented and other use cases.</p> <p>Notice how Li created a Python-like / Ruby-like clean interface for this library. His libraries always try to port elegant user inferfaces from popular Python libraries to Scala.</p> <p>You should strive to build elegant user interfaces when building Scala libraries. If you want to build a popular open source Scala project, take a popular Python project, and port it over to Scala with an identical user interface.</p> <p>Notice that ujson does not have a complicated import interface. You can type <code>ujson.read(jsonString)</code> without doing any imports. Always strive to provide minimalistic import interfaces for your users. Complicated imports are indicative of leaky abstractions.</p>"},{"location":"scala/scalate-templates-mustache-ssp/","title":"Scala Templates with Scalate, Mustache, and SSP","text":"<p>The scalate library makes it easy to use Mustache or SSP templates with Scala.</p> <p>This blog post will show how to use Mustache and SSP templates and compares the different templating philosophies (Mustache is logic-less and SSP templates contain logic).</p> <p>There are pros / cons to the different templating styles. Luckily scalate makes it easy for you to use either style.</p>"},{"location":"scala/scalate-templates-mustache-ssp/#simple-mustache-example","title":"Simple Mustache example","text":"<p>Suppose you have the following <code>simple_example.mustache</code> file:</p> <pre><code>I like {{programming_language}}\nThe code is {{code_description}}\n</code></pre> <p>Here's how to generate a file with <code>programming_language</code> set to \"Scala\" and <code>code_description</code> set to \"pretty\".</p> <pre><code>import org.fusesource.scalate.TemplateEngine\n\nval sourceDataPath = new java.io.File(\"./src/test/resources/simple_example.mustache\").getCanonicalPath\nval engine = new TemplateEngine\nval someAttributes = Map(\n  \"programming_language\" -&gt; \"Scala\",\n  \"code_description\" -&gt; \"pretty\"\n)\nengine.layout(sourceDataPath, someAttributes)\n</code></pre> <p>Here's the string that's returned:</p> <pre><code>I like Scala\nThe code is pretty\n</code></pre>"},{"location":"scala/scalate-templates-mustache-ssp/#add-library-to-sbt","title":"Add library to SBT","text":"<p>You need to add the following line to your <code>build.sbt</code> file to access the scalate library:</p> <pre><code>libraryDependencies += \"org.scalatra.scalate\" %% \"scalate-core\" % \"1.9.6\"\n</code></pre> <p>The <code>import org.fusesource.scalate.TemplateEngine</code> import statement and <code>org.scalatra.scalate</code> package name aren't aligned like they are for most projects. Just keep that in mind when you're using the library.</p>"},{"location":"scala/scalate-templates-mustache-ssp/#more-mustache-examples","title":"More Mustache examples","text":""},{"location":"scala/scalate-templates-mustache-ssp/#lists","title":"Lists","text":"<p>Let's look at how to render a list of values via a Mustache template. Suppose you'd like to display a bulleted list of popular Scala projects.</p> <p>Here's a Mustache template that'll display a list of values:</p> <pre><code>{{#popular_projects}}\n  &lt;b&gt;{{name}}&lt;/b&gt;\n{{/popular_projects}}\n</code></pre> <p>And here is the Scala code:</p> <pre><code>val sourceDataPath = new java.io.File(\"./src/test/resources/scala_projects.mustache\").getCanonicalPath\nval engine = new TemplateEngine\nval someAttributes = Map(\n  \"popular_projects\" -&gt; List(\n    Map(\"name\" -&gt; \"Spark\"),\n    Map(\"name\" -&gt; \"Play\"),\n    Map(\"name\" -&gt; \"Akka\")\n  )\n)\nengine.layout(sourceDataPath, someAttributes)\n</code></pre> <p>It'll return this string:</p> <pre><code>&lt;b&gt;Spark&lt;/b&gt;\n&lt;b&gt;Play&lt;/b&gt;\n&lt;b&gt;Akka&lt;/b&gt;\n</code></pre>"},{"location":"scala/scalate-templates-mustache-ssp/#boolean-values","title":"Boolean values","text":"<p>You can include boolean values to imitate if statement logic.</p> <p>Here's a Mustache template that'll add \"probably likes functional programming\" to a list if the <code>loves_scala?</code> flag is set to true.</p> <pre><code>**nerd profile**\n* applies unix philosophy to human interactions\n* dreams in binary\n{{#loves_scala?}}\n  * probably likes functional programming\n{{/loves_scala?}}\n</code></pre> <p>Here's Scala code that uses this template and outputs a string:</p> <pre><code>val sourceDataPath = new java.io.File(\"./src/test/resources/boolean_example.mustache\").getCanonicalPath\nval engine = new TemplateEngine\nval someAttributes = Map(\n  \"loves_scala?\" -&gt; true\n)\nprintln(engine.layout(sourceDataPath, someAttributes))\n</code></pre> <pre><code>**nerd profile**\n* applies unix philosophy to human interactions\n* dreams in binary\n* probably likes functional programming\n</code></pre>"},{"location":"scala/scalate-templates-mustache-ssp/#functions","title":"Functions","text":"<p>Mustache can even handle functions that take string arguments.</p> <p>Here's a Mustache template that includes a function.</p> <pre><code>select\n  {{#comma_delimited}}\n  {{column_names}}\n{{/comma_delimited}}\nfrom\n  {{table_name}}\n</code></pre> <p>Here's the Scala code that executes the function (it assumes the column names are passed in as a pipe delimited string).</p> <pre><code>val sourceDataPath = new java.io.File(\"./src/test/resources/function_example.mustache\").getCanonicalPath\nval engine = new TemplateEngine\nval someAttributes = Map(\n  \"column_names\" -&gt; \"first_name|last_name|age\",\n  \"comma_delimited\" -&gt; ((cols: String) =&gt; cols.replaceAll(\"\\\\|\", \",\")),\n  \"table_name\" -&gt; \"fun_people\"\n)\nprintln(engine.layout(sourceDataPath, someAttributes))\n</code></pre> <p>Here's what the code outputs:</p> <pre><code>select\n  first_name,last_name,age\nfrom\n  fun_people\n</code></pre> <p>From what I can tell, the functions can only accept a single string argument. They don't work if you supply a sequence of strings for example.</p> <p>The Mustache template was intentionally formatted strangely to output the SQL code with proper indentation.</p>"},{"location":"scala/scalate-templates-mustache-ssp/#ssp","title":"SSP","text":"<p>SSP is another templating framework, similar to ERB in Ruby.</p> <p>SSP does not follow the \"logic-less templates\" philosophy used by Mustache. You can add lots of logic to a SSP template.</p> <p>Let's define an object that we'll access in our SSP template:</p> <pre><code>object FunStuff {\n  val dinnertime = \"eating stuff!\"\n}\n</code></pre> <p>Let's create a <code>simple_example.ssp</code> file that contains some arbitrary code and also accesses the <code>FunStuff</code> object.</p> <pre><code>&lt;% import mrpowers.scalate.example.FunStuff  %&gt;\n&lt;p&gt;\n    My message is \"&lt;%= List(\"hi\", \"there\", \"reader!\").mkString(\" \") %&gt;\"\n    At dinnertime, I like &lt;%= FunStuff.dinnertime %&gt;\n&lt;/p&gt;\n</code></pre> <p>Notice that the template needs to import the <code>FunStuff</code> object.</p> <p>Let's write some Scala code that'll use the template to generate a string.</p> <pre><code>val sourceDataPath = new java.io.File(\"./src/test/resources/simple_example.ssp\").getCanonicalPath\nval engine = new TemplateEngine\nprintln(engine.layout(sourceDataPath))\n</code></pre> <p>Here's the string that's returned:</p> <pre><code>&lt;p&gt;\n    My message is \"hi there reader!\"\n    At dinnertime, I like eating stuff!\n&lt;/p&gt;\n</code></pre> <p>scalate is intelligent enough to recognize that the file extension is <code>ssp</code>, so the code should be processed with the SSP templating engine.</p>"},{"location":"scala/scalate-templates-mustache-ssp/#templating-philosophies","title":"Templating philosophies","text":"<p>Logic-less templates force you to keep logic out of the templates. This makes the templates approachable to folks that are comfortable modifying HTML / SQL logic, but aren't programmers. Templates with logic aren't approachable for non-coders.</p> <p>Here is a great blog on the benefits of logic-less templating.</p> <p>The author argues that in the MVC context, it's best to keep logic in the views and keep the templates simple (with only HTML / mustaches).</p>"},{"location":"scala/scalate-templates-mustache-ssp/#tips-on-picking-templating-languages","title":"Tips on picking templating languages","text":"<p>The creator of scalate has some great tips on picking a templating language in this slide deck - see slide 18.</p> <p>Key points:</p> <ul> <li>Use mustache if you'd like non-programmers to \"own the templates\"</li> <li>Use SSP if you'd like the option to do complex template hacking</li> </ul>"},{"location":"scala/scalate-templates-mustache-ssp/#other-template-options","title":"Other template options","text":"<p>scalate also supports Scaml (a Scala version of HAML) and Jade (another template engine inspired by HAML). There is a separate Scalatags library to build XML / HTML / CSS.</p> <p>These options are only applicable for HTML templating. I personally prefer writing HTML than using fancy templating engines.</p>"},{"location":"scala/scalate-templates-mustache-ssp/#conclusion","title":"Conclusion","text":"<p>Scala offers great support for all different types of templating.</p> <p>Templates are generally used to create HTML for websites, but they can also be used to generate SQL and other types of files.</p> <p>Logic-less templating encourages a good separation of logic and makes the templates more approachable for semi-technical users, which is often desirable.</p> <p>See the examples in this repo if you'd like working code snippets of all the code covered in this blog post.</p>"},{"location":"scala/scalatest/","title":"Testing Scala with Scalatest","text":"<p>Scalatest makes it easy to test your Scala code.</p> <p>This blog post shows how to add Scalatest to a sbt project and write some basic tests.</p>"},{"location":"scala/scalatest/#writing-a-simple-test","title":"Writing a simple test","text":"<p>Make a <code>Calculator</code> object with an <code>addNumbers</code> method that adds two integers:</p> <pre><code>package com.github.mrpowers.scalatest.example\n\nobject Calculator {\n\n  def addNumbers(n1: Int, n2: Int): Int = {\n    n1 + n2\n  }\n\n}\n</code></pre> <p>Let's verify that <code>addNumbers</code> returns 7 when the inputs are 3 and 4. Let's take a look at the test code.</p> <pre><code>package com.github.mrpowers.scalatest.example\n\nimport org.scalatest.FunSpec\n\nclass CalculatorSpec extends FunSpec {\n\n  it(\"adds two numbers\") {\n\n    assert(Calculator.addNumbers(3, 4) === 7)\n\n  }\n\n\n}\n</code></pre> <p>You can find all the code in this GitHub repo.</p> <p>You can run the tests with the <code>sbt test</code> command in your Terminal or by right clicking the method and pressing \"Run CalculatorSpec.adds\" in your IntelliJ text editor.</p> <p>Let's examine the important components of the test file:</p> <ul> <li><code>import org.scalatest.FunSpec</code> imports FunSpec from Scalatest. <code>FunSpec</code> is a trait that you can mix into your test files.</li> <li><code>FunSpec</code> defines an <code>it()</code> method to group tests into blocks and a <code>===</code> operator to provide readable test output</li> </ul> <p>Let's look at how to add Scalatest as a project dependency before diving into more advanced features.</p>"},{"location":"scala/scalatest/#directory-organization","title":"Directory organization","text":"<p>Scalatest works best when you follow SBT directory conventions. A project named <code>scalatest-example</code> in the <code>com.github.mrpowers</code> package should be organized as follows:</p> <pre><code>src/\n  main/\n    scala/\n      com/\n        github/\n          mrpowers/\n            scalatest/\n              example/\n                Calculator\n  test/\n    scala/\n      com/\n        github/\n          mrpowers/\n            scalatest/\n              example/\n                CalculatorSpec\n</code></pre> <p>IntelliJ does a great job formatting the nested directories so they're readable.</p> <p></p> <p>Both the application and test code live in the <code>com.github.mrpowers.scalatest.example</code> namespace. This allows our tests to easily access the application code without any special imports.</p>"},{"location":"scala/scalatest/#buildsbt","title":"build.sbt","text":"<p>Scalatest should be specified as a test dependency in your <code>build.sbt</code> file:</p> <pre><code>libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.0.1\" % \"test\"\n</code></pre> <p>You should mark Scalatest as a test dependency, so it's not included in your JAR files.</p>"},{"location":"scala/scalatest/#more-tests","title":"More tests","text":"<p>Let's create a <code>CardiB</code> object with a couple of methods.</p> <pre><code>object CardiB {\n\n  def realName(): String = {\n    \"Belcalis Almanzar\"\n  }\n\n  def iLike(args: String*): String = {\n    \"I like \" + args.mkString(\", \")\n  }\n\n}\n</code></pre> <p>Let's use the <code>describe()</code> and <code>it()</code> method provided by <code>FunSpec</code> to neatly organize our <code>CardiB</code> tests:</p> <pre><code>class CardiBSpec extends FunSpec {\n\n  describe(\"realName\") {\n\n    it(\"returns her birth name\") {\n      assert(CardiB.realName() === \"Belcalis Almanzar\")\n    }\n\n  }\n\n  describe(\"iLike\") {\n\n    it(\"works with a single argument\") {\n      assert(CardiB.iLike(\"dollars\") === \"I like dollars\")\n    }\n\n    it(\"works with multiple arguments\") {\n      assert(CardiB.iLike(\"dollars\", \"diamonds\") === \"I like dollars, diamonds\")\n    }\n\n  }\n\n}\n</code></pre> <p>You can use <code>sbt test</code> to run the entire test suite.</p> <p>Let's look at how to run individual test files and configure the test output.</p>"},{"location":"scala/scalatest/#running-tests-and-configuring-output","title":"Running tests and configuring output","text":"<p>Here's what the test output will look like when running the entire test suite from the Terminal:</p> <p></p> <p>We can run the <code>sbt</code> command to open the SBT console and run the tests from the console. The <code>testOnly</code> command can be used to run a single test file.</p> <p></p> <p>Let's update the <code>build.sbt</code> file with configuration options that will show the runtime for each individual test.</p> <pre><code>testOptions in Test += Tests.Argument(TestFrameworks.ScalaTest, \"-oD\")\n</code></pre> <p>Restart the console, rerun the test suite, and observe how the test output contains the runtime for each individual test.</p> <p></p> <p>There are a variety of test output settings that can be configured for your project needs.</p>"},{"location":"scala/scalatest/#assertthrows","title":"assertThrows","text":"<p>Let's update the <code>iLike()</code> method to throw an error if the argument list is empty:</p> <pre><code>def iLike(args: String*): String = {\n  if (args.isEmpty) {\n    throw new java.lang.IllegalArgumentException\n  }\n  \"I like \" + args.mkString(\", \")\n}\n</code></pre> <p>We can use <code>assertThrows</code> to verify that the exception is thrown when <code>iLike()</code> is run without any arguments:</p> <pre><code>it(\"throws an error if an integer argument is supplied\") {\n  assertThrows[java.lang.IllegalArgumentException]{\n    CardiB.iLike()\n  }\n}\n</code></pre> <p>We can also write tests to verify if code compiles.</p>"},{"location":"scala/scalatest/#assertdoesnotcompile","title":"assertDoesNotCompile","text":"<p>The <code>iLike()</code> method will error out with \"(Test / compileIncremental) Compilation failed\" when run with integer arguments. <code>CardiB.iLike(1, 2, 3)</code> will return an error like this:</p> <pre><code>sbt:scalatest-example&gt; testOnly *CardiBSpec\nCompiling 1 Scala source to /Users/powers/Documents/code/my_apps/scalatest-example/target/scala-2.12/test-classes ...\n.../CardiBSpec.scala:32:20: type mismatch;\nfound   : Int(1)\nrequired: String\n    CardiB.iLike(1, 2, 3)\n                 ^\n</code></pre> <p>Here's a test to verify that this code does not compile.</p> <pre><code>it(\"does not compile with integer arguments\") {\n  assertDoesNotCompile(\"\"\"CardiB.iLike(1, 2, 3)\"\"\")\n}\n</code></pre> <p>A test like this isn't necessary in a real codebase. We can rely on the Scala compiler to make sure methods are passed the right argument type, so we don't need to write compile cases like these in general. You'll find these test cases useful when you're checking to make sure your code doesn't compile for \"user errors\".</p>"},{"location":"scala/scalatest/#other-assertions","title":"Other assertions","text":"<p>Read the Scalatest guide on using assertions for a full review of all the different types of tests you can write.</p>"},{"location":"scala/scalatest/#other-test-formats","title":"Other test formats","text":"<p>Scalatest supports a variety of testing styles.</p> <p>The examples have been using FunSpec so far.</p> <p>Let's create another example with FreeSpec, another test style that's also defined in Scalatest.</p> <p>Start by defining a <code>Person</code> class with a <code>fullName()</code> method that concatenates the <code>firstName</code> and <code>lastName</code>.</p> <pre><code>class Person(firstName: String, lastName: String) {\n\n  def fullName(): String = {\n    firstName + \" \" + lastName\n  }\n\n}\n</code></pre> <p>Here's a <code>PersonSpec</code> test that leverages the <code>FreeSpec</code> test style.</p> <pre><code>import org.scalatest.FreeSpec\n\nclass PersonSpec extends FreeSpec {\n\n  \"fullName\" - {\n\n    \"returns the first name and last name concatenated\" in {\n\n      val lilXan = new Person(\"Lil\", \"Xan\")\n      assert(lilXan.fullName() === \"Lil Xan\")\n\n    }\n\n  }\n\n}\n</code></pre> <p>It's cool that Scalatest allows different test files in a given project to use different styles, but the flexibility can also be a burden.</p> <p>Who wants to work on a project that uses multiple test styles? Yuck!</p> <p>Other test frameworks have reacted to Scalatest's extreme flexibility with the express goal of providing only one way to write a test.</p> <p>Providing only one test style is the only way to guarantee consistency in the codebase.</p>"},{"location":"scala/scalatest/#test-library-alternatives","title":"Test library alternatives","text":""},{"location":"scala/scalatest/#utest","title":"uTest","text":"<p>uTest was created with a uniform syntax for defining tests and a single way of making assertions. Scalatest provides developers with lot of flexibility and options. uTest doesn't provide any options, so it's easier to focus on your tests and your code.</p> <p>The uTest syntax is based on the Scalatest <code>FreeSpec</code> style.</p> <p>We'll talk about uTest in more detail in a separate blog post.</p>"},{"location":"scala/scalatest/#munit","title":"MUnit","text":"<p>MUnit is a Scala testing library with actionable errors and extensible APIs. It offers a set of features that are not available in any other Scala testing libraries.</p> <p>MUnit is \"heavily inspired by existing testing libraries including ScalaTest, utest, JUnit and ava\" and combines a bunch of features in a unique way for a Scala testing library.</p> <p>We'll create a separate blog post to cover the MUnit testing library soon.</p>"},{"location":"scala/scalatest/#testing-spark-applications","title":"Testing Spark applications","text":"<p>This blog post explains how to test Spark applications with Scalatest.</p> <p>The spark-fast-tests library shows how to build a Spark testing library that's compatible with Scalatest. Sometimes the built-in equality operators aren't sufficient and you need an external library to provide some extra functionality.</p>"},{"location":"scala/scalatest/#next-steps","title":"Next steps","text":"<p>You can find all the code in this GitHub repo.</p> <p>Cloning the example code repo and running the tests on your local machine is a great way to learn about Scalatest.</p> <p>Make sure to setup your test suite to run on a continuous integration server whenever you push to master. Use scoverage to make sure that your entire codebase is executed when running the test suite.</p> <p>You can also look into mocking in Scala with mockito-scala to write some more advanced test cases.</p> <p>Getting good at writing tests is one of the best ways to become a better Scala programmer.</p>"},{"location":"scala/serializing-deserializing-case-classes/","title":"Serializing and Deserializing Scala Case Classes with JSON","text":"<p>This blog post shows how to serialize and deserialize Scala case classes with the JSON file format.</p> <p>Serialization is important when persisting data to disk or transferring data over the network.</p> <p>The upickle library makes it easy to serialize Scala case classes.</p>"},{"location":"scala/serializing-deserializing-case-classes/#serializing-case-classes-with-json","title":"Serializing case classes with JSON","text":"<p>Serializing an object means taking the data stored in an object and converting it to bytes (or a string).</p> <p>Suppose you want to write the data in an object to a JSON file. JSON files store strings. JSON has no understanding about the JVM or Scala objects.</p> <p>Serializing a Scala object for JSON storage means converting the object to a string and then writing it out to disk.</p> <p>Start by creating a case class and instantiating an object.</p> <pre><code>case class City(name: String, funActivity: String, latitude: Double)\nval bengaluru = City(\"Bengaluru\", \"South Indian food\", 12.97)\n</code></pre> <p>Define a upickle writer and then serialize the object to be a string.</p> <pre><code>implicit val cityRW = upickle.default.macroRW[City]\nupickle.default.write(bengaluru) // \"{\\\"name\\\":\\\"Bengaluru\\\",\\\"funActivity\\\":\\\"South Indian food\\\",\\\"latitude\\\":12.97}\"\n</code></pre> <p>Here's how to write the serialized object to disk.</p> <pre><code>os.write(\n  os.pwd/\"tmp\"/\"serialized_city.json\",\n  upickle.default.write(bengaluru)\n)\n</code></pre> <p>Here's the content of the <code>serialized_city.json</code> file:</p> <pre><code>{\"name\":\"Bengaluru\",\"funActivity\":\"South Indian food\",\"latitude\":12.97}\n</code></pre> <p>See here for more background information on how to perform Scala filesystem operations, like constructing paths and writing strings to disk.</p> <p>Congratulations, you've successfully persisted the data in a Scala object to a JSON file!</p>"},{"location":"scala/serializing-deserializing-case-classes/#deserializing-case-classes-with-json","title":"Deserializing case classes with JSON","text":"<p>Deserializing an object means reading data from a string / file to create a Scala object.</p> <p>Lets create a string and use it to build a <code>City</code> object.</p> <pre><code>val str = \"\"\"{\"name\":\"Barcelona\",\"funActivity\":\"Eat tapas\",\"latitude\":41.39}\"\"\"\nval barcelona = upickle.default.read[City](str)\nbarcelona.getClass // City\nbarcelona.latitude // 41.39\n</code></pre> <p>upickle does all the hard work of parsing the JSON string and instantiating the City object.</p> <p>Let's show how to deserialize a JSON file. Suppose you have the following <code>beirut.json</code> file.</p> <pre><code>{\"name\":\"Beirut\",\"funActivity\":\"Eat hummus\",\"latitude\":33.89}\n</code></pre> <p>Let's read in the JSON data and create a Scala object:</p> <pre><code>val path = os.pwd/\"src\"/\"test\"/\"resources\"/\"beirut.json\"\nval data = os.read(path)\nval beirut = upickle.default.read[City](data)\nbeirut.getClass // City\nbeirut.funActivity // \"Eat hummut\"\nbeirut.toString // \"City(Beirut,Eat hummus,33.89)\"\n</code></pre> <p>upickle makes it easy to convert a JSON string to a Scala object.</p>"},{"location":"scala/serializing-deserializing-case-classes/#next-steps","title":"Next steps","text":"<p>You've seen how it's easy to serialize and deserialize Scala case classes with JSON.</p> <p>You can also serialize / deserialize Scala case classes with other formats, like the MessagePack binary format.</p> <p>JSON is nice cause it's human readable. MessagePack is more efficient, but isn't human readable.</p> <p>Make sure to understand the high level concepts around object serialization and deserialization. It's a programming design pattern that you'll encounter in a variety of domains.</p>"},{"location":"scala/testing-munit/","title":"Testing Scala with MUnit","text":"<p>MUnit is a test library with a collection of features that are not available in other Scala testing libraries.</p> <p>MUnit draws inspiration from Scalatest, utest, and ava (JavaScript testing library).</p> <p>This blog post explains how to setup MUnit and use its basic features.</p>"},{"location":"scala/testing-munit/#simple-test","title":"Simple test","text":"<p>Let's create a <code>Calculator</code> object with an <code>addNumbers</code> method that adds two integers:</p> <pre><code>package com.github.mrpowers.munit.example\n\nobject Calculator {\n\n  def addNumbers(n1: Int, n2: Int): Int = {\n    n1 + n2\n  }\n\n}\n</code></pre> <p>Let's add a MUnit test to verify that <code>addNumbers</code> returns 7 when 3 and 4 are supplied as inputs.</p> <pre><code>package com.github.mrpowers.munit.example\n\nclass CalculatorSpec extends munit.FunSuite {\n\n  test(\"adds two numbers\") {\n\n    assertEquals(Calculator.addNumbers(3, 4), 7)\n\n  }\n\n}\n</code></pre> <p>We can run the test suite with <code>sbt test</code>.</p> <p>MUnit outputs the test with a well-formatted passing message.</p> <p></p> <p>All the code snippets in this blog are from this GitHub repo.</p> <p>Let's take a look at how MUnit outputs test failure messages.</p>"},{"location":"scala/testing-munit/#pretty-printing-assertequals-failures","title":"Pretty printing assertEquals failures","text":"<p>Here's an example of a failing test:</p> <pre><code>test(\"equality error message\") {\n  val obtained = 42\n  val expected = 43\n  assertEquals(obtained, expected)\n}\n</code></pre> <p>The failing message includes the obtained value, expected value, and line number of the failure.</p> <p></p>"},{"location":"scala/testing-munit/#comparing-collections","title":"Comparing collections","text":"<p>MUnit's <code>assertEquals</code> method can be used to compare collections that have the same elements:</p> <pre><code>test(\"considers collections with the same elements equal\") {\n  assertEquals(Seq(1, 2), Seq(1, 2))\n}\n</code></pre> <p>This functionality is similar to Scalatest should matchers and utest arrow asserts.</p> <p>MUnit gives nicely formatted error messages for collection comparisons that are not equal.</p> <p>Here's a test that compares two collections that aren't equal.</p> <pre><code>test(\"gives good error messages for different collections\") {\n  assertEquals(Seq(1, 2), Seq(1, 8))\n}\n</code></pre> <p>Here's the error message MUnit outputs.</p> <p></p>"},{"location":"scala/testing-munit/#intercept","title":"intercept","text":"<p>MUnit can also check that errors are thrown.</p> <p>Let's create a <code>crankyMethod</code> that throws a <code>java.lang.IllegalArgumentException</code>.</p> <pre><code>def crankyMethod(): String = {\n  throw new java.lang.IllegalArgumentException()\n}\n</code></pre> <p>Let's write a test using <code>intercept</code> to verify that this method throws a <code>IllegalArgumentException</code>:</p> <pre><code>test(\"can intercept exceptions\") {\n  intercept[java.lang.IllegalArgumentException]{\n    Calculator.crankyMethod()\n  }\n}\n</code></pre> <p><code>intercept</code> is especially useful when making sure your code is throwing custom errors for certain logic paths.</p>"},{"location":"scala/testing-munit/#interceptmessage","title":"interceptMessage","text":"<p>Write a <code>crankyKong</code> method that'll throw a <code>java.lang.Exception</code> with the message \"DK is my grandchild\".</p> <pre><code>def crankyKong(): String = {\n  throw new java.lang.Exception(\"DK is my grandchild\")\n}\n</code></pre> <p>Write a test that makes sure the <code>crankyKong</code> method throws a <code>java.lang.Exception</code> with the expected message.</p> <pre><code>test(\"can intercept exceptions with messages\") {\n  interceptMessage[java.lang.Exception](\"DK is my grandchild\"){\n    Calculator.crankyKong()\n  }\n}\n</code></pre> <p>Use <code>interceptMessage</code> instead of <code>intercept</code> whenever possible. You should always write code with error messages that are descriptive and help users debug the underlying issue. Your test suite should verify that your error messages are awesome and descriptive.</p>"},{"location":"scala/testing-munit/#compileerrors","title":"compileErrors","text":"<p>Our <code>Calculator.addNumbers()</code> method takes two integer arguments. The code won't compile if <code>addNumbers</code> is invoked with two String arguments.</p> <p>Let's write a test to make sure <code>addNumbers()</code> errors out with a particular message when it's invoked with String arguments.</p> <pre><code>test(\"checks the error message of code that doesn't compile\") {\n  assertNoDiff(\n    compileErrors(\"\"\"Calculator.addNumbers(\"hi\", \"there\")\"\"\"),\n    \"\"\"|error:\n       |type mismatch;\n       | found   : String(\"hi\")\n       | required: Int\n       |Calculator.addNumbers(\"hi\", \"there\")\n       |                      ^\n       |\"\"\".stripMargin\n  )\n}\n</code></pre> <p>Yep, MUnit is awesome!</p>"},{"location":"scala/testing-munit/#jumping-to-failing-section-of-code","title":"Jumping to failing section of code","text":"<p>MUnit shows the source location for errors in failing tests:</p> <p>Test failures point to the source code location where the failure happened. Cmd+click on the filename to open the relevant line number in your editor (does not work in all terminals).</p> <p>For example, if you're using iTerm &amp; MacOS, you can hold Command and click the path (e.g. /Users/powers/.../FailingSpec.scala) and you'll be redirected to the failing line in your text editor of choice.</p> <p></p>"},{"location":"scala/testing-munit/#enabling-tests-based-on-conditions","title":"Enabling tests based on conditions","text":"<p>MUnit supports rich test filtering capabilities.</p> <p>These let you customize what tests are run on different operating systems and with different Scala versions.</p> <p>Let's create a test file with one test that runs for a Scala versions / operating systems and other test that only runs on Windows with Scala 2.13.</p> <pre><code>package com.github.mrpowers.munit.example\n\nimport scala.util.Properties\nimport munit._\n\nobject Windows213 extends Tag(\"Windows213\")\n\nclass MySuite extends FunSuite {\n\n  override def munitTestTransforms = super.munitTestTransforms ++ List(\n    new TestTransform(\"Windows213\", { test =&gt;\n      val isIgnored =\n        test.tags(Windows213) &amp;&amp; !(\n          Properties.isWin &amp;&amp;\n            Properties.versionNumberString.startsWith(\"2.13\")\n          )\n      if (isIgnored) test.tag(Ignore)\n      else test\n    })\n  )\n\n  test(\"windows-213\".tag(Windows213)) {\n    assertEquals(2, 3)\n  }\n\n  test(\"normal test\") {\n    assertEquals(2, 2)\n  }\n\n}\n</code></pre> <p>Here's the output when you run this test file on macOS with Scala 2.12.</p> <p></p> <p>The windows-213 test is ignored and the \"normal test\" is run.</p>"},{"location":"scala/testing-munit/#next-steps","title":"Next steps","text":"<p>MUnit is a production-ready testing library that supports a bunch of awesome testing features. Clone this repo, run the test suite, and see for yourself ;)</p> <p>Projects in the scalameta family are well supported. MUnit will likely get better and grow in popularity.</p> <p>MUnit is used in production projects like scalameta, metals, and mdoc. MUnit is a new library, but it's ready for your production app.</p> <p>Big thanks to scalameta for continuing to release awesome projects and for relentlessly pushing Scala tooling forward.</p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/","title":"New Spark 3 Array Functions (exists, forall, transform, aggregate, zip_with)","text":"<p>Spark 3 has new array functions that make working with ArrayType columns much easier.</p> <p>Spark developers previously needed to use UDFs to perform complicated array functions. The new Spark functions make it easy to process array columns with native Spark.</p> <p>Some of these higher order functions were accessible in SQL as of Spark 2.4, but they didn't become part of the <code>org.apache.spark.sql.functions</code> object until Spark 3.0.</p> <p>The <code>transform</code> and <code>aggregate</code> array functions are especially powerful general purpose functions. They provide functionality that's equivalent to <code>map</code> and <code>fold</code> in Scala and make it a lot easier to work with ArrayType columns.</p> <p>You no longer need to revert to ugly UDFs to perform complex array manipulation. You can use native Spark!</p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#exists","title":"exists","text":"<p><code>exists</code> returns true if the predicate function returns true for any value in an array.</p> <p>Suppose you have the following data and would like identify all arrays that contain at least one even number.</p> <pre><code>+---------+------------+\n|person_id|best_numbers|\n+---------+------------+\n|        a|   [3, 4, 5]|\n|        b|     [8, 12]|\n|        c|     [7, 13]|\n|        d|        null|\n+---------+------------+\n</code></pre> <p>People \"a\" and \"b\" have at least one favorite number that's even, person \"c\" only has favorite odd numbers, and person \"d\" doesn't have any data.</p> <p>Start by creating an <code>isEven</code> column function that returns <code>true</code> is a number is even:</p> <pre><code>def isEven(col: Column): Column = {\n  col % 2 === lit(0)\n}\n</code></pre> <p>Let's create a DataFrame and then run the <code>org.apache.spark.sql.functions.exists</code> function to append a <code>even_best_number_exists</code> column.</p> <pre><code>val df = spark.createDF(\n  List(\n    (\"a\", Array(3, 4, 5)),\n    (\"b\", Array(8, 12)),\n    (\"c\", Array(7, 13)),\n    (\"d\", null),\n  ), List(\n    (\"person_id\", StringType, true),\n    (\"best_numbers\", ArrayType(IntegerType, true), true)\n  )\n)\n\nval resDF = df.withColumn(\n  \"even_best_number_exists\",\n  exists(col(\"best_numbers\"), isEven)\n)\n</code></pre> <p>Print the contents of the DataFrame and verify that <code>even_best_number_exists</code> contains the expected values.</p> <pre><code>resDF.show()\n\n+---------+------------+-----------------------+\n|person_id|best_numbers|even_best_number_exists|\n+---------+------------+-----------------------+\n|        a|   [3, 4, 5]|                   true|\n|        b|     [8, 12]|                   true|\n|        c|     [7, 13]|                  false|\n|        d|        null|                   null|\n+---------+------------+-----------------------+\n</code></pre> <p>You don't have to defined <code>isEven</code> as a named function. You can also use an anonymous function and get the same result.</p> <pre><code>df.withColumn(\n  \"even_best_number_exists\",\n  exists(\n    col(\"best_numbers\"),\n    (col: Column) =&gt; col % 2 === lit(0)\n  )\n)\n</code></pre> <p>Here's the exists method signature in the Spark 3 docs.</p> <p></p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#forall","title":"forall","text":"<p>Let's take a look at some arrays that contain words:</p> <pre><code>+--------------------------+\n|words                     |\n+--------------------------+\n|[ants, are, animals]      |\n|[italy, is, interesting]  |\n|[brazilians, love, soccer]|\n|null                      |\n+--------------------------+\n</code></pre> <p>Let's use <code>forall</code> to identify the arrays with words that all begin with the letter \"a\":</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(\"ants\", \"are\", \"animals\")),\n    (Array(\"italy\", \"is\", \"interesting\")),\n    (Array(\"brazilians\", \"love\", \"soccer\")),\n    (null),\n  ), List(\n    (\"words\", ArrayType(StringType, true), true)\n  )\n)\n\nval resDF = df.withColumn(\n  \"uses_alliteration_with_a\",\n  forall(\n    col(\"words\"),\n    (col: Column) =&gt; col.startsWith(\"a\")\n  )\n)\n</code></pre> <p>Let's check out the contents of <code>resDF</code> and confirm it returns true for \"ants are animals\":</p> <pre><code>resDF.show(false)\n\n+--------------------------+------------------------+\n|words                     |uses_alliteration_with_a|\n+--------------------------+------------------------+\n|[ants, are, animals]      |true                    |\n|[italy, is, interesting]  |false                   |\n|[brazilians, love, soccer]|false                   |\n|null                      |null                    |\n+--------------------------+------------------------+\n</code></pre> <p>A more interesting function would be one that returns true for any array that uses alliteration. We'll solve that problem with the more advanced array functions.</p> <p>Here's the <code>forall</code> method signature in the docs.</p> <p></p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#filter","title":"filter","text":"<p>Suppose you have the following data:</p> <pre><code>+-----------------------+\n|words                  |\n+-----------------------+\n|[bad, bunny, is, funny]|\n|[food, is, bad, tasty] |\n|null                   |\n+-----------------------+\n</code></pre> <p>Let's filter out all the array values equal to \"bad\":</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(\"bad\", \"bunny\", \"is\", \"funny\")),\n    (Array(\"food\", \"is\", \"bad\", \"tasty\")),\n    (null),\n  ), List(\n    (\"words\", ArrayType(StringType, true), true)\n  )\n)\n\nval resDF = df.withColumn(\n  \"filtered_words\",\n  filter(\n    col(\"words\"),\n    (col: Column) =&gt; col =!= lit(\"bad\")\n  )\n)\n</code></pre> <p>Print the contents of <code>resDF</code> and make sure the <code>filtered_words</code> column does not contain the word \"bad\".</p> <pre><code>resDF.show(false)\n\n+-----------------------+------------------+\n|words                  |filtered_words    |\n+-----------------------+------------------+\n|[bad, bunny, is, funny]|[bunny, is, funny]|\n|[food, is, bad, tasty] |[food, is, tasty] |\n|null                   |null              |\n+-----------------------+------------------+\n</code></pre> <p>The <code>filter</code> method is overloaded to take a function that accepts either two or one column argument.</p> <p></p> <p>Send me an example of a <code>filter</code> invocation with a column function that takes two arguments if you can figure it out.</p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#transform","title":"transform","text":"<p>Suppose we have a dataset with arrays that contain fun cities.</p> <pre><code>+----------------------+\n|places                |\n+----------------------+\n|[New York, Seattle]   |\n|[Barcelona, Bangalore]|\n|null                  |\n+----------------------+\n</code></pre> <p>Let's add a <code>fun_places</code> column that makes it clear how fun all of these cities really are!</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(\"New York\", \"Seattle\")),\n    (Array(\"Barcelona\", \"Bangalore\")),\n    (null),\n  ), List(\n    (\"places\", ArrayType(StringType, true), true)\n  )\n)\n\n\nval resDF = df.withColumn(\n  \"fun_places\",\n  transform(\n    col(\"places\"),\n    (col: Column) =&gt; concat(col, lit(\" is fun!\"))\n  )\n)\n</code></pre> <p>Print out <code>resDF</code> and confirm that <code>is fun!</code> has been appended all the elements in each array.</p> <pre><code>resDF.show(false)\n\n+----------------------+--------------------------------------+\n|places                |fun_places                            |\n+----------------------+--------------------------------------+\n|[New York, Seattle]   |[New York is fun!, Seattle is fun!]   |\n|[Barcelona, Bangalore]|[Barcelona is fun!, Bangalore is fun!]|\n|null                  |null                                  |\n+----------------------+--------------------------------------+\n</code></pre> <p><code>transform</code> works similar to the <code>map</code> function in Scala. I'm not sure why they chose to name this function <code>transform</code>\u2026 I think <code>array_map</code> would have been a better name, especially because the <code>Dataset#transform</code> function is commonly used to chain DataFrame transformations.</p> <p>Using a method name that already exists confuses folks that don't understand OOP. It also makes stuff hard to Google.</p> <p>Let's not focus on the negative. <code>org.apache.spark.functions.transform</code> now exists and is an absolute joy to work with. This is a great addition to the API.</p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#aggregate","title":"<code>aggregate</code>","text":"<p>Suppose you have a DataFrame with an array of numbers:</p> <pre><code>+------------+\n|     numbers|\n+------------+\n|[1, 2, 3, 4]|\n|   [5, 6, 7]|\n|        null|\n+------------+\n</code></pre> <p>You can calculate the sum of the numbers in the array with the <code>aggregate</code> function.</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(1, 2, 3, 4)),\n    (Array(5, 6, 7)),\n    (null),\n  ), List(\n    (\"numbers\", ArrayType(IntegerType, true), true)\n  )\n)\n\nval resDF = df.withColumn(\n  \"numbers_sum\",\n  aggregate(\n    col(\"numbers\"),\n    lit(0),\n    (col1: Column, col2: Column) =&gt; col1 + col2\n  )\n)\n</code></pre> <p>Let's check out the result:</p> <pre><code>resDF.show()\n\n+------------+-----------+\n|     numbers|numbers_sum|\n+------------+-----------+\n|[1, 2, 3, 4]|         10|\n|   [5, 6, 7]|         18|\n|        null|       null|\n+------------+-----------+\n</code></pre> <p><code>aggregate</code> isn't the best name. This concept is referred to as <code>reduce</code> in Python, <code>inject</code> in Ruby, and <code>fold</code> in Scala. The function name <code>aggregate</code> makes you think about database aggregations, not reducing an array.</p> <p>The <code>aggregate</code> function is amazingly awesome, despite the name. It opens the door for all types of interesting array reductions.</p> <p>The <code>aggregate</code> docs are hard to follow because there are so many column arguments:</p> <p></p> <p>Let me know if you have a good example of an <code>aggregate</code> function that uses the <code>finish</code> function.</p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#zip_with","title":"<code>zip_with</code>","text":"<p>Suppose we have a DataFrame with <code>letters1</code> and <code>letters2</code> columns that contain arrays of letters.</p> <pre><code>+--------+--------+\n|letters1|letters2|\n+--------+--------+\n|  [a, b]|  [c, d]|\n|  [x, y]|  [p, o]|\n|    null|  [e, r]|\n+--------+--------+\n</code></pre> <p>Let's zip the <code>letters1</code> and <code>letters2</code> arrays and join them with a <code>***</code> delimiter. We want to convert <code>[a, b]</code> and <code>[c, d]</code> into a single array: <code>[a***c, b***d]</code>.</p> <pre><code>val df = spark.createDF(\n  List(\n    (Array(\"a\", \"b\"), Array(\"c\", \"d\")),\n    (Array(\"x\", \"y\"), Array(\"p\", \"o\")),\n    (null, Array(\"e\", \"r\"))\n  ), List(\n    (\"letters1\", ArrayType(StringType, true), true),\n    (\"letters2\", ArrayType(StringType, true), true)\n  )\n)\n\nval resDF = df.withColumn(\n  \"zipped_letters\",\n  zip_with(\n    col(\"letters1\"),\n    col(\"letters2\"),\n    (left: Column, right: Column) =&gt; concat_ws(\"***\", left, right)\n  )\n)\n</code></pre> <p>Let's take a look at the results.</p> <pre><code>resDF.show()\n\n+--------+--------+--------------+\n|letters1|letters2|zipped_letters|\n+--------+--------+--------------+\n|  [a, b]|  [c, d]|[a***c, b***d]|\n|  [x, y]|  [p, o]|[x***p, y***o]|\n|    null|  [e, r]|          null|\n+--------+--------+--------------+\n</code></pre>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#ugly-udfs-from-the-past","title":"Ugly UDFs from the past","text":"<p>spark-daria implemented <code>exists</code> as a UDF and the code is pretty gross:</p> <pre><code>def exists[T: TypeTag](f: (T =&gt; Boolean)) = udf[Boolean, Seq[T]] { (arr: Seq[T]) =&gt;\n  arr.exists(f(_))\n}\n</code></pre> <p>The spark-daria <code>forall</code> UDF implementation was equally unappealing:</p> <pre><code>def forall[T: TypeTag](f: T =&gt; Boolean) = udf[Boolean, Seq[T]] { arr: Seq[T] =&gt;\n  arr.forall(f(_))\n}\n</code></pre> <p>It'll be cool to get rid of this cruft in spark-daria.</p> <p>The more spark-daria functions that are supported natively in Spark, the better.</p> <p>Let's hope we can get the spark-daria <code>createDF</code> function merged in with the Spark codebase some day\u2026</p>"},{"location":"spark-3/array-exists-forall-transform-aggregate-zip_with/#conclusion","title":"Conclusion","text":"<p>Spark 3 has added some new high level array functions that'll make working with ArrayType columns a lot easier.</p> <p>The <code>transform</code> and <code>aggregate</code> functions don't seem quite as flexible as <code>map</code> and <code>fold</code> in Scala, but they're a lot better than the Spark 2 alternatives.</p> <p>The Spark core developers really \"get it\". They're doing a great job continuing to make Spark better.</p>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/","title":"How to Use ST_Touches: Finding Geometries That Share Boundaries","text":"<p><code>ST_Touches</code> identifies geometries that share a boundary but don't overlap. You can use <code>ST_Touches</code> to identify adjacent countries, neighboring parcels, or polygons that share edges. It works consistently across spatial query engines like PostGIS, DuckDB, and Apache Sedona.</p> <p><code>ST_Touches</code> differs from other spatial predicates:</p> <ul> <li><code>ST_Intersects</code> includes overlaps</li> <li><code>ST_Contains</code> checks for one shape within another</li> </ul> <p><code>ST_Touches</code> specifically finds shapes that share boundaries.</p> <p>Adjacent states or countries that share borders are examples of geometries that touch.  Let\u2019s dive into a conceptual overview and then see how to use <code>ST_Touches</code> with PostGIS, DuckDB, and Apache Sedona.</p>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#how-st_touches-functions","title":"How ST_Touches functions","text":"<p><code>ST_Touches</code> returns true when two geometries share at least one boundary point, but their interiors don't overlap.</p> <p>Classic examples of touching geometries:</p> <ul> <li>Two squares that share an edge</li> <li>A line that ends exactly at a polygon boundary</li> <li>Countries that share a border (like the USA and Canada)</li> </ul> <p>What doesn't count as touching:</p> <ul> <li>Shapes that overlap, which is an intersection</li> <li>Completely separate shapes</li> <li>When one shape is completely inside another, which is a \u201ccontains\u201d relationship</li> </ul> <p>Touching means sharing boundaries, not sharing interiors.</p> <p>Here\u2019s the basic SQL syntax for <code>ST_Touches</code>:</p> <pre><code>SELECT ST_Touches(geometry1, geometry2) AS touches;\n</code></pre> <p>Let\u2019s look at three parcels, P001, P002, and P003, for a concrete example:</p> <p></p> <p>Here\u2019s a summary of the touches:</p> <ul> <li>P001 and P002 share an edge</li> <li>P001 and P003 share an edge</li> <li>P002 and P003 don't touch each other</li> </ul> <p>Let\u2019s examine how to summarize the touching relationships with Apache Sedona.</p>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#apache-sedona-st_touches-examples","title":"Apache Sedona ST_Touches Examples","text":"<p>Apache Sedona supports <code>ST_Touches</code> with Apache Spark, Apache Flink, and Snowflake via SnowSedona.  The following example uses PySpark and Apache Sedona.</p> <p>Let\u2019s create some sample parcels using adjacent polygons that touch:</p> <pre><code>sample_parcels_data = [\n    (\"P001\", \"POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))\"),\n    (\"P002\", \"POLYGON((10 0, 10 5, 20 5, 20 0, 10 0))\"),\n    (\"P003\", \"POLYGON((0 10, 0 20, 10 20, 10 10, 0 10))\")\n]\nparcels_df = spark.createDataFrame(sample_parcels_data, [\"parcel_id\", \"geometry\"])\n\nparcels_df = parcels_df.selectExpr(\n    \"parcel_id\",\n    \"ST_GeomFromWKT(geometry) as geometry\"\n)\n\nparcels_df.createOrReplaceTempView(\"parcels\")\n</code></pre> <p>Here\u2019s the content of <code>parcels_df</code>:</p> <pre><code>+----------+--------------------------------------------+\n|parcel_id |geometry                                    |\n+----------+--------------------------------------------+\n|P001      |POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))      |\n|P002      |POLYGON((10 0, 10 5, 20 5, 20 0, 10 0))     |\n|P003      |POLYGON((0 10, 0 20, 10 20, 10 10, 0 10))   |\n+----------+--------------------------------------------+\n</code></pre> <p>Now let\u2019s find all the touching relationships:</p> <pre><code>result = spark.sql(\"\"\"\nSELECT \n    p1.parcel_id,\n    p2.parcel_id\nFROM parcels p1, parcels p2\nWHERE ST_Touches(p1.geometry, p2.geometry)\n\"\"\")\n</code></pre> <p>Here\u2019s the result of the query:</p> <pre><code>+----------+--------------+\n|parcel_id |parcel_id     |\n+----------+--------------+\n|P001      |P002          |\n|P002      |P001          |\n|P001      |P003          |\n|P003      |P001          |\n+----------+--------------+\n</code></pre> <p>The query returns both directions of each touching relationship, so there is one row for P001 touching P002 and another row for P002 touching P001.</p> <p>Here\u2019s how you can modify the query to return only one direction of the touching relationship:</p> <pre><code>SELECT \n    p1.parcel_id,\n    p2.parcel_id\nFROM parcels p1, parcels p2\nWHERE ST_Touches(p1.geometry, p2.geometry)\nAND p1.parcel_id &lt; p2.parcel_id\n</code></pre> <p>Here are the results of the query:</p> <pre><code>+----------+----------+\n|parcel_id |parcel_id |\n+----------+----------+\n|P001      |P002      |\n|P001      |P003      |\n+----------+----------+\n</code></pre>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#duckdb-st_touches-examples","title":"DuckDB ST_Touches Examples","text":"<p>DuckDB also supports <code>ST_Touches</code> via the <code>spatial</code> extension.</p> <p>Start by installing the <code>spatial</code> extension:</p> <pre><code>INSTALL spatial;\nLOAD spatial;\n</code></pre> <p>Now, create the <code>parcels</code> table:</p> <pre><code>CREATE TABLE parcels AS\nSELECT \n    parcel_id,\n    ST_GeomFromText(geometry) AS geometry\nFROM (\n    SELECT * FROM VALUES\n        ('P001', 'POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))'),\n        ('P002', 'POLYGON((10 0, 10 5, 20 5, 20 0, 10 0))'),\n        ('P003', 'POLYGON((0 10, 0 20, 10 20, 10 10, 0 10))')\n    AS t(parcel_id, geometry)\n);\n</code></pre> <p>Join the parcels table to see the geometries that are touching:</p> <pre><code>SELECT\n    p1.parcel_id,\n    p2.parcel_id\nFROM\n    parcels p1\nJOIN\n    parcels p2\nON\n    ST_Touches(p1.geometry, p2.geometry)\nWHERE\n    p1.parcel_id &lt; p2.parcel_id;\n</code></pre> <p>Here\u2019s the result:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 parcel_id \u2502 parcel_id \u2502\n\u2502  varchar  \u2502  varchar  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 P001      \u2502 P002      \u2502\n\u2502 P001      \u2502 P003      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#postgis-st_touches-examples","title":"PostGIS ST_Touches Examples","text":"<p>This section shows how to use <code>ST_Touches</code> with PostGIS.</p> <p>Here\u2019s how to create the parcels table with PostGIS:</p> <pre><code>CREATE TABLE parcels AS\nSELECT \n    parcel_id,\n    ST_GeomFromText(geometry) AS geometry\nFROM (\n    SELECT * FROM (VALUES\n        ('P001', 'POLYGON((0 0, 0 10, 10 10, 10 0, 0 0))'),\n        ('P002', 'POLYGON((10 0, 10 5, 20 5, 20 0, 10 0))'),\n        ('P003', 'POLYGON((0 10, 0 20, 10 20, 10 10, 0 10))')\n    ) AS t(parcel_id, geometry)\n);\n</code></pre> <p>Now see the geometries that are touching with PostGIS:</p> <pre><code>SELECT\n    p1.parcel_id,\n    p2.parcel_id\nFROM\n    parcels p1\nJOIN\n    parcels p2\nON\n    ST_Touches(p1.geometry, p2.geometry)\nWHERE\n    p1.parcel_id &lt; p2.parcel_id;\n</code></pre> <p>Here\u2019s the result of the query:</p> <pre><code>parcel_id | parcel_id \n-----------+-----------\n P001      | P002\n P001      | P003\n</code></pre>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#common-gotchas-with-st_touches","title":"Common Gotchas with ST_Touches","text":""},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#point-to-point-touching","title":"Point-to-Point Touching","text":"<p>Points can't touch other points in most implementations. They can be equal, but points with identical coordinates aren\u2019t considered touching.  This query returns false for all of them on this page.</p> <pre><code>SELECT ST_Touches(ST_Point(1, 1), ST_Point(1, 1));\n</code></pre> <p>Here\u2019s the result in Sedona:</p> <pre><code>spark.sql(\"SELECT ST_Touches(ST_Point(1, 1), ST_Point(1, 1));\").show()\n\n+------------------------------------------+\n|st_touches(st_point(1, 1), st_point(1, 1))|\n+------------------------------------------+\n|                                     false|\n+------------------------------------------+\n</code></pre> <p>Here\u2019s the result in DuckDB:</p> <pre><code>SELECT ST_Touches(ST_Point(1, 1), ST_Point(1, 1));\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 st_touches(st_point(1, 1), st_point(1, 1)) \u2502\n\u2502                  boolean                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 false                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Here\u2019s the result in PostGIS:</p> <pre><code>matthewpowers=# SELECT ST_Touches(ST_Point(1, 1), ST_Point(1, 1));\n\n st_touches \n------------\n f\n</code></pre>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#self-touches","title":"Self-Touches","text":"<p>A geometry can't touch itself, so the following code always returns false:</p> <pre><code>SELECT ST_Touches(geom, geom);\n</code></pre>"},{"location":"spatial/functions/st-touches-sedona-duckdb-postgis/#st_touches-conclusion","title":"ST_Touches Conclusion","text":"<p><code>ST_Touches</code> is commonly used for boundary analysis with geospatial engines. It consistently identifies adjacent geometries that don\u2019t overlap.</p> <p><code>ST_Touches</code> adheres to the DE-9IM (Dimensionally Extended 9-Intersection Model) specification, ensuring consistent behavior across various spatial engines and databases.</p>"},{"location":"sqlite/create-database-load-csv-python/","title":"Creating a sqlite database from CSVs with Python","text":"<p>This blog post demonstrates how to build a sqlite database from CSV files.</p> <p>Python is perfect language for this task because it has great libraries for sqlite and CSV DataFrames.</p>"},{"location":"sqlite/create-database-load-csv-python/#creating-a-sqlite-database","title":"Creating a sqlite database","text":"<p>sqlite is a lightweight database that can be started as an empty text file. You can create the file with <code>touch my_data.db</code> or with this equivalent Python code:</p> <pre><code>from pathlib import Path\nPath('my_data.db').touch()\n</code></pre> <p>A zero byte text file is a great starting point for a lightweight database!</p>"},{"location":"sqlite/create-database-load-csv-python/#creating-sqlite-table","title":"Creating sqlite table","text":"<p>Create a database connection and cursor to execute queries.</p> <pre><code>import sqlite3\n\nconn = sqlite3.connect('my_data.db')\nc = conn.cursor()\n</code></pre> <p>Execute a query that'll create a <code>users</code> table with <code>user_id</code> and <code>username</code> columns.</p> <pre><code>c.execute('''CREATE TABLE users (user_id int, username text)''')\n</code></pre>"},{"location":"sqlite/create-database-load-csv-python/#load-csv-file-into-sqlite-table","title":"Load CSV file into sqlite table","text":"<p>Suppose you have the following <code>users.csv</code> file:</p> <pre><code>user_id,username\n1,pokerkid\n2,crazyken\n</code></pre> <p>Pandas makes it easy to load this CSV data into a sqlite table:</p> <pre><code>import pandas as pd\n\n# load the data into a Pandas DataFrame\nusers = pd.read_csv('users.csv')\n# write the data to a sqlite table\nusers.to_sql('users', conn, if_exists='append', index = False)\n</code></pre> <p>The <code>to_sql</code> method makes it easy to write DataFrames to databases.</p>"},{"location":"sqlite/create-database-load-csv-python/#fetch-values-from-sqlite-table","title":"Fetch values from sqlite table","text":"<p>Fetch all the rows from the <code>users</code> table:</p> <pre><code>c.execute('''SELECT * FROM users''').fetchall() # [(1, 'pokerkid'), (2, 'crazyken')]\n</code></pre> <p>The <code>fetchall()</code> method returns an array of tuples.</p> <p><code>c.execute()</code> returns a <code>sqlite3.Cursor</code> object. Cursors can be thought of as iterators in the database world.</p>"},{"location":"sqlite/create-database-load-csv-python/#load-another-csv-into-the-databases","title":"Load another CSV into the databases","text":"<p>Suppose you have the following <code>orders.csv</code> file:</p> <pre><code>order_id,user_id,item_name\n1,1,speaker\n2,1,phone\n3,2,spoon\n4,2,fork\n5,2,speaker\n</code></pre> <p>Create a table and then load the orders data into the database.</p> <pre><code>c.execute('''CREATE TABLE orders (order_id int, user_id int, item_name text)''')\norders = pd.read_csv('orders.csv') # load to DataFrame\norders.to_sql('orders', conn, if_exists='append', index = False) # write to sqlite table\n</code></pre>"},{"location":"sqlite/create-database-load-csv-python/#fetch-results-of-database-join","title":"Fetch results of database join","text":"<p>Join the <code>users</code> and <code>orders</code> tables on the <code>user_id</code> value and print the results:</p> <pre><code>c.execute('''SELECT * FROM users u LEFT JOIN orders o ON u.user_id = o.user_id''')\nc.fetchall()\n</code></pre> <p>Here's the array that's returned:</p> <pre><code>[(1, 'pokerkid', 1, 1, 'speaker'),\n (1, 'pokerkid', 2, 1, 'phone'),\n (2, 'crazyken', 3, 2, 'spoon'),\n (2, 'crazyken', 4, 2, 'fork'),\n (2, 'crazyken', 5, 2, 'speaker')]\n</code></pre> <p>You can also read the SQL query directly into a Pandas DataFrame.</p> <pre><code>pd.read_sql('''SELECT * FROM users u LEFT JOIN orders o ON u.user_id = o.user_id''', conn)\n</code></pre> <p></p>"},{"location":"sqlite/create-database-load-csv-python/#next-steps","title":"Next steps","text":"<p>Python's build in sqlite library coupled with Pandas DataFrames makes it easy to load CSV data into sqlite databases.</p> <p>sqlite databases are great for local experimentation and are used extensively on mobile phones. It's a great database when you'd like relational database query functionality without the overhead of Postgres.</p> <p>Python's great support for sqlite will make you love it in no time.</p>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/","title":"Write sqlite tables to CSV / Parquet files","text":"<p>This blog post explains how to write sqlite tables to CSV and Parquet files. It'll also show how to output SQL queries to CSV files.</p> <p>It'll even show how to output all the tables in a sqlite database to files with a single command.</p>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/#create-sqlite-database","title":"Create sqlite database","text":"<p>We'll start by creating a sqlite database. The database will contain the following <code>trees</code> and <code>orders</code> tables:</p> <p></p> <p></p> <p>We'll use Pandas to create DataFrames that can be loaded into the sqlite database.</p> <pre><code>import pandas as pd\nimport sqlite3\n\n# establish database connection\nconn = sqlite3.connect('nature.db')\nc = conn.cursor()\n\n# create sqlite database tables\nc.execute('''CREATE TABLE trees (id int, tree_type text, has_leaves int)''')\nc.execute('''CREATE TABLE orders (id int, tree_id int, price real)''')\n\n# create DataFrames\ntree_data = [(1, 'oak', 1),\n    (2, 'pine', 0),\n    (3, 'palm', 0)]\n\ntree_df = pd.DataFrame.from_records(tree_data, columns=['id', 'tree_type', 'has_leaves'])\n\norders_data = [(1, 1, 19.99),\n    (2, 1, 29.99),\n    (3, 3, 49.95)]\n\norders_df = pd.DataFrame.from_records(orders_data, columns=['id', 'tree_id', 'price'])\n\n# load DataFrames into sqlite database\ntree_df.to_sql('trees', conn, if_exists='append', index = False)\norders_df.to_sql('orders', conn, if_exists='append', index = False)\n</code></pre> <p>We're ready to start exporting tables to files now that we have data loaded into a database.</p>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/#export-entire-table-to-file","title":"Export entire table to file","text":"<p>Here's how to export the <code>orders</code> table to a CSV file.</p> <pre><code># save sqlite table in a DataFrame\ndf = pd.read_sql('SELECT * from orders', conn)\n\n# write DataFrame to CSV file\ndf.to_csv('orders.csv', index = False)\n</code></pre> <p>Here are the contents of the <code>orders.csv</code> file:</p> <pre><code>id,tree_id,price\n1,1,19.99\n2,1,29.99\n3,3,49.95\n</code></pre> <p>Notice that the CSV file includes the data header row.</p>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/#export-part-of-table-to-file","title":"Export part of table to file","text":"<p>Here's how to export all the <code>orders</code> that cost more than $25 to a CSV file.</p> <pre><code>df = pd.read_sql_query('SELECT * from orders where price &gt; 25', conn)\ndf.to_csv('orders_over_25.csv', index = False)\n</code></pre> <p>Here's what the <code>orders_over_25.csv</code> file contains:</p> <pre><code>id,tree_id,price\n2,1,29.99\n3,3,49.95\n</code></pre> <p>The SQL query is executed in the database before the data is passed to the DataFrame. This is known as predicate pushdown filtering and can result in a significant performance boost.</p>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/#export-all-tables-to-files","title":"Export all tables to files","text":"<p>Here's how to export all the sqlite tables to CSV files with a single command:</p> <pre><code>for table in c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall():\n    t = table[0]\n    df = pd.read_sql('SELECT * from ' + t, conn)\n    df.to_csv(t + '_one_command.csv', index = False)\n</code></pre> <p>Notice that the <code>sqlite_master</code> table is queried to get all the tables in the database.</p> <p>Here's what the <code>orders_one_command.csv</code> file contains:</p> <pre><code>id,tree_id,price\n1,1,19.99\n2,1,29.99\n3,3,49.95\n</code></pre> <p>Here's what the <code>trees_one_command.csv</code> file contains:</p> <pre><code>id,tree_type,has_leaves\n1,oak,1\n2,pine,0\n3,palm,0\n</code></pre>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/#export-parquet-files","title":"Export Parquet files","text":"<p>Here's code that'll export the <code>trees</code> table to a Parquet file:</p> <pre><code>df = pd.read_sql('SELECT * from trees', conn)\ndf.to_parquet('trees.parquet', index = False)\n</code></pre> <p>Parquet files are not human readable, but they're a way better storage format compared to CSV in almost all cases, as explained here.</p> <p>You need to make sure PyArrow is installed before you can call the <code>to_parquet</code> command.</p>"},{"location":"sqlite/export-tables-queries-to-csv-parquet/#next-steps","title":"Next steps","text":"<p>See here for a Jupyter notebook that contains all the commands covered in this blog post.</p> <p>It's easy to export sqlite tables or query results to CSV / Parquet files with Python. Pandas does all the heavy lifting.</p> <p>See here to learn how to load CSV files into sqlite tables.</p>"},{"location":"unix/appending-executables-path/","title":"Adding executables to your PATH for fun","text":"<p>Adding executables to your PATH is fun, easy, and a great way to learn about how your machine works.</p> <p>This pattern is especially useful for long commands that you need to run frequently.</p> <p>Learning how to add executables to your PATH will help you understand how libraries like pyenv use the shim design pattern to seamlessly switch between Python versions.</p> <p>While we're learning about this design pattern, we'll also review how your machine uses the PATH and where executables are stored on our system.</p>"},{"location":"unix/appending-executables-path/#building-a-more-useful-file-listing-command","title":"Building a more useful file listing command","text":"<p>The <code>ls</code> command outputs the files and folders in the current directory.</p> <p></p> <p>The <code>ls</code> output is more useful when it is run with flags so the output shows hidden directories and file sizes.</p> <p></p> <p>It's common to create a <code>ll</code> command that runs <code>ls -ahlF</code>. You don't want to always type <code>ls -ahlF</code> every time you do a file listing.</p> <p>It's best to use a <code>~/.bash_profile</code> alias for something simple like this, as described in this answer. We'll create an executable and add it to our path for learning purposes. This will set us up nicely for a more complicated example.</p> <p>Let's start by creating a <code>~/.cali/ll</code> file. We'll put our executables in the <code>~/.cali</code> directory, following this design pattern.</p> <pre><code>mkdir ~/.cali\ntouch ~/.cali/ll\n</code></pre> <p>Open the <code>ll</code> file with a text editor and add <code>ls -ahlF</code>.</p> <p>We can run the <code>ll</code> script with <code>bash ~/.cali/ll</code>, but we can't run <code>~/.cali/ll</code> yet because the file isn't an executable.</p> <p>Let's make the <code>ll</code> file an executable: <code>chmod +x ~/.cali/ll</code>.</p> <p>We want to be able to run <code>ll</code> from the command line, just like any other Terminal command. We can run <code>~/.cali/ll</code>, but we want to be able to simply run <code>ll</code> from any directory, similar to <code>ls</code>.</p> <p>Let's append <code>~/.cali</code> to our PATH environment variable.</p> <p>Add this code to the <code>~/.bash_profile</code> file:</p> <pre><code>export PATH=$PATH:~/.cali\n</code></pre> <p>We can reload the bash profile with <code>source ~/.bash_profile</code> and then run <code>ll</code> from the command line.</p> <p></p>"},{"location":"unix/appending-executables-path/#understanding-path","title":"Understanding PATH","text":"<p>The PATH environment variable is an ordered list of directories that contain executables.</p> <p>You can view the contents of your PATH by running <code>echo $PATH</code>. It'll return something like this: <code>/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/matthewpowers/.cali</code>.</p> <p>Each directory is separated by a colon. It's easier to view them as a list:</p> <ul> <li>/usr/local/bin</li> <li>/usr/bin</li> <li>/bin</li> <li>/usr/sbin</li> <li>/sbin</li> <li>/Users/matthewpowers/.cali</li> </ul> <p>Here are some of the files contained in <code>/usr/local/bin</code> on my machine.</p> <p></p> <p>We can type <code>which autoheader</code> to find where this executable file is stored on my system.</p> <p></p> <p>We can type <code>which ls</code> to see that the <code>ls</code> executable is stored in the <code>/bin</code> directory.</p> <p></p> <p>When <code>ls ~/Documents</code> is run, your computer will start by looking for an <code>ls</code> executable in the <code>/usr/local/bin</code> directory. It won't find one, so it'll look in <code>/usr/bin</code>. It wont find one there either, so it'll look in <code>/bin</code>. It'll find the executable in <code>/bin</code>, so it'll stop looking and use the <code>/bin/ls</code> executable.</p> <p>We can even type <code>which which</code> to see that the <code>which</code> executable is stored in <code>/usr/bin</code>:</p> <p></p> <p>This is too much fun!!!</p> <p>Let's type <code>which ll</code> and see where the <code>ll</code> executable is stored:</p> <p></p> <p>Of course, <code>ll</code> is in the <code>~/.cali</code> directory - that's where we put it!</p>"},{"location":"unix/appending-executables-path/#adding-s3_ll","title":"Adding <code>s3_ll</code>","text":"<p>The AWS CLI offers this command to see the size of a S3 bucket: <code>aws s3 ls --summarize --human-readable --recursive s3://bucket-name/directory</code>.</p> <p>This command is too long to remember. Let's create an executable that takes a single argument. We want <code>s3_ll my_bucket/some_folder</code> to run <code>aws s3 ls --summarize --human-readable --recursive s3://my_bucket/some_folder</code>.</p> <p>Let's create the <code>~/.cali/s3_ll</code> file.</p> <pre><code>touch `~/.cali/s3_ll`\n</code></pre> <p>Open the file and add this code: <code>aws s3 ls --summarize --human-readable --recursive s3://$1</code></p> <p>The <code>$1</code> is how we pass the argument from the Terminal command to the shell script. In the <code>s3_ll s3_ll my_bucket/some_folder</code>, <code>s3_ll</code> is the command and <code>my_bucket/some_folder</code> is the argument. <code>$1</code> is how we access the argument in the shell script.</p>"},{"location":"unix/appending-executables-path/#shim-design-pattern","title":"Shim design pattern","text":"<p>We appended the <code>.cali</code> directory to our path - we put the directory at the end of the PATH environment variable.</p> <p>The shim design pattern depends on prepending a directory to our path to intercept commands.</p> <p>pyenv and rbenv use the shim design pattern to allow users to seamlessly switch between Python and Ruby versions.</p> <p>Read this post to learn more about the shim design pattern.</p>"},{"location":"unix/appending-executables-path/#next-steps","title":"Next steps","text":"<p>Adding executables to your path is fun and useful.</p> <p>It teaches you about how your system executes commands.</p> <p>Customizing your environment is great, as long as the customizations are documented and stored in a version controlled repo. You should be able to easily replicate your custom setup on a different machine. Take a look at cali for an example of customizations that can be easily transitioned to a new machine.</p>"}]}